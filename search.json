[
  {
    "objectID": "blogs/blog.html",
    "href": "blogs/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Sort by:\n    \n        Date\n        Type\n    \n    View:\n    \n        List\n        Cards\n    \n\n\n\n\n\n    \n        \n            \n                \n            \n            \n                Self-Regulated Neurogenesis for Online Data-Incremental Learning\n                Method 2025-06-05 CoLLAs 2025\n                We present SERENA, a neuro-inspired solution for continual learning that mimics the self-regulated neurogenesis process in the human brain.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Continual Learning with Dynamic Sparse Training\n                Analysis 2025-06-04 CPAL 2024\n                We show how sparse training helps us learn much faster while forgetting less. Based on our CPAL paper \"Continual Learning with Dynamic Sparse Training\".\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Continual Learning with Informative Samples\n                Analysis 2025-06-03\n                We show how continual learning benefits from selecting only the more informative (or surprising) new data points.\".\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Meta-learning for Likelihood-free Bayesian Optimization\n                Method 2025-06-02 ICML 2024\n                We introduce MALIBO, a novel and scalable framework that leverages meta-learning for fast and efficient Bayesian optimization.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Adaptive Continual Learning\n                Method 2025-06-01 ContinualAI Unconference 2023\n                We introduce AdaCL, a new method that optimally adapts continual learning hyperparameters to every new task.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                The AutoML Benchmark\n                Benchmarking 2024-12-06 JMLR\n                About why we wrote our paper \"AMLB: an AutoML Benchmark\" and its main contributions.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                OpenML x Probabl Hackathon\n                Hackathon 2024-09-19\n                We visited Probabl in Paris to discuss open source and open science. \n            \n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    CoLLAs 2025\n            Self-Regulated Neurogenesis for Online Data-Incremental Learning\n            We present SERENA, a neuro-inspired solution for continual learning that mimics the self-regulated neurogenesis process in the human brain.\n        \n        \n            2025-06-05\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    CPAL 2024\n            Continual Learning with Dynamic Sparse Training\n            We show how sparse training helps us learn much faster while forgetting less. Based on our CPAL paper \"Continual Learning with Dynamic Sparse Training\".\n        \n        \n            2025-06-04\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    \n            Continual Learning with Informative Samples\n            We show how continual learning benefits from selecting only the more informative (or surprising) new data points.\".\n        \n        \n            2025-06-03\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    ICML 2024\n            Meta-learning for Likelihood-free Bayesian Optimization\n            We introduce MALIBO, a novel and scalable framework that leverages meta-learning for fast and efficient Bayesian optimization.\n        \n        \n            2025-06-02\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    ContinualAI Unconference 2023\n            Adaptive Continual Learning\n            We introduce AdaCL, a new method that optimally adapts continual learning hyperparameters to every new task.\n        \n        \n            2025-06-01\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Benchmarking\n    JMLR\n            The AutoML Benchmark\n            About why we wrote our paper \"AMLB: an AutoML Benchmark\" and its main contributions.\n        \n        \n            2024-12-06\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Hackathon\n    \n            OpenML x Probabl Hackathon\n            We visited Probabl in Paris to discuss open source and open science. \n        \n        \n            2024-09-19\n            ➤\n        \n    \n    Analysis\n    \n        \n            \n                \n            \n            \n                Continual Learning with Dynamic Sparse Training\n                Analysis 2025-06-04 CPAL 2024\n                We show how sparse training helps us learn much faster while forgetting less. Based on our CPAL paper \"Continual Learning with Dynamic Sparse Training\".\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Continual Learning with Informative Samples\n                Analysis 2025-06-03\n                We show how continual learning benefits from selecting only the more informative (or surprising) new data points.\".\n            \n        \n    \n    Benchmarking\n    \n        \n            \n                \n            \n            \n                The AutoML Benchmark\n                Benchmarking 2024-12-06 JMLR\n                About why we wrote our paper \"AMLB: an AutoML Benchmark\" and its main contributions.\n            \n        \n    \n    Hackathon\n    \n        \n            \n                \n            \n            \n                OpenML x Probabl Hackathon\n                Hackathon 2024-09-19\n                We visited Probabl in Paris to discuss open source and open science. \n            \n        \n    \n    Method\n    \n        \n            \n                \n            \n            \n                Self-Regulated Neurogenesis for Online Data-Incremental Learning\n                Method 2025-06-05 CoLLAs 2025\n                We present SERENA, a neuro-inspired solution for continual learning that mimics the self-regulated neurogenesis process in the human brain.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Meta-learning for Likelihood-free Bayesian Optimization\n                Method 2025-06-02 ICML 2024\n                We introduce MALIBO, a novel and scalable framework that leverages meta-learning for fast and efficient Bayesian optimization.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Adaptive Continual Learning\n                Method 2025-06-01 ContinualAI Unconference 2023\n                We introduce AdaCL, a new method that optimally adapts continual learning hyperparameters to every new task.\n            \n        \n    \n    Analysis\n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    CPAL 2024\n            Continual Learning with Dynamic Sparse Training\n            We show how sparse training helps us learn much faster while forgetting less. Based on our CPAL paper \"Continual Learning with Dynamic Sparse Training\".\n        \n        \n            2025-06-04\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    \n            Continual Learning with Informative Samples\n            We show how continual learning benefits from selecting only the more informative (or surprising) new data points.\".\n        \n        \n            2025-06-03\n            ➤\n        \n    \n    Benchmarking\n    \n        \n            \n        \n        \n            \n    \n        Benchmarking\n    JMLR\n            The AutoML Benchmark\n            About why we wrote our paper \"AMLB: an AutoML Benchmark\" and its main contributions.\n        \n        \n            2024-12-06\n            ➤\n        \n    \n    Hackathon\n    \n        \n            \n        \n        \n            \n    \n        Hackathon\n    \n            OpenML x Probabl Hackathon\n            We visited Probabl in Paris to discuss open source and open science. \n        \n        \n            2024-09-19\n            ➤\n        \n    \n    Method\n    \n        \n            \n        \n        \n            \n    \n        Method\n    CoLLAs 2025\n            Self-Regulated Neurogenesis for Online Data-Incremental Learning\n            We present SERENA, a neuro-inspired solution for continual learning that mimics the self-regulated neurogenesis process in the human brain.\n        \n        \n            2025-06-05\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    ICML 2024\n            Meta-learning for Likelihood-free Bayesian Optimization\n            We introduce MALIBO, a novel and scalable framework that leverages meta-learning for fast and efficient Bayesian optimization.\n        \n        \n            2025-06-02\n            ➤\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    ContinualAI Unconference 2023\n            Adaptive Continual Learning\n            We introduce AdaCL, a new method that optimally adapts continual learning hyperparameters to every new task.\n        \n        \n            2025-06-01\n            ➤"
  },
  {
    "objectID": "people/people.html",
    "href": "people/people.html",
    "title": "People",
    "section": "",
    "text": "Permanent researchers\n\n\n\n    \n        \n            \n        \n        \n            \n                Joaquin Vanschoren is an associate professor and head of the Automated Machine Learning lab at TU Eindhoven. He aims to scientifically understand (human-like) intelligence and build AI systems with advanced capabilities for the benefit of all humanity. He authored the first book on AutoML, gave tutorials at NeurIPS and AAAI and won several awards, including the Dutch Data Prize and Amazon Research Award. He founded OpenML, a useful open science platform for machine learning, and co-founded the Croissant standard for sharing AI resources. He was the inaugural chair of the NeurIPS Datasets and Benchmarks track, editor-in-chief of the DMLR journal, and co-chair of the MLCommons AI Risk & Reliability working group. He is a founding member of the European AI societies ELLIS and CAIRNE.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nResearch engineers\n\n\n\n    \n        \n            \n        \n        \n            \n                Pieter Gijsbers is working on making (automated) machine learning research simple through developing open source software. He is a long-term contributor to openml-python, started the AutoML Benchmark and GAMA. Pieter is currently working on improving the AI-on-Demand platform.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Taniya Das is a Research Engineer solving various engineering and machine learning tasks for openml.org and  AI-on-Demand platform (an EU project), to make ML research better. She has contributed to openml-tensorflow  and openml-pytorch extensions for OpenML, making it possible to use solve deep learning tasks using openml-python API. She is currently working on using LLMs to  make both the platforms more intelligent.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Subhaditya Mukherjee is a Research Engineer working on various engineering and machine learning tasks for openml.org. He is currently working on making the OpenML experience better and more user-friendly.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nLecturers\n\n\n\n    \n        \n            \n        \n        \n            \n                Prabhant Singh I am currently working as a lecturer and researcher. I have experience in Python, data and systems as well as infrastructure engineering.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nPostdocs\n\n\n\n    \n        \n            \n        \n        \n            \n                Alexis Cvetkov-Iliev is a postdoc in the group, applying machine-learning techniques to study and optimize building renovation. His research interests include active learning, Bayesian optimization, and transfer learning.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Anna Vettoruzzo is a postdoctoral researcher contributing to the OpenEuroLLM project, where she focuses on designing scalable model architectures for Large Language Models (LLMs) and developing efficient post-training techniques. She received her Ph.D. in machine learning from Halmstad University, Sweden, as part of the Center for Applied and Intelligent Systems Research (CAISR). Her doctoral research focused on enhancing the generalization capabilities of machine learning models through meta-learning, exploring how models can learn to learn. Anna received her M.Sc. degree in ICT for Internet and Multimedia from the University of Padova, in 2021, with a focus on machine learning for healthcare. Her broader research interests include model generalization and adaptability, few-shot learning, continual learning, and in-context learning.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Gennaro Gala is a postdoctoral researcher contributing to the OpenEuroLLM project. His current research focuses on large language models, probabilistic generative modeling, structured representation learning, and tensor networks. In March 2025, he completed his PhD in probabilistic ML at Eindhoven University of Technology, under the supervision of Prof. Cassio de Campos and Dr. Erik Quaeghebeur. From 2015 to 2020, he completed both his BSc and MSc in Computer Science at the Università degli Studi di Bari Aldo Moro (Italy), with a focus on Machine Intelligence and Knowledge Engineering. During this period, he actively collaborated with the Italian National Research Council on projects applying machine learning and deep learning to marine biology.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Matthew Danish is a postdoctoral researcher in the AutoML group at Eindhoven University of Technology (TU/e) in the Netherlands. He is currently engaged with the AI Pathfinder initiative. He obtained his Ph.D. in 2015 from Boston University, worked on research projects at Cambridge University department of Computer Science and Technology ranging from lightweight Fortran code verification to computer vision and sensor networks, collaborating with industry and governmental initiatives. Later, after moving to the Netherlands, he joined the Geosciences department of Utrecht University for a cross-disciplinary machine learning project.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nPhD students\n\n\n\n    \n        \n            \n        \n        \n            \n                Andrei Simion-Constantinescu is a PhD Researcher in the AutoML group focused on self-supervised learning for computer vision tasks. He holds an MSc degree from TU Delft with the graduation topic on contrastive learning for unlabeled videos. His work involves adapting state-of-the-art self-supervised techniques to solve practical problems in greenhouse crop prediction and vertical farming as part of NWO Sky High Project.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Israel Campero Jurado is a PhD student in computer science working on the European ITEA project called INNO4HEALTH. His interests focus on applying automated machine learning (AutoML) to healthcare solutions and democratising AutoML.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Jiarong Pan is a PhD student in the group, working on improving sample efficiency in machine learning algorithms. His research interests include  meta-learning, Bayesian optimization and multi-objective optimization.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Fangqin Zhou is a PhD student working on the TWINERGY project, which aims to optimize the growth of cherry tomatos in a vertical farm. Her interests focus on the Hyperspectral Imaging using deep learning and transformer models, as well as Reinforcement Learning.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Elif Ceren Gok Yildirim is a PhD student working on continual learning. Her research centers on exploring the capabilities of machine learning algorithms and advancing them to adapt and evolve over time. She likes to explore novel approaches to enable machines to accumulate knowledge from past experiences and apply it to new tasks without forgetting and contribute to the CL field.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Murat Onur Yildirim is a PhD Candidate in the AutoML group. He focuses on automatically and continually learning sparse experts for computer vision tasks. His research is dedicated to creating efficient continual learners by leveraging sparsity in networks, data, and labels.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Bilge Celik is a PhD student focusing on Automated Machine Learning for online data streams. She develops machine learning systems automatically adjusting to changing dynamics of real-time data streams.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Dalton Harmsen obtained his M.Sc. degree in Data Science & Artificial Intelligence at Eindhoven University of Technology (Netherlands). His M.Sc. dissertation was carried out on the topic of benchmarking post-training quantization techniques for large language models under the supervision of J.M. Tomczak. Currently, he is a Ph.D. student at Eindhoven University of Technology – Data Science Domain with the DAI Cluster. Dalton Harmsen works on the OpenEuroLLM project under the supervision of Joaquin Vanschoren.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Diana Alexandra Onutu is a PhD candidate in the AutoML group at Eindhoven University of Technology (TU/e), Netherlands. She is involved in the Open Euro LLM project, which focuses on developing the next generation of open-source, multilingual foundation models. Her main interests include model exploration, pre-training strategies, and diffusion-based language models. Diana obtained her M.Sc. degree in Data Science & Artificial Intelligence from TU/e. Her M.Sc. thesis was carried out within the field of Machine Learning for Science, in which she proposed a score matching generative model for large geometric graphs designed to produce cosmological data.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nAlumni and visitors\n\n\n\n    \n        \n            \n        \n        \n            \n                Branislav Pecher is a former visiting PhD student from Kempelen Institute of Intelligent Technologies. His research mainly focuses on learning with limited labelled data and a better understanding of its sensitivity to different factors that influence how well these approaches work. During the visit, he worked on few-shot learning, exploring how different subsets of samples of different characteristics affect the success of transfer in these approaches, and proposing a new method for selecting a subset of high-quality samples for few-shot learning.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Jos van der Velde is a senior engineer focusing on software architecture and infrastructure. I previously worked on the Croissant standard, and the AI-on-Demand platform. I'm still actively contributing to the OpenML infrastructure.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Mert Kilickaya is a former postdoctoral fellow researching autonomous visual learners that can continuously improve by extracting their own supervision from dynamic visual data streams. He advanced techniques in self-supervised continual learning, enabling AI systems to autonomously adapt and improve without relying on external annotations.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Rafael Gomes Mantovani is a former PhD student, and currently a professor at the Federal Technology University - Paraná (UTFPR), campus of Apucarana, Brazil. He researches Machine Learning, Data Mining, Meta-learning, and Automated Machine Learning/Data Science.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Jan van Rijn Jan is a former PhD student, and currently assistant professor at Leiden University. During his PhD, he co-founded and developed OpenML.org, and did research towards meta-learning and algorithm selection based on OpenML data. He co-authored the Open Access book on Metalearning. His current research activities include AutoML and Trustworthy Artificial Intelligence."
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "2025\nSculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental LearningM. O. Yildirim, E. C. Gok Yildirim, and J. VanschorenarXiv preprint arXiv:2502.14762, 2025 (2025)\n        \n        Preprint\n    \nSelf-Regulated Neurogenesis for Online Data-Incremental LearningM. O. Yildirim, E. C. Gok Yildirim, D. C. Mocanu, and J. VanschorenConference on Lifelong Learning Agents (CoLLAs) (2025)\n        \n        Preprint\n     \n        \n        Github\n    \nIn-depth sensitivity analysis of heating demand and overheating in Dutch terraced houses using interpretable machine learning.Alexis Cvetkov-Iliev, Vasilis Soulios, Luyi Xu, Günsu Merin Abbas, Evangelos Kyrou, Lisanne Havinga, Pieter Jan Hoes, Roel Loonen, and Joaquin VanschorenEnergy & Buildings, 337, no. 11561 (2025)\n        \n        PDF\n     \n        \n        Published\n    \nOn Supernet Transfer Learning for Effective Task AdaptationP. Singh and J. VanschorenConference on Lifelong Learning Agents (CoLLAs 2025) (2025)\n        \n        Preprint\n    \n2024\nAMLB: An AutoML BenchmarkP. Gijsbers, M. L. P. Bueno, S. Coors, E. LeDell, S. Poirier, J. Thomas, B. Bischl, and J. VanschorenJournal of Machine Learning Research, 25 (101), 1--65 (2024)\n        \n        PDF\n     \n        \n        Github\n     \n        \n        Published\n    \nTowards Efficient AutoML: A Pipeline Synthesis Approach Leveraging Pre-Trained Transformers for Multimodal DataA. Moharil, J. Vanschoren, P. Singh, and D. TamburriMachine Learning, 113, 7011–7053 (2024)\n        \n        PDF\n     \n        \n        Published\n    \nAdvances and Challenges in Meta-Learning: A Technical ReviewA. Vettoruzzo, M.-R. Bouguelia, J. Vanschoren, T. Rognvaldsson, and K. C. SantoshIEEE Transactions on Pattern Analysis and Machine Intelligence (2024)\n        \n        Published\n    \nCan Fairness Be Automated? Guidelines and Opportunities for Fairness-Aware AutoMLH. Weerts, F. Pfisterer, M. Feurer, K. Eggensperger, E. Bergman, N. Awad, J. Vanschoren, M. Pechenizkiy, B. Bischl, and F. HutterJournal of Artificial Intelligence Research, 79, 639--677 (2024)\n        \n        Published\n    \nCroissant: A Metadata Format for ML-Ready DatasetsM. Akhtar, O. Benjelloun, C. Conforti, P. Gijsbers, J. Giner-Miguelez, N. Jain, M. Kuchnik, Q. Lhoest, P. Marcenac, M. Maskey, P. Mattson, L. Oala, P. Ruyssen, R. Shinde, E. Simperl, G. Thomas, S. Tykhonov, J. Vanschoren, J. van der Velde, S. Vogler, and C.-J. WuAdvances in Neural Information Processing Systems (NeurIPS 2024) (2024)\n        \n        Published\n    \nTrustLLM: Trustworthiness in Large Language ModelsY. Huang, L. Sun, H. Wang, and others, and J. VanschorenInternational Conference on Machine Learning (ICML 2024), 20166--20270 (2024)\n        \n        Published\n    \nMALIBO: Meta-Learning for Likelihood-Free Bayesian OptimizationJ. Pan, S. Falkner, F. Berkenkamp, and J. VanschorenInternational Conference on Machine Learning (ICML 2024) (2024)\n        \n        PDF\n     \n        \n        Github\n     \n        \n        Published\n    \nLearning to Learn without Forgetting Using AttentionA. Vettoruzzo, J. Vanschoren, M.-R. Bouguelia, and T. RögnvaldssonConference on Lifelong Learning Agents (CoLLAs 2024) (2024)\n        \n        PDF\n     \n        \n        Published\n    \nContinual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model UpdatesM. O. Yildirim, E. C. Gok, G. Sokar, D. C. Mocanu, and J. VanschorenConference on Parsimony and Learning (CPAL 2024), 94--107 (2024)\n        \n        Github\n     \n        \n        Published\n    \nHyTAS: A Hyperspectral Image Transformer Architecture Search Benchmark and AnalysisF. Zhou, M. Kilickaya, J. Vanschoren, and R. PiaoEuropean Conference on Computer Vision (ECCV 2024) (2024)\n        \n        PDF\n     \n        \n        Github\n     \n        \n        Published\n    \nCroissant: A Metadata Format for ML-Ready DatasetsM. Akhtar, O. Benjelloun, C. Conforti, P. Gijsbers, J. Giner-Miguelez, N. Jain, M. Kuchnik, Q. Lhoest, P. Marcenac, M. Maskey, P. Mattson, L. Oala, P. Ruyssen, R. Shinde, E. Simperl, G. Thomas, V. Tykhonov, J. Vanschoren, S. Vogler, and C.-J. WuSIGMOD/PODS Workshop on Data Management for End-to-End Machine Learning (DEEM 2024), 1--6 (2024)\n        \n        Published\n    \nInternational Conference on Automated Machine LearningM. Lindauer, K. Eggensperger, R. Garnett, and J. VanschorenProceedings of Machine Learning Research, Volume 256, PMLR, 2024 (2024)\n        \n        Published\n    \nA Standardized Machine-Readable Dataset Documentation Format for Responsible AIN. Jain, M. Akhtar, J. Giner-Miguelez, R. Shinde, J. Vanschoren, S. Vogler, S. Goswami, Y. Rao, T. Santos, and L. OalaarXiv preprint arXiv:2407.16883, 2024 (2024)\n        \n        Preprint\n    \nAutomatic Combination of Sample Selection Strategies for Few-Shot LearningB. Pecher, I. Srba, M. Bielikova, and J. VanschorenarXiv preprint arXiv:2402.03038, 2024 (2024)\n        \n        Preprint\n    \nCLAMS: A System for Zero-Shot Model Selection for ClusteringP. Singh, P. Gijsbers, M. O. Yildirim, E. C. Gok, and J. VanschorenarXiv preprint arXiv:2407.11286, 2024 (2024)\n        \n        Preprint\n    \nCan Time Series Forecasting Be Automated? A Benchmark and AnalysisA. T. Sreedhara and J. VanschorenarXiv preprint arXiv:2407.16445, 2024 (2024)\n        \n        Preprint\n    \nUnsupervised Meta-Learning via In-Context LearningA. Vettoruzzo, L. Braccaioli, J. Vanschoren, and M. NowaczykarXiv preprint arXiv:2405.16124, 2024 (2024)\n        \n        Preprint\n    \nIntroducing v0.5 of the AI Safety Benchmark from MLCommonsB. Vidgen, A. Agrawal, A. M. Ahmed, V. Akinwande, N. Al-Nuaimi, N. Alfaraj, E. Alhajjar, L. Aroyo, T. Bavalatti, B. Blili-Hamelin, and J. VanschorenarXiv preprint arXiv:2404.12241, 2024 (2024)\n        \n        Preprint\n    \nContinual Learning on a Data DietE. C. Gok Yildirim, M. O. Yildirim, and J. VanschorenarXiv preprint arXiv:2410.17715, 2024 (2024)\n        \n        Preprint\n    \n2023\nSignal Quality Analysis for Long-Term ECG Monitoring Using a Health Patch in Cardiac PatientsI. Campero Jurado, I. Lorato, J. Morales, L. Fruytier, S. Stuart, P. Panditha, D. M. Janssen, N. Rossetti, N. Uzunbajakava, I. B. Serban, L. Rikken, M. de Kok, J. Vanschoren, and A. BrombacherSensors, 23 (4), Art. 2130 (2023)\n        \n        Published\n    \nOnline AutoML: An Adaptive AutoML Framework for Online LearningB. Celik, P. Singh, and J. VanschorenMachine Learning, 112 (6), 1897--1921 (2023)\n        \n        Published\n    \nAutomated Machine Learning Approach in Material Discovery of Hole Selective Layers for Perovskite Solar CellsM. O. Yildirim, E. C. Gok Yildirim, E. Eren, P. Huang, M. P. U. Haris, S. Kazim, J. Vanschoren, A. Uygun Oksuz, and S. AhmadEnergy Technology, 11 (1) (2023)\n        \n        Published\n    \nEfficient-DASH: Automated Radar Neural Network Design Across Tasks and DatasetsT. Boot, N. Cazin, W. Sanberg, and J. VanschorenIEEE Intelligent Vehicles Symposium (IV 2023), 1--7 (2023)\n        \n        Published\n    \nAn Analysis of Evolutionary Migration Models for Multi-Objective, Multi-Fidelity AutoMLI. Campero-Jurado and J. VanschorenIEEE International Conference on Systems, Man, and Cybernetics (SMC 2023), 2940--2945 (2023)\n        \n        Published\n    \nNeural Architecture Search for Visual Anomaly SegmentationT. Kerssies and J. VanschorenAutoML Conference (AutoML 2023) (2023)\n        \n        Published\n    \nAre Labels Needed for Incremental Instance Learning?M. Kilickaya and J. VanschorenIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023), 2401--2409 (2023)\n        \n        Published\n    \nDataperf: Benchmarks for Data-Centric AI DevelopmentM. Mazumder, C. Banbury, X. Yao, and others, and J. VanschorenAdvances in Neural Information Processing Systems (NeurIPS 2023) (2023)\n        \n        Published\n    \nAutoML for Outlier Detection with Optimal Transport DistancesP. Singh and J. VanschorenProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI 2023), 7175--7178 (2023)\n        \n        Published\n    \nAdaCL: Adaptive Continual LearningE. C. Gok Yildirim, M. O. Yildirim, M. Kilickaya, and J. VanschorenContinual AI Unconference (ContinualAI 2024), PMLR, 249, 15--24 (2023)\n        \n        Published\n    \nLocality-Aware Hyperspectral ClassificationF. Zhou, M. Kilickaya, and J. VanschorenThe British Machine Vision Conference (BMVC 2023) (2023)\n        \n        Published\n    \nNeurIPS’22 Cross-Domain MetaDL Challenge: Results and Lessons LearnedD. Carrión-Ojeda, M. Alam, S. Escalera, A. Farahat, D. Ghosh, T. G. Diaz, C. Gupta, I. Guyon, J. R. Ky, X. Y. Lee, X. Liu, F. Mohr, M. H. Nguyen, E. Pintelas, S. Roth, S. Schaub-Meyer, H. Sun, I. Ullah, J. Vanschoren, L. Vidyaratne, J. Wu, and X. YinNeurIPS 2022 Competition Track, 50--72 (2023)\n        \n        Published\n    \nDemocratising Artificial Intelligence to Accelerate Scientific DiscoveryJ. VanschorenIn: Artificial Intelligence in Science, OECD, 2023 (2023)\n        \n        Published\n    \nEvaluating Continual Test-Time Adaptation for Contextual and Semantic Domain ShiftsT. Kerssies, M. Kılıçkaya, and J. VanschorenarXiv preprint arXiv:2208.08767, 2023 (2023)\n        \n        Preprint\n    \nWhat Can AutoML Do for Continual Learning?M. Kılıçkaya and J. VanschorenarXiv preprint arXiv:2311.11963, 2023 (2023)\n        \n        Preprint\n    \nDMLR: Data-Centric Machine Learning Research -- Past, Present and FutureL. Oala, M. Maskey, L. Bat-Leah, A. Parrish, N. M. Gürel, T.-S. Kuo, Y. Liu, R. Dror, D. Brajovic, X. Yao, and J. VanschorenarXiv preprint arXiv:2311.13028, 2023 (2023)\n        \n        Preprint\n    \n2022\nAgroML: An Open-Source Repository to Forecast Reference Evapotranspiration in Different Geo-Climatic Conditions Using Machine Learning and Transformer-Based ModelsJ. A. Bellido-Jiménez, J. Estévez, J. Vanschoren, and A. P. García-MarínAgronomy, 12 (3), 656 (2022)\n        \n        Published\n    \nInterpretable Assessment of ST-Segment Deviation in ECG Time SeriesI. Campero Jurado, A. Fedjajevs, J. Vanschoren, and A. BrombacherSensors, 22 (13), Art. 4919 (2022)\n        \n        Published\n    \nMeta-Features for Meta-LearningA. Rivolli, L. P. F. Garcia, C. Soares, J. Vanschoren, and A. C. P. L. F. de CarvalhoKnowledge-Based Systems, 240, 108101 (2022)\n        \n        Published\n    \nTheory-Based Habit Modeling for Enhancing Behavior Prediction in Behavior Change Support SystemsC. Zhang, J. Vanschoren, A. van Wissen, D. Lakens, B. de Ruyter, and W. A. IJsselsteijnUser Modeling and User-Adapted Interaction, 23 (2022)\n        \n        Published\n    \nMulti-Fidelity Optimization Method with Asynchronous Generalized Island Model for AutoMLI. Campero-Jurado and J. VanschorenGenetic and Evolutionary Computation Conference (GECCO 2022) (2022)\n        \n        Published\n    \nMeta-Album: Multi-Domain Meta-Dataset for Few-Shot Image ClassificationI. Ullah, D. Carrión-Ojeda, S. Escalera, I. Guyon, M. Huisman, F. Mohr, J. N. van Rijn, H. Sun, J. Vanschoren, and P. A. VuAdvances in Neural Information Processing Systems 35 (NeurIPS 2022), 3232--3247 (2022)\n        \n        Published\n    \nRegularized Meta-Learning for Neural Architecture SearchR. van Gastel and J. VanschorenAutomated Machine Learning Conference (AutoML 2022) (2022)\n        \n        Published\n    \nFaster Performance Estimation for NAS with Embedding Proximity ScoreG. Franken, P. Singh, and J. VanschorenECMLPKDD Workshop on Meta-Knowledge Transfer, PMLR, 51--61 (2022)\n        \n        Published\n    \nIntroduction to the Special Section on AI in Manufacturing: Current Trends and ChallengesJ. Lijffijt, D. Gkorou, P. Van Hertum, A. Ypma, M. Pechenizkiy, and J. VanschorenACM SIGKDD Explorations Newsletter, 24 (2), 81--85 (2022)\n        \n        Published\n    \nMetalearning: Applications to Automated Machine Learning and Data MiningP. Brazdil, J. N. van Rijn, C. Soares, and J. VanschorenSpringer Nature, 2022 (2022)\n        \n        Published\n    \nAutomated Reinforcement Learning: An OverviewR. R. Afshar, Y. Zhang, J. Vanschoren, and U. KaymakarXiv preprint arXiv:2201.05000, 2022 (2022)\n        \n        Preprint\n    \nWarm-starting DARTS Using Meta-LearningM. Grobelnik and J. VanschorenarXiv preprint arXiv:2205.06355, 2022 (2022)\n        \n        Preprint\n    \n2021\nA Comparison of Optimisation Algorithms for High-Dimensional Particle and Astrophysics ApplicationsC. Balázs, M. van Beekveld, S. Caron, B. M. Dillon, B. Farmer, A. Fowlie, W. Handley, L. Hendriks, G. Jóhannesson, A. Leinweber, J. Mamužić, G. D. Martinez, P. Scott, E. C. Garrido-Merchán, R. Ruiz de Austri, Z. Searle, B. Stienen, J. Vanschoren, and M. WhiteJournal of High Energy Physics, 2021 (5), 1–46 (2021)\n        \n        Published\n    \nAdaptation Strategies for Automated Machine Learning on Evolving DataB. Celik and J. VanschorenIEEE Transactions on Pattern Analysis and Machine Intelligence, 43 (9) (2021)\n        \n        Published\n    \nOpenML-Python: An Extensible Python API for OpenMLM. Feurer, J. N. van Rijn, A. Kadra, P. Gijsbers, N. Mallik, S. Ravi, A. Mueller, J. Vanschoren, and F. HutterJournal of Machine Learning Research, 22 (100), 1–5 (2021)\n        \n        Github\n     \n        \n        Published\n    \nTransformational Machine Learning: Learning How to Learn from Many Related Scientific ProblemsI. Olier, I. O. Oghenejokpeme, T. Dash, A. Davis, L. N. Soldatova, J. Vanschoren, and R. D. KingProceedings of the National Academy of Sciences (PNAS), 118 (49) (2021)\n        \n        Published\n    \nOpenML Benchmarking SuitesB. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn, and J. VanschorenProceedings of the NeurIPS Track on Datasets and Benchmarks 2021 (2021)\n        \n        Published\n    \nMeta-Learning for Symbolic Hyperparameter DefaultsP. Gijsbers, F. Pfisterer, J. N. van Rijn, B. Bischl, and J. VanschorenGenetic and Evolutionary Computation Conference (GECCO) Companion, 2021 (2021)\n        \n        Github\n     \n        \n        Published\n    \nGAMA: A General Automated Machine Learning AssistantP. Gijsbers and J. VanschorenProceedings of ECMLPKDD 2021. Lecture Notes in Computer Science, 12461 (2021), p560-564 (2021)\n        \n        Github\n     \n        \n        Published\n    \nAdvances in MetaDL: AAAI 2021 Challenge and WorkshopA. E. Baz, I. Guyon, Z. Liu, J. van Rijn, S. Treguer, and J. VanschorenAAAI 2021 Workshop on Meta-Learning and MetaDL, PMLR 140:1--16 (2021)\n        \n        Published\n    \nVariational Task Encoders for Model-Agnostic Meta-Learning with Uncertainty Over Task DistributionsL. Schragen and J. VanschorenWorkshop on Meta-Learning @ NeurIPS 2021 (2021)\n        \n        Published\n    \nFrom Strings to Data Science: A Practical Framework for Automated String HandlingJ. van Lith and J. VanschorenWorkshop on Automated Data Science @ ECMLPKDD 2021 (2021)\n        \n        Published\n    \nOpen-Ended Learning Strategies for Learning Complex Locomotion SkillsF. Zhou and J. VanschorenWorkshop on Meta-Learning @ NeurIPS 2021 (2021)\n        \n        Published\n    \nProceedings of the Neural Information Processing Systems Track on Datasets and BenchmarksJ. Vanschoren and S. YeungNeurIPS Foundation, Curran Associates, 2021 (2021)\n        \n        Published\n    \nProceedings of the AAAI 2021 Workshop on Meta-Learning and MetaDL ChallengeI. Guyon, J. N. van Rijn, S. Treguer, and J. VanschorenPMLR, 2021 (2021)\n        \n        Published\n    \nAutomated Feature Selection and Classification for High-Dimensional Biomedical DataT. P. Beishuizen, J. Vanschoren, P. A. Hilbers, and D. BošnačkiResearchSquare, 2021 (2021)\n        \n        Published\n    \nCats, Not CAT Scans: A Study of Dataset Similarity in Transfer Learning for 2D Medical Image ClassificationI. van den Brandt, F. Fok, B. Mulders, J. Vanschoren, and V. CheplyginaarXiv preprint arXiv:2107.05940, 2021 (2021)\n        \n        Preprint\n    \nFrugal Machine LearningM. Evchenko, J. Vanschoren, H. H. Hoos, M. Schoenauer, and M. SebagarXiv preprint arXiv:2111.03731, 2021 (2021)\n        \n        Preprint\n    \nFixed-Point Quantization of Convolutional Neural Networks for Quantized Inference on Embedded PlatformsR. Goyal, J. Vanschoren, V. Van Acht, and S. NijssenarXiv preprint arXiv:2102.02147, 2021 (2021)\n        \n        Preprint\n    \n2020\nAerial Imagery Pixel-Level SegmentationM. R. Heffels and J. VanschorenarXiv preprint arXiv:2012.02024, 2020 (2020)\n        \n        Preprint\n    \nImportance of Tuning Hyperparameters of Machine Learning AlgorithmsH. Weerts, A. Mueller, and J. VanschorenarXiv preprint arXiv:2007.07588, 2020 (2020)\n        \n        Preprint\n    \n2019\nGAMA: Genetic Automated Machine Learning AssistantP. Gijsbers and J. VanschorenJournal of Open Source Software, 4 (33), 1–2 (2019)\n        \n        Github\n     \n        \n        Published\n    \nA Meta-Learning Recommender System for Hyperparameter Tuning: Predicting When Tuning Improves SVM ClassifiersR. G. Mantovani, A. L. D. Rossi, E. Alcobaca, J. Vanschoren, and A. C. P. L. F. CarvalhoInformation Sciences, 501, 193–221 (2019)\n        \n        Published\n    \nMulti-Task Learning with a Natural Metric for Quantitative Structure Activity Relationship LearningN. Sadawi, I. Olier, J. Vanschoren, J. N. van Rijn, J. Besnard, R. Bickerton, C. Grosan, L. Soldatova, and R. D. KingJournal of Cheminformatics, 11 (1), Art. 68 (2019)\n        \n        Published\n    \nBeyond Bag-of-Concepts: Vectors of Locally Aggregated ConceptsM. Grootendorst and J. VanschorenProceedings of ECMLPKDD 2019 (2019)\n        \n        Published\n    \nThe ABC of Data: A Classifying Framework for Data ReadinessL. A. Castelijns, Y. Maas, and J. VanschorenWorkshop on Automating Data Science @ ECMLPKDD 2019 (2019)\n        \n        Published\n    \nLearning to Go with the Flow: On the Adaptability of Automated Machine Learning to Evolving DataB. Celik and J. VanschorenWorkshop on Automating Data Science @ ECMLPKDD 2019 (2019)\n        \n        Published\n    \nAn Open Source AutoML BenchmarkP. Gijsbers, E. Ledell, J. Thomas, S. Poirier, B. Bischl, and J. VanschorenAutomated Machine Learning Workshop @ ICML 2019 (2019)\n        \n        Published\n    \nMeta-Learning for Algorithm and Hyperparameter Optimization with Surrogate Model EnsemblesG. Manolache and J. VanschorenMeta-Learning Workshop @ NeurIPS 2019 (2019)\n        \n        Published\n    \nLearning to Reinforcement Learn for Neural Architecture SearchJ. Robles and J. VanschorenNew in ML Symposium @ NeurIPS 2019 (2019)\n        \n        Published\n    \nHyperBoost: Hyperparameter Optimization by Gradient Boosting Surrogate ModelsJ. van Hoof and J. VanschorenWorkshop on Automating Data Science @ ECMLPKDD 2019 (2019)\n        \n        Published\n    \nMeta-LearningJ. VanschorenIn: Automatic Machine Learning: Methods, Systems, Challenges. Springer, 2019 (2019)\n        \n        Published\n    \nAutomatic Machine Learning: Methods, Systems, ChallengesF. Hutter, L. Kotthoff, and J. VanschorenSpringer, 2019 (2019)\n        \n        Published\n    \nMLSys: The New Frontier of Machine Learning SystemsA. Ratner, J. Vanschoren, and and othersarXiv preprint arXiv:1904.03257, 2019 (2019)\n        \n        Preprint\n    \n2018\nSpeeding Up Algorithm Selection via Meta-Learning and Active TestingS. Abdulrahman, P. Brazdil, J. N. van Rijn, and J. VanschorenMachine Learning, 107 (1), 79–108 (2018)\n        \n        Published\n    \nMeta-QSAR: Learning How to Learn QSARsI. Olier, N. Sadawi, G. R. Bickerton, J. Vanschoren, C. Grosan, L. Soldatova, and R. D. KingMachine Learning, 107 (1), 285–311 (2018)\n        \n        Published\n    \nThe Online Performance Estimation Framework: Heterogeneous Ensemble Learning for Data StreamsJ. N. van Rijn, G. Holmes, B. Pfahringer, and J. VanschorenMachine Learning, 107 (1), 149–176 (2018)\n        \n        Published\n    \nML Schema: Exposing the Semantics of Machine Learning with Schemas and OntologiesG. Correa Publio, D. Esteves, A. Ławrynowicz, P. Panov, L. Soldatova, T. Soru, J. Vanschoren, and H. ZafarICML 2018 Workshop on Reproducibility in Machine Learning (2018)\n        \n        Published\n    \nMeta Learning for Defaults: Symbolic DefaultsJ. N. van Rijn, F. Pfisterer, J. Thomas, A. Mueller, B. Bischl, and J. VanschorenMeta-Learning Workshop @ NeurIPS 2018 (2018)\n        \n        Published\n    \nData Augmentation Using Conditional Generative Adversarial Networks for Leaf Counting in Arabidopsis PlantsY. Zhu, M. Aoun, M. Krijn, and J. VanschorenCCCPV Workshop @ BMVC 2018 (2018)\n        \n        Published\n    \nProceedings of the 21st International Conference on Discovery ScienceL. Soldatova, J. Vanschoren, G. Papadopoulos, and M. CeciLecture Notes in Artificial Intelligence 11198, DS 2018 (2018)\n        \n        Published\n    \nMetalearning: A SurveyJ. VanschorenarXiv preprint arXiv:1810.03548, 2018 (2018)\n        \n        Preprint\n    \nTowards Reproducible Empirical Research in Meta-LearningA. Rivolli, L. Garcia, C. Soares, J. Vanschoren, and A. C. de CarvalhoarXiv preprint arXiv:1808.10406, 2018 (2018)\n        \n        Preprint\n    \n2017\nOpenML: An R Package to Connect to the Networked Machine Learning PlatformG. Casalicchio, B. Hofner, M. Lang, D. Kirchhoff, P. Kerschke, H. Seibold, J. Bossek, J. Vanschoren, and B. BischlComputational Statistics, 32 (3), 1–15 (2017)\n        \n        Published\n    \nLayered TPOT: Speeding Up Tree-Based Pipeline OptimizationP. Gijsbers, J. Vanschoren, and R. OlsonAutoML Workshop @ ECML 2017, CEUR Workshop Proceedings vol. 1998 (2017)\n        \n        Published\n    \nProceedings of the Twenty-Sixth Benelux Conference on Machine LearningW. Duivesteijn, M. Pechenizkiy, G. H. L. Fletcher, V. Menkovski, E. J. Postma, and J. VanschorenEindhoven University of Technology Eindhoven, 2017 (2017)\n        \n        Published\n    \n2016\nAn Algorithm, Implementation and Execution Ontology Design PatternA. Lawrynowicz, D. Esteves, P. Panov, T. Soru, S. Dzeroski, and J. VanschorenIn: Studies on the Semantic Web (Hitzler.P., Gangemi, A., Janowicz, K., Krisnadhi, A., Presutti, V., eds.), IOS Press (2016)\n        \n        Published\n    \nASlib: A Benchmark Library for Algorithm SelectionB. Bischl, P. Kerschke, L. Kotthoff, M. Lindauer, Y. Malitsky, A. Frechette, H. Hoos, F. Hutter, K. Leyton-Brown, K. Tierney, and J. VanschorenArtificial Intelligence, 237, 41–58 (2016)\n        \n        Published\n    \nReduction of False Arrhythmia Alarms Using Signal Selection and Machine LearningL. M. Eerikainen, J. Vanschoren, M. J. Rooijakkers, R. Vullings, and R. M. AartsPhysiological Measurement, 37 (8), 1204–1216 (2016)\n        \n        Published\n    \nTowards Understanding Online Sentiment Expression: An Interdisciplinary Approach with Subgroup Comparison and VisualizationB. Gao, B. Berendt, and J. VanschorenSocial Network Analysis and Mining, 6 (1), 68:1–68:16 (2016)\n        \n        Published\n    \nConnecting R to the OpenML Project for Open Machine LearningB. Bischl, G. Casalicchio, B. Hofner, P. Kerschke, D. Kirchhoff, M. Lang, H. Seibold, and J. VanschorenUseR! Conference (UseR 2016), 1--11 (2016)\n        \n        Published\n    \nAnticipating Habit Formation: A Psychological Computing Approach to Behavior Change SupportC. Zhang, A. van Wissen, D. Lakens, J. Vanschoren, B. De Ruyter, and W. A. IJsselsteijnProceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp 2016), 1247--1254 (2016)\n        \n        Published\n    \nHyper-Parameter Tuning of a Decision Tree Induction AlgorithmR. G. Mantovani, T. Horvath, R. Cerri, A. P. L. F. Carvalho, and J. VanschorenBrazilian Conference on Intelligent Systems (BRACIS 2016) (2016)\n        \n        Published\n    \nProceedings of the 10th International Conference on Learning and Intelligent OptimizationP. Festa, M. Sellmann, and J. VanschorenLecture Notes in Computer Science 10079, LION 2016 (2016)\n        \n        Published\n    \nProceedings of the ICML 2016 Workshop on Automatic Machine LearningF. Hutter, L. Kotthoff, and J. VanschorenPMLR, 2016 (2016)\n        \n        Published\n    \n2015\nDecreasing the False Alarm Rate of Arrhythmias in Intensive Care Using a Machine Learning ApproachL. M. Eerikainen, J. Vanschoren, M. J. Rooijakkers, R. Vullings, and R. M. AartsIEEE Computing in Cardiology, 42, 293-297 (2015)\n        \n        Published\n    \nWho is More Positive in Private? Analyzing Sentiment Differences Across Privacy Levels and Demographic Factors in Facebook Chats and PostsB. Gao, B. Berendt, and J. VanschorenIEEE/ACM Proceedings of ASONAM 2015, 605-610 (2015)\n        \n        Published\n    \nTo Tune or Not to Tune: Recommending When to Adjust SVM Hyper-Parameters via Meta-LearningR. G. Mantovani, A. L. D. Rossi, J. Vanschoren, B. Bischl, and A. C. P. L. F. CarvalhoIEEE Proceedings of IJCNN 2015, 1-8 (2015)\n        \n        Published\n    \nEffectiveness of Random Search in SVM Hyper-Parameter TuningR. G. Mantovani, A. L. D. Rossi, J. Vanschoren, B. Bischl, and A. C. P. L. F. CarvalhoIEEE Proceedings of IJCNN 2015, 1-8 (2015)\n        \n        Published\n    \nFast Algorithm Selection Using Learning CurvesJ. N. van Rijn, S. M. Abdulrahman, P. Brazdil, and J. VanschorenAdvances in Intelligent Data Analysis XIV (IDA 2015), Lecture Notes in Computer Science 9385, 298-309 (2015)\n        \n        Published\n    \nTowards a Data Science CollaboratoryJ. Vanschoren, B. Bischl, F. Hutter, M. Sebag, B. Kegl, M. Schmid, G. Napolitano, K. Wolstencroft, A. R. Williams, and N. LawrenceAdvances in Intelligent Data Analysis XIV (IDA 2015), Lecture Notes in Computer Science 9385, XIX-XXI (2015)\n        \n        Published\n    \nAlgorithm Selection via Meta-Learning and Sample-Based Active TestingS. Abdulrahman, P. Brazdil, J. N. van Rijn, and J. VanschorenMetaSel Workshop @ PKDD/ECML 2015, CEUR Workshop Proceedings 1455, 55-66 (2015)\n        \n        Published\n    \nMeta-Learning Recommendation of Default Hyper-Parameter Values for SVMs in Classification TasksR. G. Mantovani, A. L. D. Rossi, J. Vanschoren, and A. C. P. L. F. CarvalhoMetaSel Workshop @ PKDD/ECML 2015, CEUR Workshop Proceedings 1455, 80-92 (2015)\n        \n        Published\n    \nSharing RapidMiner Workflows and Experiments with OpenMLJ. N. van Rijn and J. VanschorenMetaSel Workshop @ PKDD/ECML 2015, CEUR Workshop Proceedings 1455, 93-103 (2015)\n        \n        Published\n    \nTaking Machine Learning Research Online with OpenMLJ. Vanschoren, J. N. van Rijn, and B. BischlJMLR Workshop and Conference Proceedings (BigMine 2015), 41, 1-4 (2015)\n        \n        Published\n    \nTowards a Collaborative Platform for Advanced Meta-Learning in Healthcare Predictive AnalyticsM. Vukicevic, S. Radovanovic, J. Vanschoren, G. Napolitano, and B. DelibasicMetaSel Workshop @ PKDD/ECML 2015, CEUR Workshop Proceedings 1455, 112-114 (2015)\n        \n        Published\n    \nProceedings of the 2015 International Workshop on Meta-Learning and Algorithm Selection @ ECMLPKDDJ. Vanschoren, P. Brazdil, C. G. Giraud-Carrier, and L. KotthoffCEUR Workshop Proceedings 1455, CEUR 2015 (2015)\n        \n        Published\n    \n2014\nOpenML: Networked Science in Machine LearningJoaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis TorgoACM SIGKDD Explorations Newsletter (2014)\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n    \nTowards Meta-Learning on Data StreamsJ. N. van Rijn, G. Holmes, B. Pfahringer, and J. VanschorenMetaSel Workshop @ ECAI 2014, CEUR Workshop Proceedings 1201, 37-38 (2014)\n        \n        Published\n    \nAlgorithm Selection on Data StreamsJ. N. van Rijn, G. Holmes, B. Pfahringer, and J. VanschorenProceedings of Discovery Science 2014, Lecture Notes in Computer Science 8777, 325-336 (2014)\n        \n        Published\n    \nProceedings of the 2014 International Workshop on Meta-Learning and Algorithm Selection @ ECAIJ. Vanschoren, P. Brazdil, and L. KotthoffCEUR Workshop Proceedings 1201, CEUR 2014 (2014)\n        \n        Published\n    \nReconstructing Medieval Social Networks from English and Latin ChartersA. J. Knobbe, M. Meeng, J. Vanschoren, S. Rees Jones, and S. Merlo PenningPopulation Reconstruction 2014 (2014)\n        \n        Published\n    \n2013\nA Survey of Intelligent Assistants for Data AnalysisF. Serban, J. Vanschoren, J. U. Kietz, and A. BernsteinACM Computing Surveys, 45 (3), Art. 31 (2013)\n        \n        Published\n    \nA RapidMiner Extension for Open Machine LearningJ. N. van Rijn, V. Umaashankar, S. Fischer, B. Bischl, T. Lorgo, B. Gao, P. Winter, B. Wiswedel, M. R. Berthold, and J. VanschorenProceedings of RCOMM 2013, 59-70 (2013)\n        \n        Published\n    \n2012\nExperiment Databases: A New Way to Share, Organize and Learn from ExperimentsJ. Vanschoren, H. Blockeel, B. Pfahringer, and G. HolmesMachine Learning, 87 (2), 127–158 (2012)\n        \n        Published"
  },
  {
    "objectID": "notebooks/amlb-introduction.html",
    "href": "notebooks/amlb-introduction.html",
    "title": "The AutoML Benchmark",
    "section": "",
    "text": "This blogpost is about why we set out to write our paper “AMLB: an AutoML Benchmark” (📄paper, 🤖code) and provides a brief overview of its main contributions. In the end, we will share how you can make an impact on the future of this benchmark and automated machine learning (AutoML). But before we get started, here’s the tl;dr in haiku form:"
  },
  {
    "objectID": "notebooks/amlb-introduction.html#why-do-we-need-a-standardized-benchmark",
    "href": "notebooks/amlb-introduction.html#why-do-we-need-a-standardized-benchmark",
    "title": "The AutoML Benchmark",
    "section": "Why do we need a standardized benchmark?",
    "text": "Why do we need a standardized benchmark?\nWe started the project back in 2018. Over the summer, I (Pieter Gijsbers) got together with Erin LeDell and Janek Thomas to work on three problems we had identified:\n\nin the field of automated machine learning (in 2018), different papers would rarely evaluate their methods on the same data, and\nresearchers would often make simple “user errors” when evaluating methods proposed by others, and\nthe analyses were almost exclusively focused on predictive performance.\n\nWe decided to address these problems by introducing a benchmark suite, benchmark software, and a multi-dimensional analysis. That initially culminated into an introductory paper at the AutoML Workshop at ICML 2019, and ultimately in our publication at the Journal of Machine Learning Research. Let’s have a closer look at why these problems needed solving and our resulting contributions.\n\nThe Benchmark Suite\nResearch papers evaluating their methods using different sets of data inadvertently means that results are not comparable across different papers. This makes it impossible to determine state-of-the-art or to track progress in the field overtime. It may also lead to accidentally reporting optimistic results. For example, when researchers use a dataset both during development and evaluation, design decisions are made during development which improve performance for that dataset, but it is unlikely that it improves performance for unseen datasets equally. Thus, performance on that dataset is likely optimistic (i.e., better) compared to what you would expect on unseen datasets.\nEven if datasets used in evaluation are not used during development, the researchers’ bias on which type of data they find important, such as biomedical data or big data, may lead to a similar “accidental cherry picking”. That type of data is reasonably assumed to be present during both development and evaluation, and other types of data for which the methods were not developed may be absent also in evaluation. This means that areas where methods may have blind spots are not evaluated and not reported on, giving an incomplete picture of the method’s strengths and weaknesses.\n\n\n\nHaving a curated set of datasets for evaluation managed by the community and carefully chosen to represent various data domains and characteristics should alleviate these problems. That is why we propose the initial version of such a standardized benchmark suite. The benchmarking suite contains 71 classification and 33 regression datasets that come from many different domains and exhibit many different dataset characteristics. We hope that with a benchmarking suite of this size and variety, we can trust in the generalizability of the results obtained.\n\n\nThe Benchmark Software\nAnother consequence of using a different set of data for each paper is that researchers need to evaluate the methods of others on their data. There are multiple examples where a method’s performance reported in papers are, for lack of a better word, the result of misunderstandings (or “user errors”). For example, a paper introducing method \\(X\\) and comparing it against state-of-the-art framework \\(Y\\) would report a failure of framework \\(Y\\) on dataset \\(D\\). In several cases we could diagnose the problem that caused the failure of framework \\(Y\\) on dataset \\(D\\) directly from the paper or a few minutes of inspecting the code. Examples would include using an out-of-date release or a wrong configuration of framework \\(Y\\).\n\n\n\nLuckily, as computer scientists, we have a wonderful tool in our belt to reduce human error: automation. That’s why we built standardized benchmarking software complete with developer-friendly AutoML framework integrations.\nWith the paper we released our open source benchmarking software. It allows you to run an AutoML evaluation with a single command. A simple python runbenchmark.py autosklearn openml/s/271 1h8c is all you need to evaluate autosklearn on our entire classification benchmark (OpenML suite 271). The software takes care of installing autosklearn, (down)loading the data and providing it to autosklearn, configuring autosklearn with a 1 hour time constraints with a limit of 8 CPU cores (hence 1h8c), monitoring its progress, and processing the predictions of the final model.\nThis level of automation means that no one is accidentally training on a test set. No one accidentally uses different parallelization for different AutoML frameworks, and no one accidentally optimizes for the wrong metrics. It’s a great step forward for fair comparisons. At least, that’s the idea. In many cases we make good on the promise, but we are working with a moving target (new framework releases) and bugs happen (see “The Future” below).\nThe way we achieve this is by collaborating with AutoML framework developers. We built a strong shared framework that takes care of bootstrapping the experimental setup. It has generic functionalities for (down)loading data, installing dependencies, keeping track of experiments, processing results, and so on. Then, for each framework we have a minimal integration module which requires an installation script and a Python script which takes as input the data and task description and is expected to produce predictions as output. These integration scripts are often developed by, or in collaboration with, the authors of the AutoML frameworks which significantly reduces the chance that frameworks are used or configured incorrectly.\n\n\nA Multi-Dimensional Analysis\nThe last problem we identified was a single-minded analysis of the results. AutoML frameworks were compared (almost) exclusively on predictive performance. While this is a very important aspect of the final model, it provides an incomplete picture. Depending on the application that the model is trained for, other aspects like fast inference time or interpretability may be important or even required. In those cases, being able to assess the trade-off between predictive performance and other model dimensions is crucial. The characteristics of the AutoML framework itself may also be important: is it robust? is it configurable? does it adhere to resource constraints? That’s why we advocate for a multi-dimensional analysis of the results.\n\n\n\nIn our paper we evaluate 9 AutoML frameworks on our benchmarking suite and analyze the results. When analyzing predictive performance, we answer questions like: Which framework ranks best? Is it significant? How does that translate to absolute and relative differences? Do results hold across all data characteristics, or do we see some AutoML frameworks which work particularly well on datasets with certain characteristics?\nBut we also look beyond predictive accuracy. We inspect the learned models’ inference time, and discuss its trade-off with predictive performance. We also investigate when AutoML frameworks fail and why: Is it triggered by the data? Is it misuse of resources? Do they adhere to the given time constraints?\nThe experiments in the paper are from 2023, so results would indubitably be different for frameworks today. Bugs are fixed, better methods for building better models are developed. However, we hope the paper serves as an inspiration and example for providing a more thorough multi-dimensional analysis.\n\n\nChallenges and Limitations\nFor more background, examples, challenges, and references you will want to have a look at the paper. It presents ideas presented here in more depth but also includes additional information on e.g., limitations of the benchmark, and it also contains the results and analysis of 9 frameworks on 104 datasets!"
  },
  {
    "objectID": "notebooks/amlb-introduction.html#the-future",
    "href": "notebooks/amlb-introduction.html#the-future",
    "title": "The AutoML Benchmark",
    "section": "The Future",
    "text": "The Future\nOvertime the landscape of data problems changes, as do the capabilities of AutoML frameworks, and the benchmarking suite should be updated to reflect that. One glaring ommission in the benchmark (in my personal opinion) is the lack of text data. In the original benchmarking suite presented in the paper, all datasets have strictly numerical or categorical data. This is for historical reasons, as a significant number of the AutoML frameworks in 2018 did not handle text data natively. But text data is ubiquitous, modern AutoML frameworks are well-equipped to deal with it, so it’s time we reflect that in the benchmark.\nMoreover, there is nothing uniquely “AutoML” about the AutoML benchmark. It benchmarks anything for which you write an integration script that takes as input a task and constraint description, and provides predictions as output. Sure, many algorithms may not deal with various dataset characteristics out-of-the-box (e.g., text data may need to be encoded) and the method may not adhere to time constraints, but that doesn’t mean the software can provide much of the same benefits of automation to evaluating “regular” ML algorithms (in fact, TabRepo used AMLB). We hope to explore this further and potentially position AMLB for more general tabular ML benchmarking.\nAnd last but not least, though perhaps not as sexy, is the brunt of software engineering work. As time goes on, AutoML frameworks have new releases, people use different hardware, and AutoML is used for new problems, which means we are always working with a moving target. The benchmarking software needs to be maintained to work with modern stacks, extended to allow evaluation of different problem types, and incidental bugs need to be squashed. On top of that, we are continuously exploring ways to make the software easier to use for both developers and researchers.\nThe benchmark isn’t perfect, or even finished. It’s a start. And we invite you to collaborate with us.\nThere are many ways to contribute. The easiest way is to help is to leave a ⭐️ on our GitHub project, talk about the AutoML benchmark, and use it in your work. There are also a number of no-code contributions that help us out greatly:\n\n🤝 Help people on our GitHub issue tracker\n📚 Improve our documentation\n🤓 Identify interesting new datasets\n\nAnd finally, we very much welcome code contributions. 🧑‍💻\n\nThis post was written by Pieter Gijsbers and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/adacl-introduction.html",
    "href": "notebooks/adacl-introduction.html",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Elif Ceren Gok Yildirim, Murat Onur Yildirim, Mert Kilickaya, Joaquin Vanschoren\n\n\nIn the rapidly evolving field of machine learning, one challenge remains particularly persistent: the ability to continually learn new information without forgetting previously acquired knowledge. This forgetting problem is generally referred to in the field as “catastrophic forgetting”. Catastrophic forgetting occurs because, while models are learning new tasks, they lose the ability to recall information about previously learned tasks. This happens because the model’s parameters are updated to fit the new data, which can overwrite the representations of the old data.\n\n\n\nClass-Incremental Learning is one challenging scenario in CL which aims to update a model’s parameters and expand the classification layer to learn new categories while maintaining its accuracy on previously observed classes. Then during the test time, it assumes not to have access to task ids.\n\n\n\nThree major approaches have been explored to mitigate this issue:\n\nRegularization-based Methods: These techniques prevent abrupt shifts in the neural network weights by applying penalties that stabilize important parameters.\nReplay-based Methods: These involve storing a subset of training data and replaying it during the learning of new tasks to maintain previous knowledge.\nArchitecture-based Methods: These methods adapt the network structure, either by expanding it or isolating parts of it to retain past information.\n\n\n\n\nAll these CL methods bring additional hyperparameters to the learning process. For example: regularization strength, learning rate, memory size, etc. Current methods define these hyperparameters to a fixed value and assume that using the same hyperparameter setting will be sufficient for learning all different subsequent tasks. However, this approach does not reflect a realistic scenario. We can imagine 2 realistic scenarios:\n##High Plasticity- Low Stability: Let’s say you have learned large animal categories and now you will have to learn vehicle categories. In this case, you would go for mid-to low regularization strength to learn the new categories. In addition to that, you might want to store many exemplars from the large animal category in the memory buffer because you will probably forget that since the vehicle category will change the model’s parameters a lot.\n\n\n\n\n\n\nThis time you have learned small vehicle categories and now you will have to learn large vehicle categories. In this case, you would think that these two subsequent tasks are intuitively similar so you don’t need to lower regularization strength because with high regularization, model weights will not change too much and current weights will probably be sufficient to learn the large vehicle category. Also, you might not want to store many exemplars from small vehicle categories because you did not change the model’s weights too much and you don’t want to use your budget unnecessarily.\n\n\n\nAdaCL (Adaptive Continual Learning) introduces a novel approach where learning rate, regularization, and memory size are treated as dynamic, tunable variables. Instead of being fixed, these parameters adapt according to the learner’s current condition and the complexity of the task.\nBut how to find optimum hyperparameter values?\nSearch Algorithm = Tree Parzen Estimators\nValidation set = some amount of data from the current task + some amount of data from previous tasks.\nTo achieve this, we use an AutoML tool which is Optuna and as a search algorithm, we apply Bayesian Optimization Tree Parzen Estimators. AdaCL searches and predicts the optimal values for these hyperparameters by evaluating the model’s performance on the validation set.\n\n\n\n\n\n\nOur experiments utilized two well-known datasets: CIFAR100 and MiniImageNet. Each dataset contains images from 100 different categories, and we trained all models with 10 tasks, with each task containing 10 classes. Standard metrics for evaluation were used:\nAccuracy (ACC) measures the final accuracy averaged over all tasks.\nBackward Transfer (BWT) measures the average accuracy change of each task after learning new tasks.\nWe compared AdaCL with various baseline methods including EWC, LwF, iCaRL, and WA but the GOOD NEWS is: it is a plugged-in approach that means you can actually combine it with other methods that require better hyperparameter setting in CL.\n\n\n\nKey Findings:\n\nAdaCL boosts the performance of regularization-based methods.\nAdaCL yields better resource usage by leading to less memory size with the same performance.\n\n\n\n\n\n\n\nAdaCL treats crucial hyperparameters as adaptable, addressing the challenges of CIL more effectively than traditional fixed-parameter approaches. We aim to underscore the importance of flexible, dynamic adaptation in the CL scenario. For those interested in further details, the full paper provides an in-depth analysis of the methods, experimental setup, and results. Read the full paper here.\n\nThis post was written by Elif Ceren Gok Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#understanding-continual-learning-and-the-catastrophic-forgetting",
    "href": "notebooks/adacl-introduction.html#understanding-continual-learning-and-the-catastrophic-forgetting",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "In the rapidly evolving field of machine learning, one challenge remains particularly persistent: the ability to continually learn new information without forgetting previously acquired knowledge. This forgetting problem is generally referred to in the field as “catastrophic forgetting”. Catastrophic forgetting occurs because, while models are learning new tasks, they lose the ability to recall information about previously learned tasks. This happens because the model’s parameters are updated to fit the new data, which can overwrite the representations of the old data."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#understanding-class-incremental-learning",
    "href": "notebooks/adacl-introduction.html#understanding-class-incremental-learning",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Class-Incremental Learning is one challenging scenario in CL which aims to update a model’s parameters and expand the classification layer to learn new categories while maintaining its accuracy on previously observed classes. Then during the test time, it assumes not to have access to task ids."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#solutions-for-solving-catastrophic-forgetting",
    "href": "notebooks/adacl-introduction.html#solutions-for-solving-catastrophic-forgetting",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Three major approaches have been explored to mitigate this issue:\n\nRegularization-based Methods: These techniques prevent abrupt shifts in the neural network weights by applying penalties that stabilize important parameters.\nReplay-based Methods: These involve storing a subset of training data and replaying it during the learning of new tasks to maintain previous knowledge.\nArchitecture-based Methods: These methods adapt the network structure, either by expanding it or isolating parts of it to retain past information."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#motivation-for-adacl",
    "href": "notebooks/adacl-introduction.html#motivation-for-adacl",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "All these CL methods bring additional hyperparameters to the learning process. For example: regularization strength, learning rate, memory size, etc. Current methods define these hyperparameters to a fixed value and assume that using the same hyperparameter setting will be sufficient for learning all different subsequent tasks. However, this approach does not reflect a realistic scenario. We can imagine 2 realistic scenarios:\n##High Plasticity- Low Stability: Let’s say you have learned large animal categories and now you will have to learn vehicle categories. In this case, you would go for mid-to low regularization strength to learn the new categories. In addition to that, you might want to store many exemplars from the large animal category in the memory buffer because you will probably forget that since the vehicle category will change the model’s parameters a lot."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#high-stability---low-plasticity",
    "href": "notebooks/adacl-introduction.html#high-stability---low-plasticity",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "This time you have learned small vehicle categories and now you will have to learn large vehicle categories. In this case, you would think that these two subsequent tasks are intuitively similar so you don’t need to lower regularization strength because with high regularization, model weights will not change too much and current weights will probably be sufficient to learn the large vehicle category. Also, you might not want to store many exemplars from small vehicle categories because you did not change the model’s weights too much and you don’t want to use your budget unnecessarily."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#introducing-adacl",
    "href": "notebooks/adacl-introduction.html#introducing-adacl",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "AdaCL (Adaptive Continual Learning) introduces a novel approach where learning rate, regularization, and memory size are treated as dynamic, tunable variables. Instead of being fixed, these parameters adapt according to the learner’s current condition and the complexity of the task.\nBut how to find optimum hyperparameter values?\nSearch Algorithm = Tree Parzen Estimators\nValidation set = some amount of data from the current task + some amount of data from previous tasks.\nTo achieve this, we use an AutoML tool which is Optuna and as a search algorithm, we apply Bayesian Optimization Tree Parzen Estimators. AdaCL searches and predicts the optimal values for these hyperparameters by evaluating the model’s performance on the validation set."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#experimental-setup",
    "href": "notebooks/adacl-introduction.html#experimental-setup",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Our experiments utilized two well-known datasets: CIFAR100 and MiniImageNet. Each dataset contains images from 100 different categories, and we trained all models with 10 tasks, with each task containing 10 classes. Standard metrics for evaluation were used:\nAccuracy (ACC) measures the final accuracy averaged over all tasks.\nBackward Transfer (BWT) measures the average accuracy change of each task after learning new tasks.\nWe compared AdaCL with various baseline methods including EWC, LwF, iCaRL, and WA but the GOOD NEWS is: it is a plugged-in approach that means you can actually combine it with other methods that require better hyperparameter setting in CL."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#results",
    "href": "notebooks/adacl-introduction.html#results",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Key Findings:\n\nAdaCL boosts the performance of regularization-based methods.\nAdaCL yields better resource usage by leading to less memory size with the same performance."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#conclusion",
    "href": "notebooks/adacl-introduction.html#conclusion",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "AdaCL treats crucial hyperparameters as adaptable, addressing the challenges of CIL more effectively than traditional fixed-parameter approaches. We aim to underscore the importance of flexible, dynamic adaptation in the CL scenario. For those interested in further details, the full paper provides an in-depth analysis of the methods, experimental setup, and results. Read the full paper here.\n\nThis post was written by Elif Ceren Gok Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/serena-introduction.html",
    "href": "notebooks/serena-introduction.html",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "Murat Onur Yildirim, Elif Ceren Gok Yildirim, Decebal Constantin Mocanu, Joaquin Vanschoren.\nTL;DR: This blog post dives into our recent paper “Self-Regulated Neurogenesis for Online Data-Incremental Learning” (📄 Paper, 🤖 Code) published in CoLLAs 2025. SERENA is a brain-inspired continual learning method that detects concept drift and allocates dedicated sparse neural paths called concept cells for each task, enabling efficient and replay-free learning without forgetting. It outperforms state-of-the-art and even offline supervised training in online data-incremental scenarios. Read on for a brief overview of our key findings 😊\n\n\n\nTraditional neural networks are great at recognizing patterns when all data is presented at once. But in the real world, data arrives bit by bit, like a movie watched frame by frame. Most AI systems fall apart here, suffering from catastrophic forgetting, where learning something new erases what was previously learned. Even worse, many current solutions rely on replaying old data (which isn’t always possible due to privacy) or growing the model’s architecture indefinitely (which becomes inefficient fast).\n\n\n\nOur new approach takes its inspirations directly from the brain’s remarkable ability of “self-regulated neurogenesis” to learn and adapt throughout life. It promises to revolutionize how AI handles dynamic, real-world data streams. Specifically, it detects new concepts on its own and carves paths called concept cells which are embedded in a single over-parameterized network, meaning no model growth, no replay buffer. Once a concept is mastered, its corresponding concept cell is “frozen”, effectively safeguarding that knowledge from future interference.\n\n\n\n\n\n\nSERENA’s design allows it to operate in real-time, online data-incremental learning scenarios without needing explicit task identifiers or multiple training epochs. Here’s a simplified look at its core mechanisms:\n\nZero-Cost Path Allocation: When a new concept is detected in the data stream, SERENA allocates a new specialized neural path. This is done efficiently using zero-cost random unstructured pruning.\nDrift Detection: If the model notices it’s struggling (via accuracy dips), it knows something new is happening and carves a new concept cell.\nKnowledge Preservation: After a concept is learned, its dedicated concept cell is frozen, preventing catastrophic forgetting. This means the model doesn’t need to “re-learn” old information but can benefit from the existing knowledge while learning the new ones.\nRecency-Effect with Ensemble Inference: When making decisions, SERENA doesn’t rely on a single neural path. Instead, it employs a neuro-inspired ensemble approach, giving more weight to recently acquired knowledge while still integrating past information. This “recency effect” enhances adaptability to evolving data distributions.\n\n\n\n\n\n\n\nAcross over ten benchmarks, SERENA doesn’t just outperform all continual learning baselines, it even beats offline supervised batch learning which sees all data in advance and gets to train on it many times. Let that sink in: a one-pass, online, biologically inspired method outperforms traditional training.\nThis is a monumental achievement, as offline learning typically has the luxury of revisiting data multiple times. SERENA achieves this superior performance without the need for storing subsets of data for replay or expanding the network architecture, addressing critical concerns around computational overhead,privacy, and memory limitations, so it is game-changer and highly “Real-World Ready” 🌍.\n\n\n\n\n\n\nSERENA is more than a clever acronym—it represents a significant leap forward in making AI truly adaptive and intelligent. By drawing inspiration from how our brains learn, it shows that continual learning in AI doesn’t have to be clunky, memory-hungry, or brittle. It can be elegant. Efficient. And maybe… just a little bit human. If you want to know more, you can check the paper 😊\nStay curious, keep exploring, and let’s continue pushing the boundaries of what intelligent systems can achieve! 🧠🚀\n\nThis post was written by Murat Onur Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/serena-introduction.html#the-problem-ais-short-term-memory",
    "href": "notebooks/serena-introduction.html#the-problem-ais-short-term-memory",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "Traditional neural networks are great at recognizing patterns when all data is presented at once. But in the real world, data arrives bit by bit, like a movie watched frame by frame. Most AI systems fall apart here, suffering from catastrophic forgetting, where learning something new erases what was previously learned. Even worse, many current solutions rely on replaying old data (which isn’t always possible due to privacy) or growing the model’s architecture indefinitely (which becomes inefficient fast)."
  },
  {
    "objectID": "notebooks/serena-introduction.html#serena-self-regulated-neurogenesis-meets-continual-learning",
    "href": "notebooks/serena-introduction.html#serena-self-regulated-neurogenesis-meets-continual-learning",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "Our new approach takes its inspirations directly from the brain’s remarkable ability of “self-regulated neurogenesis” to learn and adapt throughout life. It promises to revolutionize how AI handles dynamic, real-world data streams. Specifically, it detects new concepts on its own and carves paths called concept cells which are embedded in a single over-parameterized network, meaning no model growth, no replay buffer. Once a concept is mastered, its corresponding concept cell is “frozen”, effectively safeguarding that knowledge from future interference."
  },
  {
    "objectID": "notebooks/serena-introduction.html#how-serena-works-its-magic",
    "href": "notebooks/serena-introduction.html#how-serena-works-its-magic",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "SERENA’s design allows it to operate in real-time, online data-incremental learning scenarios without needing explicit task identifiers or multiple training epochs. Here’s a simplified look at its core mechanisms:\n\nZero-Cost Path Allocation: When a new concept is detected in the data stream, SERENA allocates a new specialized neural path. This is done efficiently using zero-cost random unstructured pruning.\nDrift Detection: If the model notices it’s struggling (via accuracy dips), it knows something new is happening and carves a new concept cell.\nKnowledge Preservation: After a concept is learned, its dedicated concept cell is frozen, preventing catastrophic forgetting. This means the model doesn’t need to “re-learn” old information but can benefit from the existing knowledge while learning the new ones.\nRecency-Effect with Ensemble Inference: When making decisions, SERENA doesn’t rely on a single neural path. Instead, it employs a neuro-inspired ensemble approach, giving more weight to recently acquired knowledge while still integrating past information. This “recency effect” enhances adaptability to evolving data distributions."
  },
  {
    "objectID": "notebooks/serena-introduction.html#performance-that-surpasses-the-best",
    "href": "notebooks/serena-introduction.html#performance-that-surpasses-the-best",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "Across over ten benchmarks, SERENA doesn’t just outperform all continual learning baselines, it even beats offline supervised batch learning which sees all data in advance and gets to train on it many times. Let that sink in: a one-pass, online, biologically inspired method outperforms traditional training.\nThis is a monumental achievement, as offline learning typically has the luxury of revisiting data multiple times. SERENA achieves this superior performance without the need for storing subsets of data for replay or expanding the network architecture, addressing critical concerns around computational overhead,privacy, and memory limitations, so it is game-changer and highly “Real-World Ready” 🌍."
  },
  {
    "objectID": "notebooks/serena-introduction.html#final-thoughts-and-wrapping-it-up",
    "href": "notebooks/serena-introduction.html#final-thoughts-and-wrapping-it-up",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "SERENA is more than a clever acronym—it represents a significant leap forward in making AI truly adaptive and intelligent. By drawing inspiration from how our brains learn, it shows that continual learning in AI doesn’t have to be clunky, memory-hungry, or brittle. It can be elegant. Efficient. And maybe… just a little bit human. If you want to know more, you can check the paper 😊\nStay curious, keep exploring, and let’s continue pushing the boundaries of what intelligent systems can achieve! 🧠🚀\n\nThis post was written by Murat Onur Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/cldst-introduction.html",
    "href": "notebooks/cldst-introduction.html",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "Murat Onur Yildirim, Elif Ceren Gok Yildirim, Ghada Sokar, Decebal Constantin Mocanu, Joaquin Vanschoren.\nTL;DR: This blog post dives into our recent paper “Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates” (📄 Paper, 🤖 Code) published in CPAL 2024. We explore how Continual Learning can benefit from Dynamic Sparse Training, a process that closely mirrors the brain’s ability to constantly rewire and optimize its connections. Read on for a brief overview of our key findings 😊\n\nHey there!\nHave you ever wondered how our brains learn new skills without forgetting the old ones? Imagine learning a new language while still chatting away in your native tongue, or watching yourself grow from crawling to walking, running, and even biking—all while never losing that ability to crawl! It’s truly fascinating, and yet, teaching our deep learning models to do the same remains an exciting challenge.\n\n\nTraditional neural networks tend to overwrite older information when learning new tasks, which isn’t very brain-like at all. This major hurdle is called catastrophic forgetting. Imagine if every time you learned a new recipe, you suddenly couldn’t remember your grandma’s secret and delicious recipe anymore! That’s where continual learning (CL) steps in—allowing models to learn from a stream of data without letting past knowledge vanish into thin air.\n\n\n\n\n\n\nIn neuroscience, it’s well known that our brain is always under a change and on the move! Our synaptic connections are constantly being reshaped or kept steady based on what we experience—a delightful dance of plasticity and stability that fuels our lifelong ability to learn and adapt. Picture your brain as a vibrant, ever-changing city where new pathways light up and old ones evolve based on your experiences.\n\n\n\nTo mimic this amazing ability, a technique called Dynamic Sparse Training (DST) offers an artificial version of the plasticity and the stability, ensuring that our neural networks remain both efficient and adaptable over time. Imagine starting with an overparameterized network, like a bustling metropolis with way more roads than you actually need. But instead of keeping all these roads open, DST acts as an ingenious city planner: it decides which roads to close down (prune) and which new ones to build (regrow) so that traffic flows optimally—all while staying within a fixed budget.\n\n\nWe kick things off by initializing our network using one of two methods: - Uniform Initialization: Every layer gets an equal share of connections, kind of like giving every neighborhood the same number of streets. - Erdős-Rényi Kernel (ERK) Initialization: Instead of treating all layers equally, ERK smartly allocates more connections to the “busy” areas (the layers with more parameters) and fewer to the “quiet” ones. This is a bit like investing more in the main highways that keep the city moving.\n\n\n\nAfter some initial training, the network doesn’t just sit idle but actively refines itself. Here’s how it works: - Pruning: Based on the magnitude of the weights, the network prunes away a fixed number (which can be scheduled) of the least important connections. Think of it as cutting off the rarely used, winding side streets to save resources. - Regrowth: To maintain the same overall sparsity level for each task, the network then regrows exactly same amount of connections. And guess what? There are three main approaches to choose which connections to regrow: - Random: Sometimes, a bit of randomness helps explore new possibilities, like testing out an unexpected shortcut. - Unfired: Alternatively, the model can explore the connections that is never tried or checked before. - Gradient-based: Finally, it can look at the gradient or momentum signals that indicate which potential connections could boost performance—and regrow those most promising ones.\n\n\n\nWhat’s truly fascinating in this approach is that, even though the network is carving out different “subnetworks” for each new task, it also allows for sharing connections between tasks. This means that if two tasks are similar, they can use the same neural “road,” which enhances knowledge sharing and makes learning even more efficient. It’s like having a communal library where everyone benefits from the same resources!\n\n\n\n\n\n\n\nNow, combine DST with Continual Learning (CL)—a setup where a single network learns a series of tasks over time without forgetting earlier ones, just like how you can learn to cook a new recipes without forgetting grandma’s special.\nTraditionally, many systems tackle each new task with a separate network or by keeping extra data around. But here, DST is applied to one overparameterized network that evolves over time. As new tasks come along, the network dynamically adjusts itself by pruning away unused connections and regrowing new ones, while allowing to share common connections to build on past knowledge.\n\n\n\nFor those of you who love the technical nitty-gritty, here’s a quick summary of the key findings from our study: - Initialization Matters: At low to moderate sparsity levels (around 80–90%), the ERK initialization leverages the network’s capacity more efficiently by assigning fewer connections to narrow layers. This helps in learning incremental tasks effectively. At higher sparsities, ERK still doing its job when the right growth strategy is selected. - Growth Strategy Dynamics: The choice of growth strategy is closely tied to the initialization and the degree of sparsity. While gradient-based and momentum-based growth methods excel at high sparsity by carefully selecting promising connections, random growth performs competitively at lower sparsity levels where there’s less room for exploration. - Adaptive Approaches are Promising: There’s no one-size-fits-all DST setup. Even a simple adaptive strategy that switches between growth methods (like using random growth initially and shifting to gradient-based growth later) can boost performance compared to sticking with a fixed strategy. - Shared Connections Enhance Knowledge Transfer: Allowing the network to share connections between tasks not only saves resources but also boosts overall learning by transferring knowledge from previous tasks, much like how our brains reuse familiar circuits when learning something new.\n\n\n\n\n\n\nBy starting with a clever initialization, then iteratively pruning and regrowing a fixed number of connections based on weight magnitude and gradient signals, DST transforms a single, overparameterized network into a versatile, continually learning powerhouse. And the cherry on top? Allowing subnetworks to share connections across different tasks means that the network not only learns continuously but also builds upon past knowledge, much like our own brains do.\nIn this study, we dive deep into the various ways to run DST in a continual learning setup, offering both technical insights and inspiration for future research. We believe it is a beautiful blend of neuroscience and engineering—a step closer to creating AI that learns and adapts just like we do. Again, if you want to know more, you can check the paper 😊\nStay curious, keep exploring, and let’s continue pushing the boundaries of what intelligent systems can achieve! 🧠🚀\n\nThis post was written by Murat Onur Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#the-challenge-catastrophic-forgetting",
    "href": "notebooks/cldst-introduction.html#the-challenge-catastrophic-forgetting",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "Traditional neural networks tend to overwrite older information when learning new tasks, which isn’t very brain-like at all. This major hurdle is called catastrophic forgetting. Imagine if every time you learned a new recipe, you suddenly couldn’t remember your grandma’s secret and delicious recipe anymore! That’s where continual learning (CL) steps in—allowing models to learn from a stream of data without letting past knowledge vanish into thin air."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#neuroscience-natures-blueprint",
    "href": "notebooks/cldst-introduction.html#neuroscience-natures-blueprint",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "In neuroscience, it’s well known that our brain is always under a change and on the move! Our synaptic connections are constantly being reshaped or kept steady based on what we experience—a delightful dance of plasticity and stability that fuels our lifelong ability to learn and adapt. Picture your brain as a vibrant, ever-changing city where new pathways light up and old ones evolve based on your experiences."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#a-smart-way-to-build-a-brain",
    "href": "notebooks/cldst-introduction.html#a-smart-way-to-build-a-brain",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "To mimic this amazing ability, a technique called Dynamic Sparse Training (DST) offers an artificial version of the plasticity and the stability, ensuring that our neural networks remain both efficient and adaptable over time. Imagine starting with an overparameterized network, like a bustling metropolis with way more roads than you actually need. But instead of keeping all these roads open, DST acts as an ingenious city planner: it decides which roads to close down (prune) and which new ones to build (regrow) so that traffic flows optimally—all while staying within a fixed budget.\n\n\nWe kick things off by initializing our network using one of two methods: - Uniform Initialization: Every layer gets an equal share of connections, kind of like giving every neighborhood the same number of streets. - Erdős-Rényi Kernel (ERK) Initialization: Instead of treating all layers equally, ERK smartly allocates more connections to the “busy” areas (the layers with more parameters) and fewer to the “quiet” ones. This is a bit like investing more in the main highways that keep the city moving.\n\n\n\nAfter some initial training, the network doesn’t just sit idle but actively refines itself. Here’s how it works: - Pruning: Based on the magnitude of the weights, the network prunes away a fixed number (which can be scheduled) of the least important connections. Think of it as cutting off the rarely used, winding side streets to save resources. - Regrowth: To maintain the same overall sparsity level for each task, the network then regrows exactly same amount of connections. And guess what? There are three main approaches to choose which connections to regrow: - Random: Sometimes, a bit of randomness helps explore new possibilities, like testing out an unexpected shortcut. - Unfired: Alternatively, the model can explore the connections that is never tried or checked before. - Gradient-based: Finally, it can look at the gradient or momentum signals that indicate which potential connections could boost performance—and regrow those most promising ones.\n\n\n\nWhat’s truly fascinating in this approach is that, even though the network is carving out different “subnetworks” for each new task, it also allows for sharing connections between tasks. This means that if two tasks are similar, they can use the same neural “road,” which enhances knowledge sharing and makes learning even more efficient. It’s like having a communal library where everyone benefits from the same resources!"
  },
  {
    "objectID": "notebooks/cldst-introduction.html#continual-learning-the-ultimate-test",
    "href": "notebooks/cldst-introduction.html#continual-learning-the-ultimate-test",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "Now, combine DST with Continual Learning (CL)—a setup where a single network learns a series of tasks over time without forgetting earlier ones, just like how you can learn to cook a new recipes without forgetting grandma’s special.\nTraditionally, many systems tackle each new task with a separate network or by keeping extra data around. But here, DST is applied to one overparameterized network that evolves over time. As new tasks come along, the network dynamically adjusts itself by pruning away unused connections and regrowing new ones, while allowing to share common connections to build on past knowledge."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#a-technical-snapshot",
    "href": "notebooks/cldst-introduction.html#a-technical-snapshot",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "For those of you who love the technical nitty-gritty, here’s a quick summary of the key findings from our study: - Initialization Matters: At low to moderate sparsity levels (around 80–90%), the ERK initialization leverages the network’s capacity more efficiently by assigning fewer connections to narrow layers. This helps in learning incremental tasks effectively. At higher sparsities, ERK still doing its job when the right growth strategy is selected. - Growth Strategy Dynamics: The choice of growth strategy is closely tied to the initialization and the degree of sparsity. While gradient-based and momentum-based growth methods excel at high sparsity by carefully selecting promising connections, random growth performs competitively at lower sparsity levels where there’s less room for exploration. - Adaptive Approaches are Promising: There’s no one-size-fits-all DST setup. Even a simple adaptive strategy that switches between growth methods (like using random growth initially and shifting to gradient-based growth later) can boost performance compared to sticking with a fixed strategy. - Shared Connections Enhance Knowledge Transfer: Allowing the network to share connections between tasks not only saves resources but also boosts overall learning by transferring knowledge from previous tasks, much like how our brains reuse familiar circuits when learning something new."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#bringing-all-together-and-wrapping-it-up",
    "href": "notebooks/cldst-introduction.html#bringing-all-together-and-wrapping-it-up",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "By starting with a clever initialization, then iteratively pruning and regrowing a fixed number of connections based on weight magnitude and gradient signals, DST transforms a single, overparameterized network into a versatile, continually learning powerhouse. And the cherry on top? Allowing subnetworks to share connections across different tasks means that the network not only learns continuously but also builds upon past knowledge, much like our own brains do.\nIn this study, we dive deep into the various ways to run DST in a continual learning setup, offering both technical insights and inspiration for future research. We believe it is a beautiful blend of neuroscience and engineering—a step closer to creating AI that learns and adapts just like we do. Again, if you want to know more, you can check the paper 😊\nStay curious, keep exploring, and let’s continue pushing the boundaries of what intelligent systems can achieve! 🧠🚀\n\nThis post was written by Murat Onur Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "2025\nELLIOT: European Large Open Multi-modal Foundation ModelsEU Horizon Europe (2025-2030)ELLIOT aims to develop the next generation of Multimodal Space-Time Foundation Models. More info soon!\n        \n        Project website\n    \nOpenEuroLLM: Open European Family of Large Language ModelsEU Horizon Europe (2025-2029)OpenEuroLLM aims to build an entirely open family of Large Language Models, publishing all data, code, and models in an open way.\n        \n        Project website\n    \nLLMs4EU: Large Language Models for the European UnionDigital Europe (2025-2029)LLMs4EU is OpenEuroLLMs sister project, aiming to develop infrastructure to robustly evaluate and exploit open LLMs. More info soon!\n        \n        Project website\n    \nMOSAIC: Electronic Components and Systems for our Automated Digital FutureEU Horizon Europe (2025-2029)MOSAIC aims to develop the next generation of electronic components offering superior cognitive system intelligence, efficiency, and robustness. More info soon!\n        \n        Project website\n    \nAssessment of Learning technologies and Frameworks for Intelligent and Ethical AIEU Horizon Europe (2025-2028)ALFIE aims to harness AI responsibly with essential skills, ethical practices, and people-centered, trustworthy AI. Our AutoML platform streamlines AI evaluation, ensuring performance aligns with ethical standards and societal values. Together, we’ll build a transparent, democratic AI ecosystem for the public good.\n        \n        Project website\n    \n2024\nSYNERGIESEU Horizon Europe (2024-2027)SYNERGIES will enhance the development, training, virtual testing, and validation of cooperative, connected and automated mobility systems.\n        \n        Project website\n    \nEDIH-SNLEU Digital Europe (2024-2025)The European Digital Innovation Hub South Netherlands aims to accellerate the digital transformation of manufacturing and maintenance SMEs.\n        \n        Project website\n    \nAutomated Machine Learning for allDutch Science Foundation, Open Science Fund (2024-2025)This project leverages OpenML and AutoML to automatically build AI models and provide intuitive reports to help scientists make progress across many different fields.\n        \n        Project website\n    \n2022\nMachine Learning for building renovationsDutch Government (2022-2026)To make cities more sustainable, this project uses machine learning to predict the energy performance of buildings and optimize renovation strategies.\n        \n        Project website\n    \nDigital Twin of a Vertical FarmDutch Science Foundation, Merian Fund (2022-2026)Vertical farming allows us to grow more food using less resources. We use AI to model plants in 3D to understand how different light, CO2 and temperature scenarios impact plant growth and photosynthesis.\n        \n        Project website\n    \nAI4EuropeEU Horizon Europe (2022-2025)The AI-on-Demand platform (AIoD) is a community-driven platform designed to empower European research and innovation in Artificial Intelligence (AI).\n        \n        Project website\n    \n2020\nStairway to AIEU Horizon 2020 (2020-2024)The StairwAI project aims to provide a matchmaking service for users of the AI-on-Demand platform to easily find AI assets, experts, knowledge, hardware resource providers and much more.\n        \n        Project website\n    \nContinuous monitoring in personal and physical healthITEA Inno4Health (2020-2024)Inno4Health stimulates continuous monitoring in personal and physical health, improving patient care and athlete performance.\n        \n        Project website\n    \nMulti Modal PhotochemistryDutch Science Foundation, TTW (2020-2024)In this project, we aimed to optimize the photochemical synthesis of complex molecules using machine learning.\nTAILOR Network of AI ExcellenceEU Horizon 2020 (2020-2024)TAILOR is one of the first European networks of research excellence in AI, focussing on Trustworthy AI.\n        \n        Project website\n    \nSkyHigh: Leveraging AI in Vertical FarmingDutch Science Foundation (2020-2024)Vertical farming allows us to grow more food using less resources. We use AI to track plant growth non-invasively and optimize growth.\n        \n        Project website\n    \n2019\nEducational platform for machine learning and medical image analysisTU Eindhoven, BOOST (2019-2025)AI education should be scalable and engaging. This project aims to leverage OpenML for AI-related university courses, challenging students to build the best AI models in an engaging environment.\n        \n        Project website\n    \nThe AutoML GymAmazon Research Award (2019-2020)This project aimed to evolve AutoML systems (agents) in an environment of increasingly difficult tasks, in which AutoML agents can be uploaded as docker images and run on AWS infrastructure.\n        \n        Project website\n    \n2017\nDynamic Data Analytics through Automatically Constructed Machine Learning PipelinesDutch Science Foundation, Commit2Data (2017-2021)This project created new online automated machine learning pipelines, new methods for multi-variate time series prediction, and new approaches for early stage Parkinson's disease diagnostics from videos.\n        \n        Project website\n    \nData Driven Discovery of ModelsDARPA (2017-2021)The first DARPA challenge on AutoML, the Data-Driven Discovery of Models (D3M) program developed automated methods to create empirical models of real, complex processes. It also lead to the creation of the AutoML benchmark.\n        \n        Project website\n    \n2016\nA Cloud-Based Platform for AutoMLMicrosoft Azure Research Award (2016-2016)With sponsorship from Microsoft, we ran the first large-scale machine learning benchmarks on Azure, which are still accessible on OpenML today.\n        \n        Project website\n    \n2012\nMassively Collaborative Machine LearningDutch Science Foundation, Free Competition (2012-2016)This project created OpenML, an open science platform for sharing data, code, and experiments in machine learning.\n        \n        Project website\n    \nMLOpen Machine Learning PlatformEU PASCAL Harvest (2012-2013)This exploratory project brought together scientists and engineers to create an open machine learning platform, forming the foundation of the OpenML community."
  },
  {
    "objectID": "notebooks/malibo-blog.html",
    "href": "notebooks/malibo-blog.html",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "",
    "text": "This blogpost is about why we set out to write our paper “MALIBO: Meta-learning for Likelihood-free Bayesian Optimization” (📄paper, 🤖code) and provides a brief overview of its main contributions."
  },
  {
    "objectID": "notebooks/malibo-blog.html#malibo-meta-learning-for-likelihood-free-bayesian-optimization",
    "href": "notebooks/malibo-blog.html#malibo-meta-learning-for-likelihood-free-bayesian-optimization",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "MALIBO: Meta-learning for Likelihood-free Bayesian Optimization",
    "text": "MALIBO: Meta-learning for Likelihood-free Bayesian Optimization\nBayesian Optimization (BO) is the tool of choice for expensive black-box optimization tasks, but its effectiveness often breaks down when dealing with high-dimensional, noisy, and heterogeneously scaled problems across diverse tasks. Traditional meta-learning BO frameworks, which are built atop Gaussian Processes (GPs), struggle with these due to their modeling assumptions and scalability limitations.\nIn this paper, we propose MALIBO (Meta-learning for LIkelihood-free Bayesian Optimization): meta-learning the acquisition function itself, rather than the surrogate model. It combines likelihood-free acuqisition functions with a task-uncertainty-aware meta-learning strategy, resulting in a robust and scalable optimization framework."
  },
  {
    "objectID": "notebooks/malibo-blog.html#what-is-meta-learning-bo",
    "href": "notebooks/malibo-blog.html#what-is-meta-learning-bo",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "What is Meta-learning BO?",
    "text": "What is Meta-learning BO?\nBayesian optimization (BO) aims to optimize an expensive black-box function: \\[\\begin{equation}\n    \\mathbf x^{*} = \\argmin_{\\mathbf x \\in \\mathcal{X}} f(\\mathbf{x})\n\\end{equation}\\]\nMeta-learning BO leverages information from past optimization experiences to accelerate the current optimization process.\n\n\n\nFig.1 - (Left) historical data from various tasks. (Right) Meta-learning model captures the information from histrical data for optimizing the target task.."
  },
  {
    "objectID": "notebooks/malibo-blog.html#motivation-why-move-beyond-gps-in-meta-bo",
    "href": "notebooks/malibo-blog.html#motivation-why-move-beyond-gps-in-meta-bo",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "Motivation: Why Move Beyond GPs in Meta-BO?",
    "text": "Motivation: Why Move Beyond GPs in Meta-BO?\nDespite the prevalence of GPs in BO, their application in meta-learning is fraught with several limitations:\n\nPoor scalability in both data and task number due the cubic computational complexity.\nSensitivity to scale mismatches across tasks (e.g., validation loss on MNIST vs. CIFAR).\nHomoscedastic Gaussian noise assumptions, often violated in real-world data.\nDeterministic task similarity models, which lead to unreliable adaptation when the target task diverges from seen distributions.\n\nTo address these, MALIBO abandons traditional surrogate modeling and instead learns a classifier-based acquisition function, which can directly infer the utility of a query without modeling the response surface."
  },
  {
    "objectID": "notebooks/malibo-blog.html#how-to-tackle-these-limitations",
    "href": "notebooks/malibo-blog.html#how-to-tackle-these-limitations",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "How to tackle these limitations?",
    "text": "How to tackle these limitations?\n\nDirect approximation for acquisition function\nMALIBO is based on likelihood-free Bayesian optimization (LFBO) [1], which bypasses surrogate modeling by directly approximating the acquisition function. This approach eliminates the computational bottleneck associated with GPs and imposes fewer assumptions on the target functions.\nSpecifically, LFBO reformulates the approximation of the acquisition function as a binary classification problem. Observed data points are labeled as “good” or “bad” based on whether they exceed a predefined threshold \\(\\tau\\). A probabilistic classifier \\(C_{\\theta}(\\mathbf{x})\\) is then trained to estimate the probability that a given input \\(\\mathbf{x}\\) belongs to the “good” class. This probability serves as a proxy for the acquisition function, guiding the selection of new evaluation points by maximizing the classifier’s output.\n\n \n\nFig.2 - (Left) A Gaussian process model and the acquisition function (Right) LFBO approximates the acquisition function directly by casting the problem into a classification problem\n\n\n\n\nUncertainty-awared Meta-learning\nThe LFBO framework enables scalable Bayesian optimization by directly approximating the acquisition function through classification, thereby avoiding strong assumptions about the black-box function or noise distribution. To extend this approach to a meta-learning setting, we require a classifier that can incorporate knowledge from prior tasks and generalize effectively to new ones. Unlike using regression model, such a meta-learning classifier is less sensitive to the heterogeneous scales in different tasks, which can improve the meta-learning performance.\n\n\n\nFig.3 - The classifier consists of two key components. 1. The task-agnostic part learns a shared feature mapping across all tasks, capturing the common structure and regularities that are transferable between tasks. 2. The task-specific embedding is modeled as a linear layer that modulates the shared features, introducing task-dependent variations to adapt the model to the current task. The final prediction is obtained by summing the outputs of these two components and applying a sigmoid activation, resulting in a probabilistic, binary prediction.\n\n\nThe meta-learning in MALIBO is constructed as followed: - Task-Agnostic Feature Mapping: A residual feedforward network maps input \\(x\\) to a shared feature space \\(\\phi(x)\\), with a mean prediction head \\(m(\\phi(x))\\).\n\nTask-Specific Latent Embeddings: Each task \\(t\\) is associated with an embedding (the weight in the linear layer) \\(z_t \\sim \\mathcal{N}(0, I)\\), and the classifier outputs \\(C(x) = \\sigma \\left( m(\\phi(x)) + z_t^\\top \\phi(x) \\right)\\)\n\nWith this meta-learning classifier, the model can adapt to new tasks by estimating a task-specific embedding \\(z\\) using the learned feature mapping. A straightforward approach would be to treat this as a maximum likelihood problem and directly optimize for \\(z\\) on the target task. However, this ignores task uncertainty and can lead to unreliable adaptation and over-exploitation of limited data. Moreover, when there is a mismatch between the meta-training data distribution and the non-i.i.d. data collected during optimization, a deterministic model may fail to generalize effectively. To address these issues, we adopt a Bayesian approach to task adaptation. By modeling the uncertainty in the task embedding, our classifier becomes more robust and exploratory, enabling better generalization to new and diverse tasks.\nWe introduce two components to mitigate these issues: a probabilistic way for meta-learning and a residual prediction module to make prediction solely based on target task.\n\nProbabilistic training and task adaptation\n\nProbabilistic meta-learning: During training, the embedding \\(z\\) is encourage to follow a prior distribution \\(\\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) in order to enable Bayesian inference during task adaptation.\nBayesian Adaptation via Laplace Approximation: A posterior over \\(z\\) is inferred using Laplace approximation around the maximum-a-posteriori estimate \\(q(z) \\approx \\mathcal{N}(z_{\\text{MAP}}, \\Sigma)\\).\nThompson Sampling: Enables exploratory behavior early on, and naturally supports parallel BO. Exploration is encouraged by sampling \\(z \\sim q(z)\\) and using: \\(C(x) = \\sigma \\left( m(\\phi(x)) + h_{z}(x) \\right)\\)\n\n\n\nResidual prediction module for robust adaptation\nTo handle out-of-distribution tasks or compensate for weak meta priors, MALIBO augments its prediction with a gradient boosting classifier trained on residuals from the meta-model.\n\n\n\nFig.4 - Effects of exploration and residual predictions. Color circles denote the optimization queries (from bright to dark), the dashed curve denotes a Thompson sample (TS) of the acquisition function and the orange curve shows the sample combined with gradient boosting (GB)."
  },
  {
    "objectID": "notebooks/malibo-blog.html#results",
    "href": "notebooks/malibo-blog.html#results",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "Results",
    "text": "Results\nBenchmarks: - NASBench-201: NAS with 6D discrete space, evaluated on CIFAR-10/100 and ImageNet-16. - HPOBench: 9D HPO problem on UCI datasets. - HPO-B: Large-scale HPO benchmark.\n\n\n\nFig.5 - Aggregated normalized regrets for BO algorithms on real-world AutoML problems.\n\n\nPerformance: - Faster convergence and better anytime performance than GP, ABLR, MetaBO, and even LFBO. - More robust to scale shifts and heteroscedastic noise. - Low computational overhead—Thompson sampling and Laplace approximation are lightweight compared to GP training or MCMC.\n\nReferences\n[1] Song, Jiaming, Lantao Yu, Willie Neiswanger, and Stefano Ermon. “A General Recipe for Likelihood-Free Bayesian Optimization.”, ICML 2022"
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html",
    "href": "notebooks/cl_with_coreset-introduction.html",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "Elif Ceren Gok Yildirim, Murat Onur Yildirim, Joaquin Vanschoren\n\n\nContinual Learning (CL) aims to enable models to learn sequentially from streaming data, retaining previously acquired knowledge while adapting to new tasks, a duality known as the stability-plasticity balance. This balance is critical for mimicking human-like learning, where accumulated knowledge is preserved (stability) yet flexibly updated with novel experiences (plasticity). There is big line of research that focus on achieving better stability-plasticity tradeoff by building and proposing new methods to literature.\n\n\n\nWhile these methods improve performance, they share a common assumption: all training samples are equally valuable and must be exhaustively utilized. By default, this standardized practice prioritizes plasticity (integrating new information) at the risk of destabilizing learned representations, as redundant or noisy samples may overwrite critical prior knowledge. This ”learn-it-all” paradigm diverges from human learning efficiency since, as humans, we are initially exposed to vast amounts of information but intuitively filter and prioritize them, focusing on key experiences (e.g. clear and novel examples) that enrich our understanding while disregarding redundant details.\n\n\n\n\n\n\nIn our approach, we put traditional CL methods to the test by comparing their performance when trained on the full dataset versus when they’re trained on a carefully selected, informative subset of samples (coreset).\nWe structured our training into two distinct phases:\n\nWarm-Up Phase: The CL model undergoes initial, partial training. This phase is key because it lets us analyze the model’s behavior and data representations, which in turn helps us pick out the most informative samples.\nLearning Phase: Once the warm-up is complete, we switch to training on just the selected subset of samples. This phase is much longer and is where the model truly refines its knowledge.\n\nNote that the warm-up phase is generally much shorter than the learning phase since it’s only long enough for the model to “warm up” and provide the insights needed for effective sample selection.\n\n\n\nBelow is a quick comparison of the CL baselines we used in our work:\n\n\n\n\n\n\n\n\nArchitecture-based\nMemory-based\nRegularization-based\n\n\n\n\nDER, FOSTER, MEMO\niCaRL, ER\nLwF\n\n\nDynamically expands the model’s representation space by adding new features for new tasks.\nHolds a memory buffer from previous tasks and keeps learning from them along with new tasks.\nRegularizes the model’s parameters when learning new tasks to retain previously acquired knowledge.\n\n\n\n\n\n\nBelow is a quick comparison of the Coreset baselines we used in our work:\n\n\n\n\n\n\nWe observed a significant improvement in continual learning performance when training with coreset samples instead of full datasets. We found out that this enhancement comes from achieving optimal stability-plasticity tradeoff.\nWhile training with all samples often boosts performance on the current task, it tends to forget earlier tasks. In contrast, using a selective subset (coreset) of samples helps retain prior knowledge better, even if it sacrifices on current task performance.\nThe heatmap below illustrates accuracy across tasks after each learning session on Split-CIFAR10. It compares full-sample training with the best-performing coreset strategy.\n\n\n\nAs seen in the figure, coreset training reduces forgetting, preserving earlier knowledge more effectively.\n\n\n\nTo better understand why this happens, we visualized the learned feature space using t-SNE plots of DER’s representations on Split-CIFAR10.\n\n\n\nWith 20% of the data, coreset-trained models form well-separated class clusters, suggesting clearer, less entangled representations.\nAs we increased the coreset fraction to 80%, and then to the full dataset, the class clusters became less distinct:\nWe also quantified this observation by calculating inter-class distances in the t-SNE-embedded space:\n\n20% coreset: 11.68\n80% coreset: 10.71\nFull dataset: 10.67\n\nThis confirms our visual findings: more data does not always lead to better representations or ensure better learning. Sometimes, it blurs the boundaries between classes, increasing the risk of forgetting past tasks specially in CL.\n\n\n\nWhile coreset-based training improves many CL methods, there are notable exceptions. Methods like FOSTER and LwF (which relies solely on regularization) perform better when trained with the full dataset. These approaches seem to require the complete data distribution to operate effectively.\nWe provide detailed insights and analysis for these exceptions in the full paper, including possible explanations and ablation studies.\nSince I don’t want to bore you with nitty-gritty details, I invite you to check out our paper. We would be happy if you want to talk about this work, so please feel free to reach out to us 😊.\n\nThis post was written by Elif Ceren Gok Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#background",
    "href": "notebooks/cl_with_coreset-introduction.html#background",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "Continual Learning (CL) aims to enable models to learn sequentially from streaming data, retaining previously acquired knowledge while adapting to new tasks, a duality known as the stability-plasticity balance. This balance is critical for mimicking human-like learning, where accumulated knowledge is preserved (stability) yet flexibly updated with novel experiences (plasticity). There is big line of research that focus on achieving better stability-plasticity tradeoff by building and proposing new methods to literature."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#motivation",
    "href": "notebooks/cl_with_coreset-introduction.html#motivation",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "While these methods improve performance, they share a common assumption: all training samples are equally valuable and must be exhaustively utilized. By default, this standardized practice prioritizes plasticity (integrating new information) at the risk of destabilizing learned representations, as redundant or noisy samples may overwrite critical prior knowledge. This ”learn-it-all” paradigm diverges from human learning efficiency since, as humans, we are initially exposed to vast amounts of information but intuitively filter and prioritize them, focusing on key experiences (e.g. clear and novel examples) that enrich our understanding while disregarding redundant details."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#the-overall-game-plan",
    "href": "notebooks/cl_with_coreset-introduction.html#the-overall-game-plan",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "In our approach, we put traditional CL methods to the test by comparing their performance when trained on the full dataset versus when they’re trained on a carefully selected, informative subset of samples (coreset).\nWe structured our training into two distinct phases:\n\nWarm-Up Phase: The CL model undergoes initial, partial training. This phase is key because it lets us analyze the model’s behavior and data representations, which in turn helps us pick out the most informative samples.\nLearning Phase: Once the warm-up is complete, we switch to training on just the selected subset of samples. This phase is much longer and is where the model truly refines its knowledge.\n\nNote that the warm-up phase is generally much shorter than the learning phase since it’s only long enough for the model to “warm up” and provide the insights needed for effective sample selection."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#cl-baselines",
    "href": "notebooks/cl_with_coreset-introduction.html#cl-baselines",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "Below is a quick comparison of the CL baselines we used in our work:\n\n\n\n\n\n\n\n\nArchitecture-based\nMemory-based\nRegularization-based\n\n\n\n\nDER, FOSTER, MEMO\niCaRL, ER\nLwF\n\n\nDynamically expands the model’s representation space by adding new features for new tasks.\nHolds a memory buffer from previous tasks and keeps learning from them along with new tasks.\nRegularizes the model’s parameters when learning new tasks to retain previously acquired knowledge."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#coreset-baselines",
    "href": "notebooks/cl_with_coreset-introduction.html#coreset-baselines",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "Below is a quick comparison of the Coreset baselines we used in our work:"
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#findings-and-insights",
    "href": "notebooks/cl_with_coreset-introduction.html#findings-and-insights",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "We observed a significant improvement in continual learning performance when training with coreset samples instead of full datasets. We found out that this enhancement comes from achieving optimal stability-plasticity tradeoff.\nWhile training with all samples often boosts performance on the current task, it tends to forget earlier tasks. In contrast, using a selective subset (coreset) of samples helps retain prior knowledge better, even if it sacrifices on current task performance.\nThe heatmap below illustrates accuracy across tasks after each learning session on Split-CIFAR10. It compares full-sample training with the best-performing coreset strategy.\n\n\n\nAs seen in the figure, coreset training reduces forgetting, preserving earlier knowledge more effectively."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#why-does-it-work-a-look-into-representations",
    "href": "notebooks/cl_with_coreset-introduction.html#why-does-it-work-a-look-into-representations",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "To better understand why this happens, we visualized the learned feature space using t-SNE plots of DER’s representations on Split-CIFAR10.\n\n\n\nWith 20% of the data, coreset-trained models form well-separated class clusters, suggesting clearer, less entangled representations.\nAs we increased the coreset fraction to 80%, and then to the full dataset, the class clusters became less distinct:\nWe also quantified this observation by calculating inter-class distances in the t-SNE-embedded space:\n\n20% coreset: 11.68\n80% coreset: 10.71\nFull dataset: 10.67\n\nThis confirms our visual findings: more data does not always lead to better representations or ensure better learning. Sometimes, it blurs the boundaries between classes, increasing the risk of forgetting past tasks specially in CL."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#when-coresets-dont-help",
    "href": "notebooks/cl_with_coreset-introduction.html#when-coresets-dont-help",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "While coreset-based training improves many CL methods, there are notable exceptions. Methods like FOSTER and LwF (which relies solely on regularization) perform better when trained with the full dataset. These approaches seem to require the complete data distribution to operate effectively.\nWe provide detailed insights and analysis for these exceptions in the full paper, including possible explanations and ablation studies.\nSince I don’t want to bore you with nitty-gritty details, I invite you to check out our paper. We would be happy if you want to talk about this work, so please feel free to reach out to us 😊.\n\nThis post was written by Elif Ceren Gok Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html",
    "href": "notebooks/OpenMLxProbabl-hackathon.html",
    "title": "OpenML x Probabl Hackathon",
    "section": "",
    "text": "scikit-learn is a free and open-source machine learning library for the Python programming language, while OpenML is an open platform for sharing datasets, algorithms, and experiments. While our teams have been working together for many years, we do not always have the time to meet in person. When we do get the chance though, many interesting discussions stem from those conversations.\nLast June we had a developers hackathon at the Paris office of Probabl, the official operating brand of scikit-learn, focused on maintaining and expanding open-source ML in Europe and beyond. We met to discuss not only the state of AI and how both our organizations fit in, but also to brainstorm solutions to challenges faced by our developers and communities.\n\n\n\nMany interesting topics were brought up, most of which were not only relevant to us but also to the broader open-source community. In the spirit of open-source, we wanted to share these insights with you."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#introduction",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#introduction",
    "title": "OpenML x Probabl Hackathon",
    "section": "",
    "text": "scikit-learn is a free and open-source machine learning library for the Python programming language, while OpenML is an open platform for sharing datasets, algorithms, and experiments. While our teams have been working together for many years, we do not always have the time to meet in person. When we do get the chance though, many interesting discussions stem from those conversations.\nLast June we had a developers hackathon at the Paris office of Probabl, the official operating brand of scikit-learn, focused on maintaining and expanding open-source ML in Europe and beyond. We met to discuss not only the state of AI and how both our organizations fit in, but also to brainstorm solutions to challenges faced by our developers and communities.\n\n\n\nMany interesting topics were brought up, most of which were not only relevant to us but also to the broader open-source community. In the spirit of open-source, we wanted to share these insights with you."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#community-engagement-and-onboarding-contributors",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#community-engagement-and-onboarding-contributors",
    "title": "OpenML x Probabl Hackathon",
    "section": "Community Engagement and Onboarding Contributors",
    "text": "Community Engagement and Onboarding Contributors\nThe focus of this discussion was around community engagement and emphasized the importance of effectively attracting, onboarding, and retaining contributors, especially newcomers.\nOver the past few years, both in OpenML and scikit-learn, we have been noticing that a majority of contributors are only active for short durations and do not stick around for too long. The ones that do are distributed between a smaller number of more experienced senior developers and a much larger pool of junior developers. This then leads to many PR’s being of lower quality and thus requiring a lot more verification and correction.\nThe question at hand then, is not only how we can attract new contributors to our projects but also how we can make it easier for them and our developers to maintain these projects. Some of the ideas that came up have been tried and tested by communities our colleagues have founded or been part of across the globe.\nTakeaways:\n\nEmotional connection : All of the participants agreed that the most important part of any community is its people. Contributors only stick around if they have an emotional connection to either the project, or the people contributing to the project. In this vein, it would be nice if the maintainers of the project could also be present at events. It also helps if the contributors use the projects themselves.\nFocus on beginners : Since we see that most of our contributors are beginners, it serves to organize sprints that are inclusive of them with a focus on beginner-friendly issues, especially documentation tasks. Having these would not only help them understand the project and contribute better, but also let them form a connection with the project.\nCurated issues : Most of our external contributors have enough on their plate already. Having a curated list of issues before sprints would ensure that no time is wasted and let our contributors focus on the tasks suitable for them.\nDifferent tiers of events : To ensure that everyone is given tasks they can handle, it would be nice to have separate events for contributors with varying levels of expertise. This would also have the added benefit of retaining beginner-friendly issues for sprints to prevent more experienced contributors from claiming them early.\nMentoring : Incorporating one-to-one mentoring to help new contributors set up their development environments would help them feel more connected to the project. It is understandable that this is time consuming, so perhaps some way of deciding who gets mentored can be set up in time.\nContributors guide : Simplifying the contributor’s guide and adding video tutorials would make it a lot more beginner-friendly, especially to those who have never filed a pull request (PR) before.\nSemi regular events : Some of our colleagues found that they tended to care about a project more if there were semi regular events they could set time aside for. Having these helped them slowly build a sense of community as well.\nIncentives : A common question that many developers have is why they should even bother contributing. Helping them understand how contributing to open source projects can aid their careers, bring them closer to the community and also help them get internships would be a good start."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#governance-funding-and-sponsorship",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#governance-funding-and-sponsorship",
    "title": "OpenML x Probabl Hackathon",
    "section": "Governance, Funding, and Sponsorship",
    "text": "Governance, Funding, and Sponsorship\nThis focus of this discussion was the governance structures of open-source projects, sustainable funding models, and the role of sponsorships in supporting project activities.\nTakeaways:\n\nEvolving Governance : Since governance is not static, we can treat it as a living document that evolves with the needs of the community.\nCommunication: It is good practise to maintain open communication channels, such as mailing lists and monthly meetings. This keeps all interested contributors in the loop.\nPull Requests: When a project becomes popular on GitHub, it often receives a large number of pull requests from the broader community, many of which may lack quality and become a time sink for senior developers. Finding a balance between approving and rejecting these contributions, without making the community feel unheard, remains an ongoing challenge.\nSponsorship: It would be beneficial to explore sponsorship models similar to the INRIA Foundation (website is in French), where sponsors have a voice in company policy during bi-annual meetings. Such a policy could also provide an added incentive for companies to sponsor, especially if they frequently use the products of their beneficiaries.\nCorporate partnerships: To keep investors interested, it would also be interesting to look into corporate partnerships (similar to those used by the Linux Foundation) that are of mutual benefit."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#development-tooling-and-workflows",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#development-tooling-and-workflows",
    "title": "OpenML x Probabl Hackathon",
    "section": "Development Tooling and Workflows",
    "text": "Development Tooling and Workflows\nSetting up and maintaining CI/CD pipelines across multiple repositories and programming languages puts a huge burden on the maintainers of the repositories.\nHaving to keep up with the trends and learn new technology very frequently also makes developers more reluctant to change the stack, even if doing so would be beneficial.\nThis discussion looked at how to tackle these challenges and make it easier for developers to handle such complex workloads.\nTakeaways:\n\nAutomation: Automating as much of the CI/CD pipeline as possible by using bots for linting and code coverage checks makes PR quality control easier.\nBetter workflows: Migrating to Github Actions and Azure workflows for testing and deployment also seems to help significantly.\nOpen source tools: There are many open source platforms/tools that offer comparable convenience to tools that are currently used. Some examples of these are CodeBerg and Forgejo as an alternative to GitHub and its issue trackers. Eventually migrating to using these tools might also help manage projects like scikit-learn and OpenML.\nCircleCI: Tools for rendering documentation examples directly in the browser can also be used to enhance the review process."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#broader-ecosystem-and-scope-of-collaboration",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#broader-ecosystem-and-scope-of-collaboration",
    "title": "OpenML x Probabl Hackathon",
    "section": "Broader Ecosystem and Scope of Collaboration",
    "text": "Broader Ecosystem and Scope of Collaboration\nWhile both OpenML and scikit-learn focus on the open source ML community, it is sometimes hard to explain how we fit into the broader AI ecosystem.\nEspecially with the rise of LLMs and Generative AI, stakeholders are somewhat inclined to think that frameworks such as ours are just not “enough”. Of course, this is not true at all.\nTakeaways:\n\nCommunity projects: There are so many successful examples of open source projects, and a lot can be learned from their efforts. Projects like Scientific Python for example, have very well set up CI documentation and governance processes that can be quite readily applied to any open source project.\nExploring connections: A longer term focus for both our teams would be to explore connections with other open-source ML frameworks, such as PyTorch, Tensorflow as well as AutoML tools. Doing so would also help integrate and strengthen the open-source ML communities and ecosystems."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#discussion-on-croissant",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#discussion-on-croissant",
    "title": "OpenML x Probabl Hackathon",
    "section": "Discussion on Croissant",
    "text": "Discussion on Croissant\nMachine Learning datasets can consist of structure, unstructured data, or both, which makes them all the more complicated to manage. This has led to the rise of multiple “dataset formats” which further make it hard to consistently load data across platforms and tools.\nCroissant is a dataset metadata format which, among other things, describes how to load and interpret the dataset, which helps load data regardless of which underlying dataset format is used. This format is now not only compatible with the most popular ML libraries/platforms (scikit-learn, PyTorch, Tensorflow, Kaggle, Hugging Face), it is also recommended by NeurIPS (one of the topmost conferences in the AI space).\nFeatures of Croissant:\n\nSchema.org: Croissant was built on top of schema.org with more metadata information specific to ML datasets. Since it does not require any changes to the underlying data structure, existing datasets can quite easily be converted to use it.\nLayers: The format has 4 layers - Dataset level metadata, resource descriptions, content structure, and ML semantics. Each of which make it possible to encode and maintain structural information about datasets regardless of platform.\nXAI and Visualization: Analysis and visualisation of the data works out of the box for all datasets and across multiple platforms. Croissant also supports the Core RAI vocabulary for explainable AI.\nSupported Platforms: Every dataset in OpenML has a Croissant representation, while a majority of data on Kaggle and Google Dataset search also support it."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#conclusion",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#conclusion",
    "title": "OpenML x Probabl Hackathon",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\nOverall, this discussion was quite a successful one for both of our teams. We learnt a lot from each other and found new ways of collaborating on our shared dream of open-source ML. So much in fact, that we wanted to share our discussion with you, dear reader.\nWe hope you learnt something new. We would love to welcome you to our community and would be glad to support your journey in this ML space.\n❤️ The OpenML and scikit-learn team"
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#reach-out",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#reach-out",
    "title": "OpenML x Probabl Hackathon",
    "section": "Reach out",
    "text": "Reach out\n\nFor more information about this hackathon or just to chat with us, feel free to reach out to us on OpenML Email, Probabl. To contact the authors, send them an email here - Subhaditya Mukherjee, Emily Chen."
  },
  {
    "objectID": "join/join.html",
    "href": "join/join.html",
    "title": "Join us!",
    "section": "",
    "text": "Impressions of our campus.\n\n\n\nCurrent openings (8)\nWe currently have room for eight outstanding machine learning Ph.D. students, PostDocs, and Engineers. The vacancies will remain open until the positions are filled. Apply now!\n\nPhD on Multi-modal Open-Source Foundation Models (2 positions)\nJoin a groundbreaking project to develop next-generation, open-source multi-modal foundation models. Collaborate with top research labs and experts to create transparent, efficient, high-performing models with novel capabilities that democratize access to AI. We are seeking a proficient and talented PhD student to develop advanced, multimodal, open source foundation models based on solid research on model architecture, data quality, scaling, generalizability, finetuning, and safety. The focus is on multimodality aspects, including tabular and time series data, as well as efficient model finetuning. [4 year position]\n\n\nPhD in Open-Source Large Language Models (1 position)\nJoin a groundbreaking project to develop a next-generation, open-source family of foundation models. We are seeking a proficient and talented PhD student with a true passion for AI. You will play a pivotal role in a team exploring new large model architectures, novel efficient finetuning and adaptation techniques, refining data to enhance learning and generalizability, creating benchmarks, and ensuring AI safety. [4 year position]\n\n\nPhD on AI for Safety-Critical Multi-modal Learning (1 position)\nAre you eager to make a difference in the advancement of AI via reliable state-of-the-art deep learning models? We are seeking a proficient and talented PhD student to develop efficient, trustworthy AI models that have a robust understanding of their environment based on various data sources. For example, the model should be able to integrate camera, radar, and other sensor modalities. Particular attention will be paid to transformer- and post-transformer architectures, and how to adapt them to learn efficiently on resource-constrained hardware, for instance with transfer learning and quantization techniques. [4 year position]\n\n\nPostdoctoral Fellow in Open-Source Foundation Models (2 positions)\nJoin a groundbreaking project to develop next-generation, open-source multi-modal foundation models. Collaborate with top research labs and experts and lead a new wave for research into transparent, efficient, high-performing models with novel capabilities that democratize access to AI. We are seeking a proficient and talented postdoctoral fellow to develop advanced, multimodal, open source foundation models based on solid research on model architecture, data quality, scaling, generalizability, finetuning, and safety. The focus is on multimodality aspects, including tabular and time series data, as well as efficient model finetuning. You’ll be part of a team with 2 PhD students and an research engineer. There are two positions, one on large language models (3 years) and one of multimodal models (2 years). [2-3 year position]\n\n\nResearch Engineer in Open-Source Foundation Models (2 positions)\nWe are looking for an exceptional research engineer with a passion for AI and open-source development. In this role, you will design and build cutting-edge tools and software that will shape the future of foundation models, used by millions of people worldwide. There are two positions, one on large language models and one of multimodal models. [3 year position]\nYou can also check out other positions in our department.\n\n\n\nWho we are looking for\nWe are seeking exceptional individuals who aspire to leave a mark on the world of machine learning and push humanity forward\n\nFearless Explorers: Visionaries who dare to think big, embrace curiosity, and thrive on pushing the boundaries of innovation.\nDedicated Builders: Innovators who turn ambitious ideas into reality, creating solutions with real-world impact.\nEmpowered Leaders: Changemakers ready to unlock their full potential, inspire others, and define the future of machine learning.\n\nWe especially appreciate people who are curious and open-minded, self-driven and collaborative, kind and respectful, and who contribute to our shared passion for research and exploration. If you’re passionate about making a difference and pursuing excellence, we want you to be part of our journey.\n\n\nHow we will empower you\n\nWe cultivate an environment that inspires and supports world-class research. Our focus is on quality over quantity, encouraging innovative, blue-sky thinking while embracing openness, transparency, and learning from mistakes. Above all, we prioritize an excellent work-life balance, with minimal meetings, plenty of time for focused work, and numerous opportunities to learn and collaborate with one another.\n\nFully Funded for Success\nAll positions are fully funded, so you can focus entirely on your research and growth. PhD positions are always funded for 4 years. PostDoc and Research Engineer positions are usually for 2-4 years, with possibilities for extension. All positions come with a good salary determined on your experience and seniority, in accordance with the Dutch universities’ labor agreement.\n\n\nWorld-leading research\nWe change the world, together. We’re a tight-knit team of about 20 PhD’s, postdocs, and engineers working together to create real impact. You can find our work in top AI conferences and journals, including NeurIPS, ICML, CVPR, JMLR, and TPAMI, and we take pride in good engineering and open science, building AI tools used by hundreds of thousands of people.\n\n\nFreedom to Study Deeply and Think Big\nWe don’t just follow trends — we set them. We encourage every team member to become a leading expert, think outside the box, and find novel avenues of research. We work on extremey efficient continual and meta-learning inspired by the human brain, we design AI models that design themselves using AutoML, we build platforms and create standards that enable research that wasn’t possible before, and even create new venues (e.g. NeurIPS Datasets & Benchmarks) to create new incentives and strengthen communities.\n\n\nPurpose-Driven Research\nYour work here will matter. Next to doing transformative fundamental research, we also leverage our insight to tackle real-world problems, in line with AI for good and sustainable development goals.\n\n\nExcellent Career Prospects\nResearch labs and employers around the globe recognize the quality of our researchers, and we collaborate extensively with other leading research labs and organizations all over the world, such as Google, HuggingFace, and MLCommons."
  },
  {
    "objectID": "software/software.html",
    "href": "software/software.html",
    "title": "Software",
    "section": "",
    "text": "This page provides an overview of the software that we built with the goal of transforming the way people do (automated) machine learning research in a way that is more open, reproducible, and accessible. Most of the software below is developed with collaborators outside of our lab.\nWe also open-source the code for our papers under our GitHub organisation. Code for each paper is also linked in the overview in the papers page.\nOpenML\nMachine learning research should be easily accessible and reusable. OpenML is an open platform for sharing datasets, algorithms, and experiments - to learn how to learn better, together.\n\n        \n        Website\n     \n        \n        Github\n    \nAutoML Benchmark\nThe AutoML Benchmark (AMLB) is software that performs end-to-end evaluation of AutoML frameworks. It can be used to compare current state-of-the-art and evaluate new methods with ablation studies. Over 10 AutoML frameworks can be evaluated out-of-the-box on over 100 datasets, and it’s easy to bring your own data or AutoML framework.\n\n        \n        Website\n     \n        \n        Github\n    \nGAMA\nThe General Automated Machine learning Assistant (GAMA) was developed to be able to easily experiment with new AutoML design through ablation studies by providing a modular AutoML framework. We have sunset the project, and decided to focus our efforts on OpenML and AMLB. If you are interested in a modern implementations driven by similar philosophies, please have a look at the AutoML Toolkit.\n\n        \n        Website\n     \n        \n        Github\n    \nSERENA\nSelf-Regulated Neurogenesis for Online Data-Incremental Learning (SERENA) is a neuro-inspired lightweight and efficient method for Online Data-Incremental Learning, designed to continually adapt to streaming data without forgetting the past knowledge.\n\n        \n        Github\n    \nCL-with-DST\nContinual Learning with Dynamic Sparse Training (CL-with-DST) investigates the evolution of sparse network topologies within the continual learning framework, where data arrives sequentially in a streaming fashion. This approach dynamically adapts the sparse structure of the network over time, enabling efficient learning from non-stationary data while mitigating forgetting.\n\n        \n        Github"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab",
    "section": "",
    "text": "Welcome to our lab on Artificial Minds through Open Research and Engineering @ TU/e!\nAlso known locally as the Automated Machine Learning lab ;)"
  },
  {
    "objectID": "index.html#mission",
    "href": "index.html#mission",
    "title": "Lab",
    "section": "Mission",
    "text": "Mission\nOur mission is to scientifically understand and build AI systems with advanced capabilities, make AI accessible to benefit all of humanity, and illuminate the world.\nWe invent new neural network architectures and train them in new ways to learn better and faster, study models systematically, and use our insights to automate this process so that AI systems can self-assemble and optimize themselves. Everything we create is open-source and crafted with user-friendliness in mind.\nIn this pursuit, we created OpenML, a global open platform with myriad curated datasets, AI models, and experiments, for streamlining machine learning research and making it accessible to all. We perform cutting-edge research on Automated Machine Learning (AutoML), Meta-Learning, Continual Learning, Foundation Models, Open-Endedness, and related fields, and validate our methods in projects that tackle sustainable development goals, including health care, food security, and climate change.\nYou can find our work in top AI conferences and journals, and we take pride in good engineering and open science to build AI models and systems that are widely used by people every day.\nWe are integrated in the Data and AI cluster, which does excellent research in all aspects of AI."
  },
  {
    "objectID": "index.html#recent-highlights",
    "href": "index.html#recent-highlights",
    "title": "Lab",
    "section": "Recent highlights",
    "text": "Recent highlights\n\n\nWe are hiring!\nOur team is growing! We have 8 new openings for PhD students, PostDocs, and Research Engineers for research on large language models and multimodal foundation models. Apply now!\n\n\nTwo new papers at CoLLAs 2025!\nWe’re presenting SERENA, a surprisingly efficient neuro-inspired approach for continual learning using sparse models, as well as SuperNet Transfer, a new method that efficiently adapts neural architectures during transfer learning.\n\n\nNeurIPS Spotlight for Croissant\nCroissant is a new standard for describing machine learning datasets, making it easier to share and reuse data and load it automatically into AI libraries. Joined work with Google, HuggingFace, Kaggle, MLCommons, and many more. Join us at NeurIPS 2024!\n\n\nICML Spotlight for MALIBO\nMALIBO is a neural network that meta-learns how to tune hyperparameters - it learns across many prior tasks to tune models much more efficiently than other techniques. Jiarong Pan will present his work as a spotlight talk at ICML 2024."
  },
  {
    "objectID": "index.html#join-us",
    "href": "index.html#join-us",
    "title": "Lab",
    "section": "Join us!",
    "text": "Join us!\nPhD, PostDoc, or AI Engineer positions: please check out our open positions. Without an open position, you need to be self-funded or have a scholarship.\nVisiting researchers: We love discussing new ideas and collaborations! Note that we generally expect you to stay for at least a few months. Our lab is generally not able to offer paid internships."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Lab",
    "section": "Contact",
    "text": "Contact\nE-mail: openml-labs@tue.nl\nAddress: Groene Loper 5, Metaforum 7.104, 5600 MB Eindhoven, The Netherlands\n\n\n\n\nThis website is powered by Quarto, with thanks to Drew Dimmery."
  }
]