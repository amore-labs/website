[
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "2026\n\n\nHow NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic EvaluationP. Singh, S. Hess, and J. VanschorenInternational Conference on Learning Representations (ICLR 2026) (2026)\n        \n        Published\n    \nAn Empirical Study of Speculative Decoding for Small Language ModelsL. Mainardi, S. Sandikci, and J. VanschorenConference of the European Chapter of the Association for Computational Linguistics (EACL 2026) (2026)\n        \n        Published\n    \nPruned Adaptation Modules: A Simple yet Strong Baseline for Continual Foundation ModelsE. C. Gok Yildirim, M. O. Yildirim, and J. VanschorenConference on Parsimony and Learning (CPAL 2026) (2026)\n        \n        Published\n    \nUnlocking [CLS] Features for Continual Post-TrainingM. O. Yildirim, E. C. Gok Yildirim, and J. VanschorenTransactions on Machine Learning Research, 2026 (2026)\n        \n        Published\n    \n\n\n2025\n\n\nCrypticBio: A Large Multimodal Dataset for Visually Confusing SpeciesG. Manolache, G. Schouten, and J. VanschorenAdvances in Neural Information Processing Systems (NeurIPS 2025) (2025)\n        \n        Published\n    \nOpenML: Insights from 10 years and more than a thousand papersB. Bischl, G. Casalicchio, T. Das, M. Feurer, S. Fischer, P. Gijsbers, S. Mukherjee, A. C. M√ºller, L. N√©meth, L. Oala, L. Purucker, S. Ravi, J. N. van Rijn, P. Singh, J. Vanschoren, J. van der Velde, and M. WeverPatterns 6(7), 101317, 2025 (2025)\n        \n        Published\n     \n        Editors‚Äô pick: Best of 2025\n    \nScore Matching on Large Geometric Graphs for Cosmology GenerationD. Onu»õu, Y. Zhao, J. Vanschoren, and V. MenkovskiInternational Conference on Discovery Science, 17-31, 2025 (2025)\n        \n        Published\n    \nAutomated Machine Learning for Unsupervised Tabular TasksP. Singh, P. Gijsbers, E. C. Gok Yildirim, M. O. Yildirim, and J. VanschorenMachine Learning Journal, ACML 2025 (2025)\n        \n        Published\n    \nAnalysis of Transferability Estimation Metrics for Surgical Phase RecognitionPrabhant Singh, Yiping Li, and Yasmina Al Khalil3rd Workshop in Data Engineering in Medical Imaging (DEMI) MICCAI-2025 Workshop (Spotlight) (2025)\n        \n        Published\n    \nUnsupervised Meta-Learning via In-Context LearningAnna Vettoruzzo, Lorenzo Braccaioli, Joaquin Vanschoren, and Marlena NowaczykInternational Conference on Learning Representations (ICLR) (2025)\n        \n        Published\n    \nSelf-Regulated Neurogenesis for Online Data-Incremental LearningM. O. Yildirim, E. C. Gok Yildirim, D. C. Mocanu, and J. VanschorenConference on Lifelong Learning Agents (CoLLAs) (2025)\n        \n        Github\n     \n        \n        Published\n    \nIn-depth sensitivity analysis of heating demand and overheating in Dutch terraced houses using interpretable machine learning.Alexis Cvetkov-Iliev, Vasilis Soulios, Luyi Xu, G√ºnsu Merin Abbas, Evangelos Kyrou, Lisanne Havinga, Pieter Jan Hoes, Roel Loonen, and Joaquin VanschorenEnergy & Buildings, 337, no. 11561 (2025)\n        \n        PDF\n     \n        \n        Published\n    \nOn Supernet Transfer Learning for Effective Task AdaptationP. Singh and J. VanschorenConference on Lifelong Learning Agents (CoLLAs 2025) (2025)\n        \n        Published\n    \nMeta-Learning Transformers to Improve In-Context GeneralizationL. Braccaioli, A. Vettoruzzo, P. Singh, J. Vanschoren, M. R. Bouguelia, and N. ConciarXiv preprint arXiv:2507.05019, 2025 (2025)\n        \n        Preprint\n    \nAutoML Benchmark with shorter time constraints and early stoppingI. C. Jurado, P. Gijsbers, and J. VanschorenarXiv preprint arXiv:2504.01222, 2025 (2025)\n        \n        Preprint\n    \nOccam's model: Selecting simpler representations for better transferability estimationP. Singh, S. Hess, and J. VanschorenarXiv preprint arXiv:2502.06925, 2025 (2025)\n        \n        Preprint\n    \nAILuminate: Introducing v1.0 of the AI risk and reliability benchmark from MLCommonsS. Ghosh, H. Frase, A. Williams, ..., P. Mattson, P. Liang, and J. VanschorenarXiv preprint arXiv:2503.05731, 2025 (2025)\n        \n        Preprint\n    \n\n\n2024\n\n\nAMLB: An AutoML BenchmarkP. Gijsbers, M. L. P. Bueno, S. Coors, E. LeDell, S. Poirier, J. Thomas, B. Bischl, and J. VanschorenJournal of Machine Learning Research, 25 (101), 1--65 (2024)\n        \n        PDF\n     \n        \n        Github\n     \n        \n        Published\n    \nTowards Efficient AutoML: A Pipeline Synthesis Approach Leveraging Pre-Trained Transformers for Multimodal DataA. Moharil, J. Vanschoren, P. Singh, and D. TamburriMachine Learning, 113, 7011‚Äì7053 (2024)\n        \n        PDF\n     \n        \n        Published\n    \nAdvances and Challenges in Meta-Learning: A Technical ReviewA. Vettoruzzo, M.-R. Bouguelia, J. Vanschoren, T. Rognvaldsson, and K. C. SantoshIEEE Transactions on Pattern Analysis and Machine Intelligence (2024)\n        \n        Published\n    \nCan Fairness Be Automated? Guidelines and Opportunities for Fairness-Aware AutoMLH. Weerts, F. Pfisterer, M. Feurer, K. Eggensperger, E. Bergman, N. Awad, J. Vanschoren, M. Pechenizkiy, B. Bischl, and F. HutterJournal of Artificial Intelligence Research, 79, 639--677 (2024)\n        \n        Published\n    \nCroissant: A Metadata Format for ML-Ready DatasetsM. Akhtar, O. Benjelloun, C. Conforti, P. Gijsbers, J. Giner-Miguelez, N. Jain, M. Kuchnik, Q. Lhoest, P. Marcenac, M. Maskey, P. Mattson, L. Oala, P. Ruyssen, R. Shinde, E. Simperl, G. Thomas, S. Tykhonov, J. Vanschoren, J. van der Velde, S. Vogler, and C.-J. WuAdvances in Neural Information Processing Systems (NeurIPS 2024) (2024)\n        \n        Published\n     \n        NeurIPS Spotlight\n    \nTrustLLM: Trustworthiness in Large Language ModelsY. Huang, L. Sun, H. Wang, and others, and J. VanschorenInternational Conference on Machine Learning (ICML 2024), 20166--20270 (2024)\n        \n        Published\n    \nMALIBO: Meta-Learning for Likelihood-Free Bayesian OptimizationJ. Pan, S. Falkner, F. Berkenkamp, and J. VanschorenInternational Conference on Machine Learning (ICML 2024) (2024)\n        \n        PDF\n     \n        \n        Github\n     \n        \n        Published\n     \n        ICML Spotlight\n    \nLearning to Learn without Forgetting Using AttentionA. Vettoruzzo, J. Vanschoren, M.-R. Bouguelia, and T. R√∂gnvaldssonConference on Lifelong Learning Agents (CoLLAs 2024) (2024)\n        \n        PDF\n     \n        \n        Published\n    \nContinual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model UpdatesM. O. Yildirim, E. C. Gok, G. Sokar, D. C. Mocanu, and J. VanschorenConference on Parsimony and Learning (CPAL 2024), 94--107 (2024)\n        \n        Github\n     \n        \n        Published\n    \nHyTAS: A Hyperspectral Image Transformer Architecture Search Benchmark and AnalysisF. Zhou, M. Kilickaya, J. Vanschoren, and R. PiaoEuropean Conference on Computer Vision (ECCV 2024) (2024)\n        \n        PDF\n     \n        \n        Github\n     \n        \n        Published\n    \nCroissant: A Metadata Format for ML-Ready DatasetsM. Akhtar, O. Benjelloun, C. Conforti, P. Gijsbers, J. Giner-Miguelez, N. Jain, M. Kuchnik, Q. Lhoest, P. Marcenac, M. Maskey, P. Mattson, L. Oala, P. Ruyssen, R. Shinde, E. Simperl, G. Thomas, V. Tykhonov, J. Vanschoren, S. Vogler, and C.-J. WuSIGMOD/PODS Workshop on Data Management for End-to-End Machine Learning (DEEM 2024), 1--6 (2024)\n        \n        Published\n    \nInternational Conference on Automated Machine LearningM. Lindauer, K. Eggensperger, R. Garnett, and J. VanschorenProceedings of Machine Learning Research, Volume 256, PMLR, 2024 (2024)\n        \n        Published\n    \nOn dataset transferability in medical image classificationDovile Juodelyte, Enzo Ferrante, Prabhant Singh, Joaquin Vanschoren, and Veronika CheplyginaarXiv preprint arXiv:2412.20172v1, 2024 (2024)\n        \n        Preprint\n    \nA Standardized Machine-Readable Dataset Documentation Format for Responsible AIN. Jain, M. Akhtar, J. Giner-Miguelez, R. Shinde, J. Vanschoren, S. Vogler, S. Goswami, Y. Rao, T. Santos, and L. OalaarXiv preprint arXiv:2407.16883, 2024 (2024)\n        \n        Preprint\n    \nAutomatic Combination of Sample Selection Strategies for Few-Shot LearningB. Pecher, I. Srba, M. Bielikova, and J. VanschorenarXiv preprint arXiv:2402.03038, 2024 (2024)\n        \n        Preprint\n    \nCLAMS: A System for Zero-Shot Model Selection for ClusteringP. Singh, P. Gijsbers, M. O. Yildirim, E. C. Gok, and J. VanschorenarXiv preprint arXiv:2407.11286, 2024 (2024)\n        \n        Preprint\n    \nCan Time Series Forecasting Be Automated? A Benchmark and AnalysisA. T. Sreedhara and J. VanschorenarXiv preprint arXiv:2407.16445, 2024 (2024)\n        \n        Preprint\n    \nIntroducing v0.5 of the AI Safety Benchmark from MLCommonsB. Vidgen, A. Agrawal, A. M. Ahmed, V. Akinwande, N. Al-Nuaimi, N. Alfaraj, E. Alhajjar, L. Aroyo, T. Bavalatti, B. Blili-Hamelin, and J. VanschorenarXiv preprint arXiv:2404.12241, 2024 (2024)\n        \n        Preprint\n    \nContinual Learning on a Data DietE. C. Gok Yildirim, M. O. Yildirim, and J. VanschorenarXiv preprint arXiv:2410.17715, 2024 (2024)\n        \n        Preprint\n    \n\n\n2023\n\n\nSignal Quality Analysis for Long-Term ECG Monitoring Using a Health Patch in Cardiac PatientsI. Campero Jurado, I. Lorato, J. Morales, L. Fruytier, S. Stuart, P. Panditha, D. M. Janssen, N. Rossetti, N. Uzunbajakava, I. B. Serban, L. Rikken, M. de Kok, J. Vanschoren, and A. BrombacherSensors, 23 (4), Art. 2130 (2023)\n        \n        Published\n    \nOnline AutoML: An Adaptive AutoML Framework for Online LearningB. Celik, P. Singh, and J. VanschorenMachine Learning, 112 (6), 1897--1921 (2023)\n        \n        Published\n    \nAutomated Machine Learning Approach in Material Discovery of Hole Selective Layers for Perovskite Solar CellsM. O. Yildirim, E. C. Gok Yildirim, E. Eren, P. Huang, M. P. U. Haris, S. Kazim, J. Vanschoren, A. Uygun Oksuz, and S. AhmadEnergy Technology, 11 (1) (2023)\n        \n        Published\n    \nEfficient-DASH: Automated Radar Neural Network Design Across Tasks and DatasetsT. Boot, N. Cazin, W. Sanberg, and J. VanschorenIEEE Intelligent Vehicles Symposium (IV 2023), 1--7 (2023)\n        \n        Published\n    \nAn Analysis of Evolutionary Migration Models for Multi-Objective, Multi-Fidelity AutoMLI. Campero-Jurado and J. VanschorenIEEE International Conference on Systems, Man, and Cybernetics (SMC 2023), 2940--2945 (2023)\n        \n        Published\n    \nNeural Architecture Search for Visual Anomaly SegmentationT. Kerssies and J. VanschorenAutoML Conference (AutoML 2023) (2023)\n        \n        Published\n    \nAre Labels Needed for Incremental Instance Learning?M. Kilickaya and J. VanschorenIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023), 2401--2409 (2023)\n        \n        Published\n    \nDataperf: Benchmarks for Data-Centric AI DevelopmentM. Mazumder, C. Banbury, X. Yao, and others, and J. VanschorenAdvances in Neural Information Processing Systems (NeurIPS 2023) (2023)\n        \n        Published\n    \nAutoML for Outlier Detection with Optimal Transport DistancesP. Singh and J. VanschorenProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI 2023), 7175--7178 (2023)\n        \n        Published\n    \nAdaCL: Adaptive Continual LearningE. C. Gok Yildirim, M. O. Yildirim, M. Kilickaya, and J. VanschorenContinual AI Unconference (ContinualAI 2024), PMLR, 249, 15--24 (2023)\n        \n        Published\n    \nLocality-Aware Hyperspectral ClassificationF. Zhou, M. Kilickaya, and J. VanschorenThe British Machine Vision Conference (BMVC 2023) (2023)\n        \n        Published\n    \nNeurIPS‚Äô22 Cross-Domain MetaDL Challenge: Results and Lessons LearnedD. Carri√≥n-Ojeda, M. Alam, S. Escalera, A. Farahat, D. Ghosh, T. G. Diaz, C. Gupta, I. Guyon, J. R. Ky, X. Y. Lee, X. Liu, F. Mohr, M. H. Nguyen, E. Pintelas, S. Roth, S. Schaub-Meyer, H. Sun, I. Ullah, J. Vanschoren, L. Vidyaratne, J. Wu, and X. YinNeurIPS 2022 Competition Track, 50--72 (2023)\n        \n        Published\n    \nDemocratising Artificial Intelligence to Accelerate Scientific DiscoveryJ. VanschorenIn: Artificial Intelligence in Science, OECD, 2023 (2023)\n        \n        Published\n    \nEvaluating Continual Test-Time Adaptation for Contextual and Semantic Domain ShiftsT. Kerssies, M. Kƒ±lƒ±√ßkaya, and J. VanschorenarXiv preprint arXiv:2208.08767, 2023 (2023)\n        \n        Preprint\n    \nWhat Can AutoML Do for Continual Learning?M. Kƒ±lƒ±√ßkaya and J. VanschorenarXiv preprint arXiv:2311.11963, 2023 (2023)\n        \n        Preprint\n    \nDMLR: Data-Centric Machine Learning Research -- Past, Present and FutureL. Oala, M. Maskey, L. Bat-Leah, A. Parrish, N. M. G√ºrel, T.-S. Kuo, Y. Liu, R. Dror, D. Brajovic, X. Yao, and J. VanschorenarXiv preprint arXiv:2311.13028, 2023 (2023)\n        \n        Preprint\n    \n\n\n2022\n\n\nAgroML: An Open-Source Repository to Forecast Reference Evapotranspiration in Different Geo-Climatic Conditions Using Machine Learning and Transformer-Based ModelsJ. A. Bellido-Jim√©nez, J. Est√©vez, J. Vanschoren, and A. P. Garc√≠a-Mar√≠nAgronomy, 12 (3), 656 (2022)\n        \n        Published\n    \nInterpretable Assessment of ST-Segment Deviation in ECG Time SeriesI. Campero Jurado, A. Fedjajevs, J. Vanschoren, and A. BrombacherSensors, 22 (13), Art. 4919 (2022)\n        \n        Published\n    \nMeta-Features for Meta-LearningA. Rivolli, L. P. F. Garcia, C. Soares, J. Vanschoren, and A. C. P. L. F. de CarvalhoKnowledge-Based Systems, 240, 108101 (2022)\n        \n        Published\n    \nTheory-Based Habit Modeling for Enhancing Behavior Prediction in Behavior Change Support SystemsC. Zhang, J. Vanschoren, A. van Wissen, D. Lakens, B. de Ruyter, and W. A. IJsselsteijnUser Modeling and User-Adapted Interaction, 23 (2022)\n        \n        Published\n    \nMulti-Fidelity Optimization Method with Asynchronous Generalized Island Model for AutoMLI. Campero-Jurado and J. VanschorenGenetic and Evolutionary Computation Conference (GECCO 2022) (2022)\n        \n        Published\n    \nMeta-Album: Multi-Domain Meta-Dataset for Few-Shot Image ClassificationI. Ullah, D. Carri√≥n-Ojeda, S. Escalera, I. Guyon, M. Huisman, F. Mohr, J. N. van Rijn, H. Sun, J. Vanschoren, and P. A. VuAdvances in Neural Information Processing Systems 35 (NeurIPS 2022), 3232--3247 (2022)\n        \n        Published\n    \nRegularized Meta-Learning for Neural Architecture SearchR. van Gastel and J. VanschorenAutomated Machine Learning Conference (AutoML 2022) (2022)\n        \n        Published\n    \nFaster Performance Estimation for NAS with Embedding Proximity ScoreG. Franken, P. Singh, and J. VanschorenECMLPKDD Workshop on Meta-Knowledge Transfer, PMLR, 51--61 (2022)\n        \n        Published\n    \nIntroduction to the Special Section on AI in Manufacturing: Current Trends and ChallengesJ. Lijffijt, D. Gkorou, P. Van Hertum, A. Ypma, M. Pechenizkiy, and J. VanschorenACM SIGKDD Explorations Newsletter, 24 (2), 81--85 (2022)\n        \n        Published\n    \nMetalearning: Applications to Automated Machine Learning and Data MiningP. Brazdil, J. N. van Rijn, C. Soares, and J. VanschorenSpringer Nature, 2022 (2022)\n        \n        Published\n    \nAutomated Reinforcement Learning: An OverviewR. R. Afshar, Y. Zhang, J. Vanschoren, and U. KaymakarXiv preprint arXiv:2201.05000, 2022 (2022)\n        \n        Preprint\n    \nWarm-starting DARTS Using Meta-LearningM. Grobelnik and J. VanschorenarXiv preprint arXiv:2205.06355, 2022 (2022)\n        \n        Preprint\n    \n\n\n2021\n\n\nA Comparison of Optimisation Algorithms for High-Dimensional Particle and Astrophysics ApplicationsC. Bal√°zs, M. van Beekveld, S. Caron, B. M. Dillon, B. Farmer, A. Fowlie, W. Handley, L. Hendriks, G. J√≥hannesson, A. Leinweber, J. Mamu≈æiƒá, G. D. Martinez, P. Scott, E. C. Garrido-Merch√°n, R. Ruiz de Austri, Z. Searle, B. Stienen, J. Vanschoren, and M. WhiteJournal of High Energy Physics, 2021 (5), 1‚Äì46 (2021)\n        \n        Published\n    \nAdaptation Strategies for Automated Machine Learning on Evolving DataB. Celik and J. VanschorenIEEE Transactions on Pattern Analysis and Machine Intelligence, 43 (9) (2021)\n        \n        Published\n    \nOpenML-Python: An Extensible Python API for OpenMLM. Feurer, J. N. van Rijn, A. Kadra, P. Gijsbers, N. Mallik, S. Ravi, A. Mueller, J. Vanschoren, and F. HutterJournal of Machine Learning Research, 22 (100), 1‚Äì5 (2021)\n        \n        Github\n     \n        \n        Published\n    \nTransformational Machine Learning: Learning How to Learn from Many Related Scientific ProblemsI. Olier, I. O. Oghenejokpeme, T. Dash, A. Davis, L. N. Soldatova, J. Vanschoren, and R. D. KingProceedings of the National Academy of Sciences (PNAS), 118 (49) (2021)\n        \n        Published\n    \nOpenML Benchmarking SuitesB. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn, and J. VanschorenProceedings of the NeurIPS Track on Datasets and Benchmarks 2021 (2021)\n        \n        Published\n    \nMeta-Learning for Symbolic Hyperparameter DefaultsP. Gijsbers, F. Pfisterer, J. N. van Rijn, B. Bischl, and J. VanschorenGenetic and Evolutionary Computation Conference (GECCO) Companion, 2021 (2021)\n        \n        Github\n     \n        \n        Published\n    \nGAMA: A General Automated Machine Learning AssistantP. Gijsbers and J. VanschorenProceedings of ECMLPKDD 2021. Lecture Notes in Computer Science, 12461 (2021), p560-564 (2021)\n        \n        Github\n     \n        \n        Published\n    \nAdvances in MetaDL: AAAI 2021 Challenge and WorkshopA. E. Baz, I. Guyon, Z. Liu, J. van Rijn, S. Treguer, and J. VanschorenAAAI 2021 Workshop on Meta-Learning and MetaDL, PMLR 140:1--16 (2021)\n        \n        Published\n    \nVariational Task Encoders for Model-Agnostic Meta-Learning with Uncertainty Over Task DistributionsL. Schragen and J. VanschorenWorkshop on Meta-Learning @ NeurIPS 2021 (2021)\n        \n        Published\n    \nFrom Strings to Data Science: A Practical Framework for Automated String HandlingJ. van Lith and J. VanschorenWorkshop on Automated Data Science @ ECMLPKDD 2021 (2021)\n        \n        Published\n    \nOpen-Ended Learning Strategies for Learning Complex Locomotion SkillsF. Zhou and J. VanschorenWorkshop on Meta-Learning @ NeurIPS 2021 (2021)\n        \n        Published\n    \nProceedings of the Neural Information Processing Systems Track on Datasets and BenchmarksJ. Vanschoren and S. YeungNeurIPS Foundation, Curran Associates, 2021 (2021)\n        \n        Published\n    \nProceedings of the AAAI 2021 Workshop on Meta-Learning and MetaDL ChallengeI. Guyon, J. N. van Rijn, S. Treguer, and J. VanschorenPMLR, 2021 (2021)\n        \n        Published\n    \nAutomated Feature Selection and Classification for High-Dimensional Biomedical DataT. P. Beishuizen, J. Vanschoren, P. A. Hilbers, and D. Bo≈°naƒçkiResearchSquare, 2021 (2021)\n        \n        Published\n    \nCats, Not CAT Scans: A Study of Dataset Similarity in Transfer Learning for 2D Medical Image ClassificationI. van den Brandt, F. Fok, B. Mulders, J. Vanschoren, and V. CheplyginaarXiv preprint arXiv:2107.05940, 2021 (2021)\n        \n        Preprint\n    \nFrugal Machine LearningM. Evchenko, J. Vanschoren, H. H. Hoos, M. Schoenauer, and M. SebagarXiv preprint arXiv:2111.03731, 2021 (2021)\n        \n        Preprint\n    \nFixed-Point Quantization of Convolutional Neural Networks for Quantized Inference on Embedded PlatformsR. Goyal, J. Vanschoren, V. Van Acht, and S. NijssenarXiv preprint arXiv:2102.02147, 2021 (2021)\n        \n        Preprint\n    \n\n\n2020\n\n\nAerial Imagery Pixel-Level SegmentationM. R. Heffels and J. VanschorenarXiv preprint arXiv:2012.02024, 2020 (2020)\n        \n        Preprint\n    \nImportance of Tuning Hyperparameters of Machine Learning AlgorithmsH. Weerts, A. Mueller, and J. VanschorenarXiv preprint arXiv:2007.07588, 2020 (2020)\n        \n        Preprint\n    \n\n\n2019\n\n\nGAMA: Genetic Automated Machine Learning AssistantP. Gijsbers and J. VanschorenJournal of Open Source Software, 4 (33), 1‚Äì2 (2019)\n        \n        Github\n     \n        \n        Published\n    \nA Meta-Learning Recommender System for Hyperparameter Tuning: Predicting When Tuning Improves SVM ClassifiersR. G. Mantovani, A. L. D. Rossi, E. Alcobaca, J. Vanschoren, and A. C. P. L. F. CarvalhoInformation Sciences, 501, 193‚Äì221 (2019)\n        \n        Published\n    \nMulti-Task Learning with a Natural Metric for Quantitative Structure Activity Relationship LearningN. Sadawi, I. Olier, J. Vanschoren, J. N. van Rijn, J. Besnard, R. Bickerton, C. Grosan, L. Soldatova, and R. D. KingJournal of Cheminformatics, 11 (1), Art. 68 (2019)\n        \n        Published\n    \nBeyond Bag-of-Concepts: Vectors of Locally Aggregated ConceptsM. Grootendorst and J. VanschorenProceedings of ECMLPKDD 2019 (2019)\n        \n        Published\n    \nThe ABC of Data: A Classifying Framework for Data ReadinessL. A. Castelijns, Y. Maas, and J. VanschorenWorkshop on Automating Data Science @ ECMLPKDD 2019 (2019)\n        \n        Published\n    \nLearning to Go with the Flow: On the Adaptability of Automated Machine Learning to Evolving DataB. Celik and J. VanschorenWorkshop on Automating Data Science @ ECMLPKDD 2019 (2019)\n        \n        Published\n    \nAn Open Source AutoML BenchmarkP. Gijsbers, E. Ledell, J. Thomas, S. Poirier, B. Bischl, and J. VanschorenAutomated Machine Learning Workshop @ ICML 2019 (2019)\n        \n        Published\n    \nMeta-Learning for Algorithm and Hyperparameter Optimization with Surrogate Model EnsemblesG. Manolache and J. VanschorenMeta-Learning Workshop @ NeurIPS 2019 (2019)\n        \n        Published\n    \nLearning to Reinforcement Learn for Neural Architecture SearchJ. Robles and J. VanschorenNew in ML Symposium @ NeurIPS 2019 (2019)\n        \n        Published\n    \nHyperBoost: Hyperparameter Optimization by Gradient Boosting Surrogate ModelsJ. van Hoof and J. VanschorenWorkshop on Automating Data Science @ ECMLPKDD 2019 (2019)\n        \n        Published\n    \nMeta-LearningJ. VanschorenIn: Automatic Machine Learning: Methods, Systems, Challenges. Springer, 2019 (2019)\n        \n        Published\n    \nAutomatic Machine Learning: Methods, Systems, ChallengesF. Hutter, L. Kotthoff, and J. VanschorenSpringer, 2019 (2019)\n        \n        Published\n    \nMLSys: The New Frontier of Machine Learning SystemsA. Ratner, J. Vanschoren, and and othersarXiv preprint arXiv:1904.03257, 2019 (2019)\n        \n        Preprint\n    \n\n\n2018\n\n\nSpeeding Up Algorithm Selection via Meta-Learning and Active TestingS. Abdulrahman, P. Brazdil, J. N. van Rijn, and J. VanschorenMachine Learning, 107 (1), 79‚Äì108 (2018)\n        \n        Published\n    \nMeta-QSAR: Learning How to Learn QSARsI. Olier, N. Sadawi, G. R. Bickerton, J. Vanschoren, C. Grosan, L. Soldatova, and R. D. KingMachine Learning, 107 (1), 285‚Äì311 (2018)\n        \n        Published\n    \nThe Online Performance Estimation Framework: Heterogeneous Ensemble Learning for Data StreamsJ. N. van Rijn, G. Holmes, B. Pfahringer, and J. VanschorenMachine Learning, 107 (1), 149‚Äì176 (2018)\n        \n        Published\n    \nML Schema: Exposing the Semantics of Machine Learning with Schemas and OntologiesG. Correa Publio, D. Esteves, A. ≈Åawrynowicz, P. Panov, L. Soldatova, T. Soru, J. Vanschoren, and H. ZafarICML 2018 Workshop on Reproducibility in Machine Learning (2018)\n        \n        Published\n    \nMeta Learning for Defaults: Symbolic DefaultsJ. N. van Rijn, F. Pfisterer, J. Thomas, A. Mueller, B. Bischl, and J. VanschorenMeta-Learning Workshop @ NeurIPS 2018 (2018)\n        \n        Published\n    \nData Augmentation Using Conditional Generative Adversarial Networks for Leaf Counting in Arabidopsis PlantsY. Zhu, M. Aoun, M. Krijn, and J. VanschorenCCCPV Workshop @ BMVC 2018 (2018)\n        \n        Published\n    \nProceedings of the 21st International Conference on Discovery ScienceL. Soldatova, J. Vanschoren, G. Papadopoulos, and M. CeciLecture Notes in Artificial Intelligence 11198, DS 2018 (2018)\n        \n        Published\n    \nMetalearning: A SurveyJ. VanschorenarXiv preprint arXiv:1810.03548, 2018 (2018)\n        \n        Preprint\n    \nTowards Reproducible Empirical Research in Meta-LearningA. Rivolli, L. Garcia, C. Soares, J. Vanschoren, and A. C. de CarvalhoarXiv preprint arXiv:1808.10406, 2018 (2018)\n        \n        Preprint\n    \n\n\n2017\n\n\nOpenML: An R Package to Connect to the Networked Machine Learning PlatformG. Casalicchio, B. Hofner, M. Lang, D. Kirchhoff, P. Kerschke, H. Seibold, J. Bossek, J. Vanschoren, and B. BischlComputational Statistics, 32 (3), 1‚Äì15 (2017)\n        \n        Published\n    \nLayered TPOT: Speeding Up Tree-Based Pipeline OptimizationP. Gijsbers, J. Vanschoren, and R. OlsonAutoML Workshop @ ECML 2017, CEUR Workshop Proceedings vol. 1998 (2017)\n        \n        Published\n    \nProceedings of the Twenty-Sixth Benelux Conference on Machine LearningW. Duivesteijn, M. Pechenizkiy, G. H. L. Fletcher, V. Menkovski, E. J. Postma, and J. VanschorenEindhoven University of Technology Eindhoven, 2017 (2017)\n        \n        Published\n    \n\n\n2016\n\n\nAn Algorithm, Implementation and Execution Ontology Design PatternA. Lawrynowicz, D. Esteves, P. Panov, T. Soru, S. Dzeroski, and J. VanschorenIn: Studies on the Semantic Web (Hitzler.P., Gangemi, A., Janowicz, K., Krisnadhi, A., Presutti, V., eds.), IOS Press (2016)\n        \n        Published\n    \nASlib: A Benchmark Library for Algorithm SelectionB. Bischl, P. Kerschke, L. Kotthoff, M. Lindauer, Y. Malitsky, A. Frechette, H. Hoos, F. Hutter, K. Leyton-Brown, K. Tierney, and J. VanschorenArtificial Intelligence, 237, 41‚Äì58 (2016)\n        \n        Published\n    \nReduction of False Arrhythmia Alarms Using Signal Selection and Machine LearningL. M. Eerikainen, J. Vanschoren, M. J. Rooijakkers, R. Vullings, and R. M. AartsPhysiological Measurement, 37 (8), 1204‚Äì1216 (2016)\n        \n        Published\n    \nTowards Understanding Online Sentiment Expression: An Interdisciplinary Approach with Subgroup Comparison and VisualizationB. Gao, B. Berendt, and J. VanschorenSocial Network Analysis and Mining, 6 (1), 68:1‚Äì68:16 (2016)\n        \n        Published\n    \nConnecting R to the OpenML Project for Open Machine LearningB. Bischl, G. Casalicchio, B. Hofner, P. Kerschke, D. Kirchhoff, M. Lang, H. Seibold, and J. VanschorenUseR! Conference (UseR 2016), 1--11 (2016)\n        \n        Published\n    \nAnticipating Habit Formation: A Psychological Computing Approach to Behavior Change SupportC. Zhang, A. van Wissen, D. Lakens, J. Vanschoren, B. De Ruyter, and W. A. IJsselsteijnProceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp 2016), 1247--1254 (2016)\n        \n        Published\n    \nHyper-Parameter Tuning of a Decision Tree Induction AlgorithmR. G. Mantovani, T. Horvath, R. Cerri, A. P. L. F. Carvalho, and J. VanschorenBrazilian Conference on Intelligent Systems (BRACIS 2016) (2016)\n        \n        Published\n    \nProceedings of the 10th International Conference on Learning and Intelligent OptimizationP. Festa, M. Sellmann, and J. VanschorenLecture Notes in Computer Science 10079, LION 2016 (2016)\n        \n        Published\n    \nProceedings of the ICML 2016 Workshop on Automatic Machine LearningF. Hutter, L. Kotthoff, and J. VanschorenPMLR, 2016 (2016)\n        \n        Published\n    \n\n\n2015\n\n\nDecreasing the False Alarm Rate of Arrhythmias in Intensive Care Using a Machine Learning ApproachL. M. Eerikainen, J. Vanschoren, M. J. Rooijakkers, R. Vullings, and R. M. AartsIEEE Computing in Cardiology, 42, 293-297 (2015)\n        \n        Published\n    \nWho is More Positive in Private? Analyzing Sentiment Differences Across Privacy Levels and Demographic Factors in Facebook Chats and PostsB. Gao, B. Berendt, and J. VanschorenIEEE/ACM Proceedings of ASONAM 2015, 605-610 (2015)\n        \n        Published\n    \nTo Tune or Not to Tune: Recommending When to Adjust SVM Hyper-Parameters via Meta-LearningR. G. Mantovani, A. L. D. Rossi, J. Vanschoren, B. Bischl, and A. C. P. L. F. CarvalhoIEEE Proceedings of IJCNN 2015, 1-8 (2015)\n        \n        Published\n    \nEffectiveness of Random Search in SVM Hyper-Parameter TuningR. G. Mantovani, A. L. D. Rossi, J. Vanschoren, B. Bischl, and A. C. P. L. F. CarvalhoIEEE Proceedings of IJCNN 2015, 1-8 (2015)\n        \n        Published\n    \nFast Algorithm Selection Using Learning CurvesJ. N. van Rijn, S. M. Abdulrahman, P. Brazdil, and J. VanschorenAdvances in Intelligent Data Analysis XIV (IDA 2015), Lecture Notes in Computer Science 9385, 298-309 (2015)\n        \n        Published\n    \nTowards a Data Science CollaboratoryJ. Vanschoren, B. Bischl, F. Hutter, M. Sebag, B. Kegl, M. Schmid, G. Napolitano, K. Wolstencroft, A. R. Williams, and N. LawrenceAdvances in Intelligent Data Analysis XIV (IDA 2015), Lecture Notes in Computer Science 9385, XIX-XXI (2015)\n        \n        Published\n    \nAlgorithm Selection via Meta-Learning and Sample-Based Active TestingS. Abdulrahman, P. Brazdil, J. N. van Rijn, and J. VanschorenMetaSel Workshop @ PKDD/ECML 2015, CEUR Workshop Proceedings 1455, 55-66 (2015)\n        \n        Published\n    \nMeta-Learning Recommendation of Default Hyper-Parameter Values for SVMs in Classification TasksR. G. Mantovani, A. L. D. Rossi, J. Vanschoren, and A. C. P. L. F. CarvalhoMetaSel Workshop @ PKDD/ECML 2015, CEUR Workshop Proceedings 1455, 80-92 (2015)\n        \n        Published\n    \nSharing RapidMiner Workflows and Experiments with OpenMLJ. N. van Rijn and J. VanschorenMetaSel Workshop @ PKDD/ECML 2015, CEUR Workshop Proceedings 1455, 93-103 (2015)\n        \n        Published\n    \nTaking Machine Learning Research Online with OpenMLJ. Vanschoren, J. N. van Rijn, and B. BischlJMLR Workshop and Conference Proceedings (BigMine 2015), 41, 1-4 (2015)\n        \n        Published\n    \nTowards a Collaborative Platform for Advanced Meta-Learning in Healthcare Predictive AnalyticsM. Vukicevic, S. Radovanovic, J. Vanschoren, G. Napolitano, and B. DelibasicMetaSel Workshop @ PKDD/ECML 2015, CEUR Workshop Proceedings 1455, 112-114 (2015)\n        \n        Published\n    \nProceedings of the 2015 International Workshop on Meta-Learning and Algorithm Selection @ ECMLPKDDJ. Vanschoren, P. Brazdil, C. G. Giraud-Carrier, and L. KotthoffCEUR Workshop Proceedings 1455, CEUR 2015 (2015)\n        \n        Published\n    \n\n\n2014\n\n\nOpenML: Networked Science in Machine LearningJoaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis TorgoACM SIGKDD Explorations Newsletter (2014)\n        \n        Preprint\n     \n        \n        Github\n     \n        \n        Published\n     \n        Dutch Data Prize 2016\n    \nTowards Meta-Learning on Data StreamsJ. N. van Rijn, G. Holmes, B. Pfahringer, and J. VanschorenMetaSel Workshop @ ECAI 2014, CEUR Workshop Proceedings 1201, 37-38 (2014)\n        \n        Published\n    \nAlgorithm Selection on Data StreamsJ. N. van Rijn, G. Holmes, B. Pfahringer, and J. VanschorenProceedings of Discovery Science 2014, Lecture Notes in Computer Science 8777, 325-336 (2014)\n        \n        Published\n    \n\n\n2013\n\n\nA Survey of Intelligent Assistants for Data AnalysisF. Serban, J. Vanschoren, J. U. Kietz, and A. BernsteinACM Computing Surveys, 45 (3), Art. 31 (2013)\n        \n        Published\n    \nOpenML: A collaborative science platformJ. N. van Rijn, B. Bischl, L. Torgo, B. Gao, V. Umaashankar, S. Fischer, P. Winter, B. Wiswedel, M. R. Berthold, and J. VanschorenEuropean Conference on Machine Learning and Knowledge Discovery in Databases (ECMLPKDD Demo Track) (2013)\n        \n        Published\n     \n        Best Demo Award\n    \n\n\n2012\n\n\nExperiment Databases: A New Way to Share, Organize and Learn from ExperimentsJ. Vanschoren, H. Blockeel, B. Pfahringer, and G. HolmesMachine Learning, 87 (2), 127‚Äì158 (2012)\n        \n        Published"
  },
  {
    "objectID": "notebooks/cldst-introduction.html",
    "href": "notebooks/cldst-introduction.html",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "Murat Onur Yildirim, Elif Ceren Gok Yildirim, Ghada Sokar, Decebal Constantin Mocanu, Joaquin Vanschoren.\nTL;DR: This blog post dives into our recent paper ‚ÄúContinual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates‚Äù (üìÑ Paper, ü§ñ Code) published in CPAL 2024. We explore how Continual Learning can benefit from Dynamic Sparse Training, a process that closely mirrors the brain‚Äôs ability to constantly rewire and optimize its connections. Read on for a brief overview of our key findings üòä\n\nHey there!\nHave you ever wondered how our brains learn new skills without forgetting the old ones? Imagine learning a new language while still chatting away in your native tongue, or watching yourself grow from crawling to walking, running, and even biking‚Äîall while never losing that ability to crawl! It‚Äôs truly fascinating, and yet, teaching our deep learning models to do the same remains an exciting challenge.\n\n\nTraditional neural networks tend to overwrite older information when learning new tasks, which isn‚Äôt very brain-like at all. This major hurdle is called catastrophic forgetting. Imagine if every time you learned a new recipe, you suddenly couldn‚Äôt remember your grandma‚Äôs secret and delicious recipe anymore! That‚Äôs where continual learning (CL) steps in‚Äîallowing models to learn from a stream of data without letting past knowledge vanish into thin air.\n\n\n\n\n\n\nIn neuroscience, it‚Äôs well known that our brain is always under a change and on the move! Our synaptic connections are constantly being reshaped or kept steady based on what we experience‚Äîa delightful dance of plasticity and stability that fuels our lifelong ability to learn and adapt. Picture your brain as a vibrant, ever-changing city where new pathways light up and old ones evolve based on your experiences.\n\n\n\nTo mimic this amazing ability, a technique called Dynamic Sparse Training (DST) offers an artificial version of the plasticity and the stability, ensuring that our neural networks remain both efficient and adaptable over time. Imagine starting with an overparameterized network, like a bustling metropolis with way more roads than you actually need. But instead of keeping all these roads open, DST acts as an ingenious city planner: it decides which roads to close down (prune) and which new ones to build (regrow) so that traffic flows optimally‚Äîall while staying within a fixed budget.\n\n\nWe kick things off by initializing our network using one of two methods: - Uniform Initialization: Every layer gets an equal share of connections, kind of like giving every neighborhood the same number of streets. - ErdoÃãs-R√©nyi Kernel (ERK) Initialization: Instead of treating all layers equally, ERK smartly allocates more connections to the ‚Äúbusy‚Äù areas (the layers with more parameters) and fewer to the ‚Äúquiet‚Äù ones. This is a bit like investing more in the main highways that keep the city moving.\n\n\n\nAfter some initial training, the network doesn‚Äôt just sit idle but actively refines itself. Here‚Äôs how it works: - Pruning: Based on the magnitude of the weights, the network prunes away a fixed number (which can be scheduled) of the least important connections. Think of it as cutting off the rarely used, winding side streets to save resources. - Regrowth: To maintain the same overall sparsity level for each task, the network then regrows exactly same amount of connections. And guess what? There are three main approaches to choose which connections to regrow: - Random: Sometimes, a bit of randomness helps explore new possibilities, like testing out an unexpected shortcut. - Unfired: Alternatively, the model can explore the connections that is never tried or checked before. - Gradient-based: Finally, it can look at the gradient or momentum signals that indicate which potential connections could boost performance‚Äîand regrow those most promising ones.\n\n\n\nWhat‚Äôs truly fascinating in this approach is that, even though the network is carving out different ‚Äúsubnetworks‚Äù for each new task, it also allows for sharing connections between tasks. This means that if two tasks are similar, they can use the same neural ‚Äúroad,‚Äù which enhances knowledge sharing and makes learning even more efficient. It‚Äôs like having a communal library where everyone benefits from the same resources!\n\n\n\n\n\n\n\nNow, combine DST with Continual Learning (CL)‚Äîa setup where a single network learns a series of tasks over time without forgetting earlier ones, just like how you can learn to cook a new recipes without forgetting grandma‚Äôs special.\nTraditionally, many systems tackle each new task with a separate network or by keeping extra data around. But here, DST is applied to one overparameterized network that evolves over time. As new tasks come along, the network dynamically adjusts itself by pruning away unused connections and regrowing new ones, while allowing to share common connections to build on past knowledge.\n\n\n\nFor those of you who love the technical nitty-gritty, here‚Äôs a quick summary of the key findings from our study: - Initialization Matters: At low to moderate sparsity levels (around 80‚Äì90%), the ERK initialization leverages the network‚Äôs capacity more efficiently by assigning fewer connections to narrow layers. This helps in learning incremental tasks effectively. At higher sparsities, ERK still doing its job when the right growth strategy is selected. - Growth Strategy Dynamics: The choice of growth strategy is closely tied to the initialization and the degree of sparsity. While gradient-based and momentum-based growth methods excel at high sparsity by carefully selecting promising connections, random growth performs competitively at lower sparsity levels where there‚Äôs less room for exploration. - Adaptive Approaches are Promising: There‚Äôs no one-size-fits-all DST setup. Even a simple adaptive strategy that switches between growth methods (like using random growth initially and shifting to gradient-based growth later) can boost performance compared to sticking with a fixed strategy. - Shared Connections Enhance Knowledge Transfer: Allowing the network to share connections between tasks not only saves resources but also boosts overall learning by transferring knowledge from previous tasks, much like how our brains reuse familiar circuits when learning something new.\n\n\n\n\n\n\nBy starting with a clever initialization, then iteratively pruning and regrowing a fixed number of connections based on weight magnitude and gradient signals, DST transforms a single, overparameterized network into a versatile, continually learning powerhouse. And the cherry on top? Allowing subnetworks to share connections across different tasks means that the network not only learns continuously but also builds upon past knowledge, much like our own brains do.\nIn this study, we dive deep into the various ways to run DST in a continual learning setup, offering both technical insights and inspiration for future research. We believe it is a beautiful blend of neuroscience and engineering‚Äîa step closer to creating AI that learns and adapts just like we do. Again, if you want to know more, you can check the paper üòä\nStay curious, keep exploring, and let‚Äôs continue pushing the boundaries of what intelligent systems can achieve! üß†üöÄ\n\nThis post was written by Murat Onur Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#the-challenge-catastrophic-forgetting",
    "href": "notebooks/cldst-introduction.html#the-challenge-catastrophic-forgetting",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "Traditional neural networks tend to overwrite older information when learning new tasks, which isn‚Äôt very brain-like at all. This major hurdle is called catastrophic forgetting. Imagine if every time you learned a new recipe, you suddenly couldn‚Äôt remember your grandma‚Äôs secret and delicious recipe anymore! That‚Äôs where continual learning (CL) steps in‚Äîallowing models to learn from a stream of data without letting past knowledge vanish into thin air."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#neuroscience-natures-blueprint",
    "href": "notebooks/cldst-introduction.html#neuroscience-natures-blueprint",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "In neuroscience, it‚Äôs well known that our brain is always under a change and on the move! Our synaptic connections are constantly being reshaped or kept steady based on what we experience‚Äîa delightful dance of plasticity and stability that fuels our lifelong ability to learn and adapt. Picture your brain as a vibrant, ever-changing city where new pathways light up and old ones evolve based on your experiences."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#a-smart-way-to-build-a-brain",
    "href": "notebooks/cldst-introduction.html#a-smart-way-to-build-a-brain",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "To mimic this amazing ability, a technique called Dynamic Sparse Training (DST) offers an artificial version of the plasticity and the stability, ensuring that our neural networks remain both efficient and adaptable over time. Imagine starting with an overparameterized network, like a bustling metropolis with way more roads than you actually need. But instead of keeping all these roads open, DST acts as an ingenious city planner: it decides which roads to close down (prune) and which new ones to build (regrow) so that traffic flows optimally‚Äîall while staying within a fixed budget.\n\n\nWe kick things off by initializing our network using one of two methods: - Uniform Initialization: Every layer gets an equal share of connections, kind of like giving every neighborhood the same number of streets. - ErdoÃãs-R√©nyi Kernel (ERK) Initialization: Instead of treating all layers equally, ERK smartly allocates more connections to the ‚Äúbusy‚Äù areas (the layers with more parameters) and fewer to the ‚Äúquiet‚Äù ones. This is a bit like investing more in the main highways that keep the city moving.\n\n\n\nAfter some initial training, the network doesn‚Äôt just sit idle but actively refines itself. Here‚Äôs how it works: - Pruning: Based on the magnitude of the weights, the network prunes away a fixed number (which can be scheduled) of the least important connections. Think of it as cutting off the rarely used, winding side streets to save resources. - Regrowth: To maintain the same overall sparsity level for each task, the network then regrows exactly same amount of connections. And guess what? There are three main approaches to choose which connections to regrow: - Random: Sometimes, a bit of randomness helps explore new possibilities, like testing out an unexpected shortcut. - Unfired: Alternatively, the model can explore the connections that is never tried or checked before. - Gradient-based: Finally, it can look at the gradient or momentum signals that indicate which potential connections could boost performance‚Äîand regrow those most promising ones.\n\n\n\nWhat‚Äôs truly fascinating in this approach is that, even though the network is carving out different ‚Äúsubnetworks‚Äù for each new task, it also allows for sharing connections between tasks. This means that if two tasks are similar, they can use the same neural ‚Äúroad,‚Äù which enhances knowledge sharing and makes learning even more efficient. It‚Äôs like having a communal library where everyone benefits from the same resources!"
  },
  {
    "objectID": "notebooks/cldst-introduction.html#continual-learning-the-ultimate-test",
    "href": "notebooks/cldst-introduction.html#continual-learning-the-ultimate-test",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "Now, combine DST with Continual Learning (CL)‚Äîa setup where a single network learns a series of tasks over time without forgetting earlier ones, just like how you can learn to cook a new recipes without forgetting grandma‚Äôs special.\nTraditionally, many systems tackle each new task with a separate network or by keeping extra data around. But here, DST is applied to one overparameterized network that evolves over time. As new tasks come along, the network dynamically adjusts itself by pruning away unused connections and regrowing new ones, while allowing to share common connections to build on past knowledge."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#a-technical-snapshot",
    "href": "notebooks/cldst-introduction.html#a-technical-snapshot",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "For those of you who love the technical nitty-gritty, here‚Äôs a quick summary of the key findings from our study: - Initialization Matters: At low to moderate sparsity levels (around 80‚Äì90%), the ERK initialization leverages the network‚Äôs capacity more efficiently by assigning fewer connections to narrow layers. This helps in learning incremental tasks effectively. At higher sparsities, ERK still doing its job when the right growth strategy is selected. - Growth Strategy Dynamics: The choice of growth strategy is closely tied to the initialization and the degree of sparsity. While gradient-based and momentum-based growth methods excel at high sparsity by carefully selecting promising connections, random growth performs competitively at lower sparsity levels where there‚Äôs less room for exploration. - Adaptive Approaches are Promising: There‚Äôs no one-size-fits-all DST setup. Even a simple adaptive strategy that switches between growth methods (like using random growth initially and shifting to gradient-based growth later) can boost performance compared to sticking with a fixed strategy. - Shared Connections Enhance Knowledge Transfer: Allowing the network to share connections between tasks not only saves resources but also boosts overall learning by transferring knowledge from previous tasks, much like how our brains reuse familiar circuits when learning something new."
  },
  {
    "objectID": "notebooks/cldst-introduction.html#bringing-all-together-and-wrapping-it-up",
    "href": "notebooks/cldst-introduction.html#bringing-all-together-and-wrapping-it-up",
    "title": "Continual Learning with Dynamic Sparse Training",
    "section": "",
    "text": "By starting with a clever initialization, then iteratively pruning and regrowing a fixed number of connections based on weight magnitude and gradient signals, DST transforms a single, overparameterized network into a versatile, continually learning powerhouse. And the cherry on top? Allowing subnetworks to share connections across different tasks means that the network not only learns continuously but also builds upon past knowledge, much like our own brains do.\nIn this study, we dive deep into the various ways to run DST in a continual learning setup, offering both technical insights and inspiration for future research. We believe it is a beautiful blend of neuroscience and engineering‚Äîa step closer to creating AI that learns and adapts just like we do. Again, if you want to know more, you can check the paper üòä\nStay curious, keep exploring, and let‚Äôs continue pushing the boundaries of what intelligent systems can achieve! üß†üöÄ\n\nThis post was written by Murat Onur Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/lora.html",
    "href": "notebooks/lora.html",
    "title": "LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere",
    "section": "",
    "text": "The world of Large Language Models (LLMs) is scaling at an exponential rate. With training costs for base models reaching millions or even billions of dollars, the traditional method of training a model from scratch for every new task or new data is simply unsustainable. This necessity has divided the LLM development pipeline into two distinct phases: pre-training and post-training.\n\nPre-training: This is the initial, resource-intensive phase where the LLM learns fundamental language patterns, facts, and broad capability through a massive dataset. This results in a base model that is only capable of performing next-token prediction, generating plausible continuations of text, yet it cannot adapt its answer to the user intent.\nPost-training: This phase adapts the highly capable base model for real-world deployment. The goal is to direct its broad knowledge toward specific goals. This process includes:\n\nTask-specific fine-tuning: The general adaptation of the base model to improve performance on new tasks, specific domains, or new data.\nAlignment: A critical refinement process that ensures the model‚Äôs responses are in the correct format, helpful and harmless. This often involves using techniques like Instruction-Tuning (IT), Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO).\n\n\nConsider, for instance, a model that is prompted with the question What is the capital of the Netherlands?. A pre-trained model which is only capable of generating continuations of text might respond by asking another question or by providing broad information about the Netherlands rather than giving a concise answer. However, the user‚Äôs expectation in this case is a clear and direct response - Amsterdam -, and this is where post-training techniques come into play. By adapting the pre-trained model to the user intent, post-training ensures that the model provides a direct answer and only later it follows up with additional relevant context. To illustrate this difference, let‚Äôs compare the outputs of Llama-3.2-1B (base model) with Llama-3.2-1B-Instruct (instruction-tuned model).\n\n\n\nLlama-3.2-1B (base model) and Llama-3.2-1B-Instruct (instruction-tuned model) with maximum length generation set to 150 tokens.\n\n\nWhen the prompt involves new information that the model was not exposed to during pre-training, the base model is unable to produce an accurate answer. In such cases, it often generates plausible but incorrect information, a common behavior in LLMs known as hallucination.\n\n\n\nLlama-3.2-1B (base model) with maximum length generation set to 150 tokens."
  },
  {
    "objectID": "notebooks/lora.html#the-two-phases-of-llm-development",
    "href": "notebooks/lora.html#the-two-phases-of-llm-development",
    "title": "LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere",
    "section": "",
    "text": "The world of Large Language Models (LLMs) is scaling at an exponential rate. With training costs for base models reaching millions or even billions of dollars, the traditional method of training a model from scratch for every new task or new data is simply unsustainable. This necessity has divided the LLM development pipeline into two distinct phases: pre-training and post-training.\n\nPre-training: This is the initial, resource-intensive phase where the LLM learns fundamental language patterns, facts, and broad capability through a massive dataset. This results in a base model that is only capable of performing next-token prediction, generating plausible continuations of text, yet it cannot adapt its answer to the user intent.\nPost-training: This phase adapts the highly capable base model for real-world deployment. The goal is to direct its broad knowledge toward specific goals. This process includes:\n\nTask-specific fine-tuning: The general adaptation of the base model to improve performance on new tasks, specific domains, or new data.\nAlignment: A critical refinement process that ensures the model‚Äôs responses are in the correct format, helpful and harmless. This often involves using techniques like Instruction-Tuning (IT), Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO).\n\n\nConsider, for instance, a model that is prompted with the question What is the capital of the Netherlands?. A pre-trained model which is only capable of generating continuations of text might respond by asking another question or by providing broad information about the Netherlands rather than giving a concise answer. However, the user‚Äôs expectation in this case is a clear and direct response - Amsterdam -, and this is where post-training techniques come into play. By adapting the pre-trained model to the user intent, post-training ensures that the model provides a direct answer and only later it follows up with additional relevant context. To illustrate this difference, let‚Äôs compare the outputs of Llama-3.2-1B (base model) with Llama-3.2-1B-Instruct (instruction-tuned model).\n\n\n\nLlama-3.2-1B (base model) and Llama-3.2-1B-Instruct (instruction-tuned model) with maximum length generation set to 150 tokens.\n\n\nWhen the prompt involves new information that the model was not exposed to during pre-training, the base model is unable to produce an accurate answer. In such cases, it often generates plausible but incorrect information, a common behavior in LLMs known as hallucination.\n\n\n\nLlama-3.2-1B (base model) with maximum length generation set to 150 tokens."
  },
  {
    "objectID": "notebooks/lora.html#the-fine-tuning-problem",
    "href": "notebooks/lora.html#the-fine-tuning-problem",
    "title": "LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere",
    "section": "The Fine-Tuning Problem",
    "text": "The Fine-Tuning Problem\nThe most straightforward form of fine-tuning is defined as Full Fine-Tuning (FullFT) and it involves updating every single weight of the model. While offering high flexibility, FullFT is computationally intensive and demands significant resources.\nThe solution lies in Parameter Efficient Fine-Tuning (PEFT), which updates only a small set of parameters, while the others remain frozen. This approach is significantly less compute and memory instensive and it offers high adaptability across various tasks.\nFor instance, fine-tuning a 7B parameter model using FullFT can require up to 50GB of GPU VRAM and only 20GB using PEFT techniques (actual memory usage depends on batch size, sequence length, and specific model architecture).\n\n\n\nFull fine-tuning vs.¬†parameter efficient fine-tuning."
  },
  {
    "objectID": "notebooks/lora.html#lora-low-rank-adaptation-explained",
    "href": "notebooks/lora.html#lora-low-rank-adaptation-explained",
    "title": "LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere",
    "section": "LoRA: Low-Rank Adaptation Explained",
    "text": "LoRA: Low-Rank Adaptation Explained\nThe most popular PEFT method today is LoRA, which stands for Low-Rank Adaptation. Introduced in 2021, the central idea is rooted in the hypothesis that the update required for fine-tuning a massive pre-trained weight matrix (\\(W\\)) has an intrinsically low rank. This means that while the original weight matrix \\(W\\) (which can be \\(d_{model} \\times d_{model}\\) size) is very large, the necessary change during fine-tuning, \\(\\Delta W = W' - W\\), (where \\(W'\\) is the fine-tuned weight matrix), can be effectively approximated using a technique called low-rank decomposition: \\[\\begin{equation*}\n\\Delta W = \\lambda BA, \\text{ where }\\lambda \\text{ is a scaling factor}.\n\\end{equation*}\\]\nInstead of learning the massive \\(\\Delta W\\) directly, LoRA models it using two smaller trainable matrices: \\(A\\) (size \\(d \\times r\\)) and \\(B\\) (size \\(r \\times k\\)).\n\n\n\nVisualization of LoRA‚Äôs weight update.\n\n\n\nThe role of rank (\\(r\\)) and scaling factor (\\(\\lambda\\))\nThe variable \\(r\\) is the rank of the LoRA matrices. It is a hyperparaneter chosen by the user and it controls the number of trainable parameters. A smaller \\(r\\) leads to fewer parameters and greater memory savings, but a larger \\(r\\) generally allows the adapter to capture more complex task-specific information. Common values are between \\(r=4\\) to \\(r=256\\), based on the dataset size and the amount of new information that must be learned by the model.\nThe scaling factor (\\(\\lambda\\)) control the magnitude of the weight update. It is usually defined as \\(\\lambda = \\frac{\\alpha}{r}\\), where \\(\\alpha\\) is a constant in \\(r\\) and it is used to prevent the scaled update from becoming too large or too small, helping stabilize the training. Tuning \\(\\alpha\\) is roughly the same as tuning the learning rate of the optimizer, so \\(\\alpha\\) is usually set equals to \\(r\\).\n\n\nWhere to apply LoRA\nLoRA is most effective when applied to the core computational layers of the transformer architecture used in LLMs. The standard practice is to apply LoRA only to the query and value projection matrices within the self-attention layer. However, recent studies have shown that applying LoRA also to the MLP component of the feed-forward network layers can yield superior performance, sometimes rivaling FullFT.\n\n\nThe efficiency of LoRA\nThe efficiency of LoRA comes from the choice of the rank. For a weight matrix \\(W\\) of size \\(d \\times k\\):\n\nThe original number of parameters in \\(W\\) is \\(d \\times k\\) (which remains frozen).\nThe number of added trainable parameters by LoRA is:\n\n\\[\\begin{equation*}\nLoRA_{params} = (d \\times r) + (r \\times k) ‚â™ d \\times k\n\\end{equation*}\\]\nFor example, if the original \\(W\\) matrix is \\(4096 \\times 4096\\) and we choose \\(r=8\\):\n\nOriginal parameters: \\(4096 \\times 4096 = 16777216\\).\nLoRA parameters: \\((4096 \\times 8) + (8 \\times 4096) = 32768 + 32768 = 65 536\\), achieving a \\(256\\times\\) reduction.\n\n\n\nAdvantages of LoRA\nThe benefits of using LoRA for fine-tuning are significant:\n\nA dramatic reduction in memory and storage requirements.\nLower GPU VRAM consumption, enabling training on smaller hardware setups.\nOver 25% faster training compared to FullFT.\nEasy task switching by simply swapping the small LoRA adapter modules.\nNo catastrophic forgetting, since the pre-trained weights remain frozen during adaptation."
  },
  {
    "objectID": "notebooks/lora.html#lora-vs.-fullft-who-is-the-winner",
    "href": "notebooks/lora.html#lora-vs.-fullft-who-is-the-winner",
    "title": "LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere",
    "section": "LoRA vs.¬†FullFT: Who is the winner?",
    "text": "LoRA vs.¬†FullFT: Who is the winner?\nWhile FullFT was long considered the gold standard, there is recently a lot of controversy in deciding whether LoRA can achieve similar ultimate performance and sample efficiency as FullFT.\nThe Illusion of Equivalence\nThe paper LoRA vs.¬†full fine-tuning: An illusion of equivalence (Shuttleworth et al., 2024) challenges the idea that LoRA is truly equivalent to FullFT. The work suggests that while LoRA can match FullFT on limited budgets, achieving true equivalence requires precise optimization across all hyperparameters, and even then, in highly complex tasks, FullFT is often preferable in terms of performance. The perceived equivalence is often an illusion created by insufficient optimization of both methods or testing on limited datasets.\nLearning and Forgetting Dynamics\nThe paper LoRA learn less and forget less (Biderman et al., 2024) explores the dynamics of knowledge transfer. It suggests that FullFT is more accurate and sample-efficient than LoRA in the majority of tasks, but LoRA forget less of the original knowledge. This property makes LoRA a safer, more stable choice for many applications, as it preserves the general capabilities of the base model while still adapting to the new task.\nLoRA Without Regret\nThis blog post demonstrate that LoRA can indeed be on par with FullFT but only when a few key factors are chosen correctly. LoRA is most effective for post-training tasks that use small-to-medium-sized datasets, provided these datasets do not exceed LoRA‚Äôs capacity. Optimal performance is achieved when LoRA is applied broadly, including not only the attention matrices but also the MLP layers, and when a sufficiently high rank is used to capture the task‚Äôs complexity. When these best practices are followed, LoRA‚Äôs performance can approach that of FullFT.\nWhen weighing these findings, the consensus is that it remains unclear if LoRA can match FullFT on all tasks. However, it is evident that LoRA is the recommended solution in budget-constrained scenarios. By correctly tuning the parameters and benefiting from its superior knowledge preservation, LoRA offers a powerful tool to fine-tune LLMs with a fraction of the usual cost."
  },
  {
    "objectID": "notebooks/lora.html#beyond-lora",
    "href": "notebooks/lora.html#beyond-lora",
    "title": "LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere",
    "section": "Beyond LoRA",
    "text": "Beyond LoRA\nThe field is rapidly innovating on top of the LoRA foundation, with new techniques further optimizing the process:\n\nQLoRA (Quantized LoRA): This technique quantizes the large, pre-trained weights to 4-bit precision (NF4) while keeping the smaller LoRA matrices in full precision. During backpropagation the original weights are dequantized on-the-fly when needed. This approach achieves an impressive 33% GPU memory saving, at the cost of a 39% increased runtime.\nDoRA (Weight Decomposed LoRA): This method decomposes the pre-trained weights into magnitude and direction. It then applies the LoRA update only to the directional component, resulting in a reported 4% accuracy improvement with minimal added parameters (0.01%).\nAdaLoRA, QALoRA, etc.\n\nFurthermore, techniques like Mixture-of-LoRAs (MoLoRA), such as on models like Phi-4-Mini-Multimodal, use separate, dedicated LoRA adapters for different input types (e.g., for vision and for audio). This parameter-efficient strategy allows a single LLM backbone to seamlessly integrate and process complex multimodal inputs without the cost of retraining the entire system for each new modality.\n\n\n\nSimplified architecture for Phi-4-Mini-Multimodal."
  },
  {
    "objectID": "notebooks/lora.html#conclusion",
    "href": "notebooks/lora.html#conclusion",
    "title": "LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere",
    "section": "Conclusion",
    "text": "Conclusion\nWhile the general question of whether LoRA is preferable than FullFT remains open, it is clear that LoRA is a powerful and versatile fine-tuning technique. It has become the preferred approach in many applications where budget, memory efficiency, or training speed are important considerations. By carefully applying LoRA to MLP layers and selecting an appropriate rank, developers can achieve near-state-of-the-art performance at a fraction of the computational cost, making advanced LLM customization accessible to everyone."
  },
  {
    "objectID": "notebooks/lora.html#references",
    "href": "notebooks/lora.html#references",
    "title": "LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere",
    "section": "References",
    "text": "References\n\nHu, Edward J., et al.¬†‚ÄúLora: Low-rank adaptation of large language models.‚Äù¬†ICLR¬† (2022).\nDettmers, Tim, et al.¬†‚ÄúQlora: Efficient finetuning of quantized llms.‚Äù¬†NeurIPS (2023): 10088-10115.\nLiu, Shih-Yang, et al.¬†‚ÄúDora: Weight-decomposed low-rank adaptation.‚Äù¬†Forty-first International Conference on Machine Learning. 2024.\nShuttleworth, Reece, et al.¬†‚ÄúLora vs full fine-tuning: An illusion of equivalence.‚Äù¬†arXiv preprint arXiv:2410.21228¬†(2024).\nBiderman, Dan, et al.¬†‚ÄúLoRA Learns Less and Forgets Less.‚Äù Transactions on Machine Learning Research. (2024)\nHoulsby, Neil, et al.¬†‚ÄúParameter-efficient transfer learning for NLP.‚Äù¬†ICML (2019).\nAbouelenin, Abdelrahman, et al.¬†‚ÄúPhi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras.‚Äù¬†arXiv preprint arXiv:2503.01743¬†(2025).\nSchulman, John and Thinking Machines Lab, ‚ÄúLoRA Without Regret‚Äù, Thinking Machines Lab: Connectionism, Sep 2025.\nPractical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation). Sebastian Raschka. 2023.\nNew LLM Pre-training and Post-training Paradigms. Sebastian Raschka. 2024.\nComparing PEFT and Full Fine-Tuning Trade-offs. ApXML."
  },
  {
    "objectID": "notebooks/serena-introduction.html",
    "href": "notebooks/serena-introduction.html",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "Murat Onur Yildirim, Elif Ceren Gok Yildirim, Decebal Constantin Mocanu, Joaquin Vanschoren.\nTL;DR: This blog post dives into our recent paper ‚ÄúSelf-Regulated Neurogenesis for Online Data-Incremental Learning‚Äù (üìÑ Paper, ü§ñ Code) published in CoLLAs 2025. SERENA is a brain-inspired continual learning method that detects concept drift and allocates dedicated sparse neural paths called concept cells for each task, enabling efficient and replay-free learning without forgetting. It outperforms state-of-the-art and even offline supervised training in online data-incremental scenarios. Read on for a brief overview of our key findings üòä\n\n\n\nTraditional neural networks are great at recognizing patterns when all data is presented at once. But in the real world, data arrives bit by bit, like a movie watched frame by frame. Most AI systems fall apart here, suffering from catastrophic forgetting, where learning something new erases what was previously learned. Even worse, many current solutions rely on replaying old data (which isn‚Äôt always possible due to privacy) or growing the model‚Äôs architecture indefinitely (which becomes inefficient fast).\n\n\n\nOur new approach takes its inspirations directly from the brain‚Äôs remarkable ability of ‚Äúself-regulated neurogenesis‚Äù to learn and adapt throughout life. It promises to revolutionize how AI handles dynamic, real-world data streams. Specifically, it detects new concepts on its own and carves paths called concept cells which are embedded in a single over-parameterized network, meaning no model growth, no replay buffer. Once a concept is mastered, its corresponding concept cell is ‚Äúfrozen‚Äù, effectively safeguarding that knowledge from future interference.\n\n\n\n\n\n\nSERENA‚Äôs design allows it to operate in real-time, online data-incremental learning scenarios without needing explicit task identifiers or multiple training epochs. Here‚Äôs a simplified look at its core mechanisms:\n\nZero-Cost Path Allocation: When a new concept is detected in the data stream, SERENA allocates a new specialized neural path. This is done efficiently using zero-cost random unstructured pruning.\nDrift Detection: If the model notices it‚Äôs struggling (via accuracy dips), it knows something new is happening and carves a new concept cell.\nKnowledge Preservation: After a concept is learned, its dedicated concept cell is frozen, preventing catastrophic forgetting. This means the model doesn‚Äôt need to ‚Äúre-learn‚Äù old information but can benefit from the existing knowledge while learning the new ones.\nRecency-Effect with Ensemble Inference: When making decisions, SERENA doesn‚Äôt rely on a single neural path. Instead, it employs a neuro-inspired ensemble approach, giving more weight to recently acquired knowledge while still integrating past information. This ‚Äúrecency effect‚Äù enhances adaptability to evolving data distributions.\n\n\n\n\n\n\n\nAcross over ten benchmarks, SERENA doesn‚Äôt just outperform all continual learning baselines, it even beats offline supervised batch learning which sees all data in advance and gets to train on it many times. Let that sink in: a one-pass, online, biologically inspired method outperforms traditional training.\nThis is a monumental achievement, as offline learning typically has the luxury of revisiting data multiple times. SERENA achieves this superior performance without the need for storing subsets of data for replay or expanding the network architecture, addressing critical concerns around computational overhead,privacy, and memory limitations, so it is game-changer and highly ‚ÄúReal-World Ready‚Äù üåç.\n\n\n\n\n\n\nSERENA is more than a clever acronym‚Äîit represents a significant leap forward in making AI truly adaptive and intelligent. By drawing inspiration from how our brains learn, it shows that continual learning in AI doesn‚Äôt have to be clunky, memory-hungry, or brittle. It can be elegant. Efficient. And maybe‚Ä¶ just a little bit human. If you want to know more, you can check the paper üòä\nStay curious, keep exploring, and let‚Äôs continue pushing the boundaries of what intelligent systems can achieve! üß†üöÄ\n\nThis post was written by Murat Onur Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/serena-introduction.html#the-problem-ais-short-term-memory",
    "href": "notebooks/serena-introduction.html#the-problem-ais-short-term-memory",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "Traditional neural networks are great at recognizing patterns when all data is presented at once. But in the real world, data arrives bit by bit, like a movie watched frame by frame. Most AI systems fall apart here, suffering from catastrophic forgetting, where learning something new erases what was previously learned. Even worse, many current solutions rely on replaying old data (which isn‚Äôt always possible due to privacy) or growing the model‚Äôs architecture indefinitely (which becomes inefficient fast)."
  },
  {
    "objectID": "notebooks/serena-introduction.html#serena-self-regulated-neurogenesis-meets-continual-learning",
    "href": "notebooks/serena-introduction.html#serena-self-regulated-neurogenesis-meets-continual-learning",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "Our new approach takes its inspirations directly from the brain‚Äôs remarkable ability of ‚Äúself-regulated neurogenesis‚Äù to learn and adapt throughout life. It promises to revolutionize how AI handles dynamic, real-world data streams. Specifically, it detects new concepts on its own and carves paths called concept cells which are embedded in a single over-parameterized network, meaning no model growth, no replay buffer. Once a concept is mastered, its corresponding concept cell is ‚Äúfrozen‚Äù, effectively safeguarding that knowledge from future interference."
  },
  {
    "objectID": "notebooks/serena-introduction.html#how-serena-works-its-magic",
    "href": "notebooks/serena-introduction.html#how-serena-works-its-magic",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "SERENA‚Äôs design allows it to operate in real-time, online data-incremental learning scenarios without needing explicit task identifiers or multiple training epochs. Here‚Äôs a simplified look at its core mechanisms:\n\nZero-Cost Path Allocation: When a new concept is detected in the data stream, SERENA allocates a new specialized neural path. This is done efficiently using zero-cost random unstructured pruning.\nDrift Detection: If the model notices it‚Äôs struggling (via accuracy dips), it knows something new is happening and carves a new concept cell.\nKnowledge Preservation: After a concept is learned, its dedicated concept cell is frozen, preventing catastrophic forgetting. This means the model doesn‚Äôt need to ‚Äúre-learn‚Äù old information but can benefit from the existing knowledge while learning the new ones.\nRecency-Effect with Ensemble Inference: When making decisions, SERENA doesn‚Äôt rely on a single neural path. Instead, it employs a neuro-inspired ensemble approach, giving more weight to recently acquired knowledge while still integrating past information. This ‚Äúrecency effect‚Äù enhances adaptability to evolving data distributions."
  },
  {
    "objectID": "notebooks/serena-introduction.html#performance-that-surpasses-the-best",
    "href": "notebooks/serena-introduction.html#performance-that-surpasses-the-best",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "Across over ten benchmarks, SERENA doesn‚Äôt just outperform all continual learning baselines, it even beats offline supervised batch learning which sees all data in advance and gets to train on it many times. Let that sink in: a one-pass, online, biologically inspired method outperforms traditional training.\nThis is a monumental achievement, as offline learning typically has the luxury of revisiting data multiple times. SERENA achieves this superior performance without the need for storing subsets of data for replay or expanding the network architecture, addressing critical concerns around computational overhead,privacy, and memory limitations, so it is game-changer and highly ‚ÄúReal-World Ready‚Äù üåç."
  },
  {
    "objectID": "notebooks/serena-introduction.html#final-thoughts-and-wrapping-it-up",
    "href": "notebooks/serena-introduction.html#final-thoughts-and-wrapping-it-up",
    "title": "Self-Regulated Neurogenesis for Online Data-Incremental Learning",
    "section": "",
    "text": "SERENA is more than a clever acronym‚Äîit represents a significant leap forward in making AI truly adaptive and intelligent. By drawing inspiration from how our brains learn, it shows that continual learning in AI doesn‚Äôt have to be clunky, memory-hungry, or brittle. It can be elegant. Efficient. And maybe‚Ä¶ just a little bit human. If you want to know more, you can check the paper üòä\nStay curious, keep exploring, and let‚Äôs continue pushing the boundaries of what intelligent systems can achieve! üß†üöÄ\n\nThis post was written by Murat Onur Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html",
    "href": "notebooks/OpenMLxProbabl-hackathon.html",
    "title": "OpenML x Probabl Hackathon",
    "section": "",
    "text": "scikit-learn is a free and open-source machine learning library for the Python programming language, while OpenML is an open platform for sharing datasets, algorithms, and experiments. While our teams have been working together for many years, we do not always have the time to meet in person. When we do get the chance though, many interesting discussions stem from those conversations.\nLast June we had a developers hackathon at the Paris office of Probabl, the official operating brand of scikit-learn, focused on maintaining and expanding open-source ML in Europe and beyond. We met to discuss not only the state of AI and how both our organizations fit in, but also to brainstorm solutions to challenges faced by our developers and communities.\n\n\n\nMany interesting topics were brought up, most of which were not only relevant to us but also to the broader open-source community. In the spirit of open-source, we wanted to share these insights with you."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#introduction",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#introduction",
    "title": "OpenML x Probabl Hackathon",
    "section": "",
    "text": "scikit-learn is a free and open-source machine learning library for the Python programming language, while OpenML is an open platform for sharing datasets, algorithms, and experiments. While our teams have been working together for many years, we do not always have the time to meet in person. When we do get the chance though, many interesting discussions stem from those conversations.\nLast June we had a developers hackathon at the Paris office of Probabl, the official operating brand of scikit-learn, focused on maintaining and expanding open-source ML in Europe and beyond. We met to discuss not only the state of AI and how both our organizations fit in, but also to brainstorm solutions to challenges faced by our developers and communities.\n\n\n\nMany interesting topics were brought up, most of which were not only relevant to us but also to the broader open-source community. In the spirit of open-source, we wanted to share these insights with you."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#community-engagement-and-onboarding-contributors",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#community-engagement-and-onboarding-contributors",
    "title": "OpenML x Probabl Hackathon",
    "section": "Community Engagement and Onboarding Contributors",
    "text": "Community Engagement and Onboarding Contributors\nThe focus of this discussion was around community engagement and emphasized the importance of effectively attracting, onboarding, and retaining contributors, especially newcomers.\nOver the past few years, both in OpenML and scikit-learn, we have been noticing that a majority of contributors are only active for short durations and do not stick around for too long. The ones that do are distributed between a smaller number of more experienced senior developers and a much larger pool of junior developers. This then leads to many PR‚Äôs being of lower quality and thus requiring a lot more verification and correction.\nThe question at hand then, is not only how we can attract new contributors to our projects but also how we can make it easier for them and our developers to maintain these projects. Some of the ideas that came up have been tried and tested by communities our colleagues have founded or been part of across the globe.\nTakeaways:\n\nEmotional connection : All of the participants agreed that the most important part of any community is its people. Contributors only stick around if they have an emotional connection to either the project, or the people contributing to the project. In this vein, it would be nice if the maintainers of the project could also be present at events. It also helps if the contributors use the projects themselves.\nFocus on beginners : Since we see that most of our contributors are beginners, it serves to organize sprints that are inclusive of them with a focus on beginner-friendly issues, especially documentation tasks. Having these would not only help them understand the project and contribute better, but also let them form a connection with the project.\nCurated issues : Most of our external contributors have enough on their plate already. Having a curated list of issues before sprints would ensure that no time is wasted and let our contributors focus on the tasks suitable for them.\nDifferent tiers of events : To ensure that everyone is given tasks they can handle, it would be nice to have separate events for contributors with varying levels of expertise. This would also have the added benefit of retaining beginner-friendly issues for sprints to prevent more experienced contributors from claiming them early.\nMentoring : Incorporating one-to-one mentoring to help new contributors set up their development environments would help them feel more connected to the project. It is understandable that this is time consuming, so perhaps some way of deciding who gets mentored can be set up in time.\nContributors guide : Simplifying the contributor‚Äôs guide and adding video tutorials would make it a lot more beginner-friendly, especially to those who have never filed a pull request (PR) before.\nSemi regular events : Some of our colleagues found that they tended to care about a project more if there were semi regular events they could set time aside for. Having these helped them slowly build a sense of community as well.\nIncentives : A common question that many developers have is why they should even bother contributing. Helping them understand how contributing to open source projects can aid their careers, bring them closer to the community and also help them get internships would be a good start."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#governance-funding-and-sponsorship",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#governance-funding-and-sponsorship",
    "title": "OpenML x Probabl Hackathon",
    "section": "Governance, Funding, and Sponsorship",
    "text": "Governance, Funding, and Sponsorship\nThis focus of this discussion was the governance structures of open-source projects, sustainable funding models, and the role of sponsorships in supporting project activities.\nTakeaways:\n\nEvolving Governance : Since governance is not static, we can treat it as a living document that evolves with the needs of the community.\nCommunication: It is good practise to maintain open communication channels, such as mailing lists and monthly meetings. This keeps all interested contributors in the loop.\nPull Requests: When a project becomes popular on GitHub, it often receives a large number of pull requests from the broader community, many of which may lack quality and become a time sink for senior developers. Finding a balance between approving and rejecting these contributions, without making the community feel unheard, remains an ongoing challenge.\nSponsorship: It would be beneficial to explore sponsorship models similar to the INRIA Foundation (website is in French), where sponsors have a voice in company policy during bi-annual meetings. Such a policy could also provide an added incentive for companies to sponsor, especially if they frequently use the products of their beneficiaries.\nCorporate partnerships: To keep investors interested, it would also be interesting to look into corporate partnerships (similar to those used by the Linux Foundation) that are of mutual benefit."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#development-tooling-and-workflows",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#development-tooling-and-workflows",
    "title": "OpenML x Probabl Hackathon",
    "section": "Development Tooling and Workflows",
    "text": "Development Tooling and Workflows\nSetting up and maintaining CI/CD pipelines across multiple repositories and programming languages puts a huge burden on the maintainers of the repositories.\nHaving to keep up with the trends and learn new technology very frequently also makes developers more reluctant to change the stack, even if doing so would be beneficial.\nThis discussion looked at how to tackle these challenges and make it easier for developers to handle such complex workloads.\nTakeaways:\n\nAutomation: Automating as much of the CI/CD pipeline as possible by using bots for linting and code coverage checks makes PR quality control easier.\nBetter workflows: Migrating to Github Actions and Azure workflows for testing and deployment also seems to help significantly.\nOpen source tools: There are many open source platforms/tools that offer comparable convenience to tools that are currently used. Some examples of these are CodeBerg and Forgejo as an alternative to GitHub and its issue trackers. Eventually migrating to using these tools might also help manage projects like scikit-learn and OpenML.\nCircleCI: Tools for rendering documentation examples directly in the browser can also be used to enhance the review process."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#broader-ecosystem-and-scope-of-collaboration",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#broader-ecosystem-and-scope-of-collaboration",
    "title": "OpenML x Probabl Hackathon",
    "section": "Broader Ecosystem and Scope of Collaboration",
    "text": "Broader Ecosystem and Scope of Collaboration\nWhile both OpenML and scikit-learn focus on the open source ML community, it is sometimes hard to explain how we fit into the broader AI ecosystem.\nEspecially with the rise of LLMs and Generative AI, stakeholders are somewhat inclined to think that frameworks such as ours are just not ‚Äúenough‚Äù. Of course, this is not true at all.\nTakeaways:\n\nCommunity projects: There are so many successful examples of open source projects, and a lot can be learned from their efforts. Projects like Scientific Python for example, have very well set up CI documentation and governance processes that can be quite readily applied to any open source project.\nExploring connections: A longer term focus for both our teams would be to explore connections with other open-source ML frameworks, such as PyTorch, Tensorflow as well as AutoML tools. Doing so would also help integrate and strengthen the open-source ML communities and ecosystems."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#discussion-on-croissant",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#discussion-on-croissant",
    "title": "OpenML x Probabl Hackathon",
    "section": "Discussion on Croissant",
    "text": "Discussion on Croissant\nMachine Learning datasets can consist of structure, unstructured data, or both, which makes them all the more complicated to manage. This has led to the rise of multiple ‚Äúdataset formats‚Äù which further make it hard to consistently load data across platforms and tools.\nCroissant is a dataset metadata format which, among other things, describes how to load and interpret the dataset, which helps load data regardless of which underlying dataset format is used. This format is now not only compatible with the most popular ML libraries/platforms (scikit-learn, PyTorch, Tensorflow, Kaggle, Hugging Face), it is also recommended by NeurIPS (one of the topmost conferences in the AI space).\nFeatures of Croissant:\n\nSchema.org: Croissant was built on top of schema.org with more metadata information specific to ML datasets. Since it does not require any changes to the underlying data structure, existing datasets can quite easily be converted to use it.\nLayers: The format has 4 layers - Dataset level metadata, resource descriptions, content structure, and ML semantics. Each of which make it possible to encode and maintain structural information about datasets regardless of platform.\nXAI and Visualization: Analysis and visualisation of the data works out of the box for all datasets and across multiple platforms. Croissant also supports the Core RAI vocabulary for explainable AI.\nSupported Platforms: Every dataset in OpenML has a Croissant representation, while a majority of data on Kaggle and Google Dataset search also support it."
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#conclusion",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#conclusion",
    "title": "OpenML x Probabl Hackathon",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\nOverall, this discussion was quite a successful one for both of our teams. We learnt a lot from each other and found new ways of collaborating on our shared dream of open-source ML. So much in fact, that we wanted to share our discussion with you, dear reader.\nWe hope you learnt something new. We would love to welcome you to our community and would be glad to support your journey in this ML space.\n‚ù§Ô∏è The OpenML and scikit-learn team"
  },
  {
    "objectID": "notebooks/OpenMLxProbabl-hackathon.html#reach-out",
    "href": "notebooks/OpenMLxProbabl-hackathon.html#reach-out",
    "title": "OpenML x Probabl Hackathon",
    "section": "Reach out",
    "text": "Reach out\n\nFor more information about this hackathon or just to chat with us, feel free to reach out to us on OpenML Email, Probabl. To contact the authors, send them an email here - Subhaditya Mukherjee, Emily Chen."
  },
  {
    "objectID": "pathfinder/index.html",
    "href": "pathfinder/index.html",
    "title": "AI Pathfinder",
    "section": "",
    "text": "This webpage is now archived\n\nPlease visit AI Pathfinder for the latest news (from Jan 2026).\n\nUnleash AI potential for your business\n\n\n\n Partner with a leading AI lab and transform your SME with customized AI solutions \n\n\nAI is revolutionizing how businesses operate: from automating tasks and improving efficiency, to supporting customers and answering enquiries. AI Pathfinder connects Dutch SMEs with expertise to build practical AI solutions that address your specific challenges. We co-create AI-driven solutions tailored to your business, with practical value and sustainable integration.\n\n\nRecent project examples include: automatically simplifying and organizing complex customer orders, AI health care agent chatbot with guardrails and EU data protection,\n\n\nWhat we offer:\n\n\n\nExpert guidance from AI specialists working directly within your team\n\n\nA flexible collaboration model starting with brainstorming sessions and growing to proofs-of-concept and prototypes, and beyond\n\n\nAccess to top AI tools, platforms and expertise through the Technical University of Eindhoven (TU/e) Data and AI Lab\n\n\n\nWhy choose AI Pathfinder?\n\n\n\nWe link state-of-the-art techniques and business needs to achieve tangible results\n\n\nOur people-centric approach focuses AI solutions on real business problems\n\n\nWe strengthen your team‚Äôs digital competencies for long-term success\n\n\n\nExample projects from the lab:\n\n\n\nVision: smart automated monitoring of plants for greenhouse optimization\n\n\nSensors: performance analysis of electrocardiogram monitoring patch signal quality\n\n\nModelling: a tool to estimate the energy savings of potential home renovations\n\n\n\nInterested? Let‚Äôs discover how a collaboration with a leading AI lab can transform your business. Contact us to explore the possibilities: md@aipathfinder.eu"
  },
  {
    "objectID": "blogs/blog.html",
    "href": "blogs/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Sort by:\n    \n        Date\n        Type\n    \n    View:\n    \n        List\n        Cards\n    \n\n\n\n\n\n    \n        \n            \n                \n            \n            \n                LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere\n                Analysis 2025-10-10\n                 We explain how to get near full fine-tuning performance with a fraction of the cost, a small set of parameters, and less memory, with LoRA. Read before your next fine-tuning job.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Self-Regulated Neurogenesis for Online Data-Incremental Learning\n                Method 2025-06-05 CoLLAs 2025\n                We present SERENA, a neuro-inspired solution for continual learning that mimics the self-regulated neurogenesis process in the human brain.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Continual Learning with Dynamic Sparse Training\n                Analysis 2025-06-04 CPAL 2024\n                We show how sparse training helps us learn much faster while forgetting less. Based on our CPAL paper \"Continual Learning with Dynamic Sparse Training\".\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Continual Learning with Informative Samples\n                Analysis 2025-06-03\n                We show how continual learning benefits from selecting only the more informative (or surprising) new data points.\".\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Meta-learning for Likelihood-free Bayesian Optimization\n                Method 2025-06-02 ICML 2024\n                We introduce MALIBO, a novel and scalable framework that leverages meta-learning for fast and efficient Bayesian optimization.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Adaptive Continual Learning\n                Method 2025-06-01 ContinualAI Unconference 2023\n                We introduce AdaCL, a new method that optimally adapts continual learning hyperparameters to every new task.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                The AutoML Benchmark\n                Benchmarking 2024-12-06 JMLR\n                About why we wrote our paper \"AMLB: an AutoML Benchmark\" and its main contributions.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                OpenML x Probabl Hackathon\n                Hackathon 2024-09-19\n                We visited Probabl in Paris to discuss open source and open science. \n            \n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    \n            LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere\n             We explain how to get near full fine-tuning performance with a fraction of the cost, a small set of parameters, and less memory, with LoRA. Read before your next fine-tuning job.\n        \n        \n            2025-10-10\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    CoLLAs 2025\n            Self-Regulated Neurogenesis for Online Data-Incremental Learning\n            We present SERENA, a neuro-inspired solution for continual learning that mimics the self-regulated neurogenesis process in the human brain.\n        \n        \n            2025-06-05\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    CPAL 2024\n            Continual Learning with Dynamic Sparse Training\n            We show how sparse training helps us learn much faster while forgetting less. Based on our CPAL paper \"Continual Learning with Dynamic Sparse Training\".\n        \n        \n            2025-06-04\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    \n            Continual Learning with Informative Samples\n            We show how continual learning benefits from selecting only the more informative (or surprising) new data points.\".\n        \n        \n            2025-06-03\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    ICML 2024\n            Meta-learning for Likelihood-free Bayesian Optimization\n            We introduce MALIBO, a novel and scalable framework that leverages meta-learning for fast and efficient Bayesian optimization.\n        \n        \n            2025-06-02\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    ContinualAI Unconference 2023\n            Adaptive Continual Learning\n            We introduce AdaCL, a new method that optimally adapts continual learning hyperparameters to every new task.\n        \n        \n            2025-06-01\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Benchmarking\n    JMLR\n            The AutoML Benchmark\n            About why we wrote our paper \"AMLB: an AutoML Benchmark\" and its main contributions.\n        \n        \n            2024-12-06\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Hackathon\n    \n            OpenML x Probabl Hackathon\n            We visited Probabl in Paris to discuss open source and open science. \n        \n        \n            2024-09-19\n            ‚û§\n        \n    \n    Analysis\n    \n        \n            \n                \n            \n            \n                LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere\n                Analysis 2025-10-10\n                 We explain how to get near full fine-tuning performance with a fraction of the cost, a small set of parameters, and less memory, with LoRA. Read before your next fine-tuning job.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Continual Learning with Dynamic Sparse Training\n                Analysis 2025-06-04 CPAL 2024\n                We show how sparse training helps us learn much faster while forgetting less. Based on our CPAL paper \"Continual Learning with Dynamic Sparse Training\".\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Continual Learning with Informative Samples\n                Analysis 2025-06-03\n                We show how continual learning benefits from selecting only the more informative (or surprising) new data points.\".\n            \n        \n    \n    Benchmarking\n    \n        \n            \n                \n            \n            \n                The AutoML Benchmark\n                Benchmarking 2024-12-06 JMLR\n                About why we wrote our paper \"AMLB: an AutoML Benchmark\" and its main contributions.\n            \n        \n    \n    Hackathon\n    \n        \n            \n                \n            \n            \n                OpenML x Probabl Hackathon\n                Hackathon 2024-09-19\n                We visited Probabl in Paris to discuss open source and open science. \n            \n        \n    \n    Method\n    \n        \n            \n                \n            \n            \n                Self-Regulated Neurogenesis for Online Data-Incremental Learning\n                Method 2025-06-05 CoLLAs 2025\n                We present SERENA, a neuro-inspired solution for continual learning that mimics the self-regulated neurogenesis process in the human brain.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Meta-learning for Likelihood-free Bayesian Optimization\n                Method 2025-06-02 ICML 2024\n                We introduce MALIBO, a novel and scalable framework that leverages meta-learning for fast and efficient Bayesian optimization.\n            \n        \n    \n    \n    \n        \n            \n                \n            \n            \n                Adaptive Continual Learning\n                Method 2025-06-01 ContinualAI Unconference 2023\n                We introduce AdaCL, a new method that optimally adapts continual learning hyperparameters to every new task.\n            \n        \n    \n    Analysis\n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    \n            LoRA and Beyond. Fine-Tuning LLMs for Anyone, Anywhere\n             We explain how to get near full fine-tuning performance with a fraction of the cost, a small set of parameters, and less memory, with LoRA. Read before your next fine-tuning job.\n        \n        \n            2025-10-10\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    CPAL 2024\n            Continual Learning with Dynamic Sparse Training\n            We show how sparse training helps us learn much faster while forgetting less. Based on our CPAL paper \"Continual Learning with Dynamic Sparse Training\".\n        \n        \n            2025-06-04\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Analysis\n    \n            Continual Learning with Informative Samples\n            We show how continual learning benefits from selecting only the more informative (or surprising) new data points.\".\n        \n        \n            2025-06-03\n            ‚û§\n        \n    \n    Benchmarking\n    \n        \n            \n        \n        \n            \n    \n        Benchmarking\n    JMLR\n            The AutoML Benchmark\n            About why we wrote our paper \"AMLB: an AutoML Benchmark\" and its main contributions.\n        \n        \n            2024-12-06\n            ‚û§\n        \n    \n    Hackathon\n    \n        \n            \n        \n        \n            \n    \n        Hackathon\n    \n            OpenML x Probabl Hackathon\n            We visited Probabl in Paris to discuss open source and open science. \n        \n        \n            2024-09-19\n            ‚û§\n        \n    \n    Method\n    \n        \n            \n        \n        \n            \n    \n        Method\n    CoLLAs 2025\n            Self-Regulated Neurogenesis for Online Data-Incremental Learning\n            We present SERENA, a neuro-inspired solution for continual learning that mimics the self-regulated neurogenesis process in the human brain.\n        \n        \n            2025-06-05\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    ICML 2024\n            Meta-learning for Likelihood-free Bayesian Optimization\n            We introduce MALIBO, a novel and scalable framework that leverages meta-learning for fast and efficient Bayesian optimization.\n        \n        \n            2025-06-02\n            ‚û§\n        \n    \n    \n    \n        \n            \n        \n        \n            \n    \n        Method\n    ContinualAI Unconference 2023\n            Adaptive Continual Learning\n            We introduce AdaCL, a new method that optimally adapts continual learning hyperparameters to every new task.\n        \n        \n            2025-06-01\n            ‚û§"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab",
    "section": "",
    "text": "Welcome to our lab on Advanced Models through Open Research and Engineering @ TU/e!"
  },
  {
    "objectID": "index.html#mission",
    "href": "index.html#mission",
    "title": "Lab",
    "section": "Mission",
    "text": "Mission\n\n\n\n\n\nOur mission is to scientifically understand and build AI systems with advanced capabilities, make AI accessible to benefit all of humanity, and illuminate the world.\nWe invent new neural network architectures and train them in new ways to learn better and faster, study models systematically, and use our insights to automate this process so that AI systems can self-assemble and optimize themselves. Everything we create is open-source and crafted with user-friendliness in mind.\nIn this pursuit, we created OpenML, a global open platform with myriad curated datasets, AI models, and experiments, for streamlining machine learning research and making it accessible to all. We perform cutting-edge research on Automated Machine Learning (AutoML), Meta-Learning, Continual Learning, Foundation Models, Open-Endedness, and related fields, and validate our methods in projects that tackle sustainable development goals, including health care, food security, and climate change.\nYou can find our work in top AI conferences and journals, and we take pride in good engineering and open science to build AI models and systems that are widely used by people every day.\nWe are integrated in the Data and AI cluster, which does excellent research in all aspects of AI."
  },
  {
    "objectID": "index.html#recent-highlights",
    "href": "index.html#recent-highlights",
    "title": "Lab",
    "section": "Recent highlights",
    "text": "Recent highlights\n\n\nTop paper in Cell Patterns\nWe‚Äôre thrilled that our paper OpenML: Insights from 10 years and more than a thousand papers is among the editor picks as one of the best papers in the Patterns journal in 2025.\n\n\nTwo new papers at CoLLAs 2025\nWe‚Äôre presenting SERENA, a surprisingly efficient neuro-inspired approach for continual learning using sparse models, as well as SuperNet Transfer, a new method that efficiently adapts neural architectures during transfer learning.\n\n\nNeurIPS Spotlight for Croissant\nCroissant is a new standard for describing machine learning datasets, making it easier to share and reuse data and load it automatically into AI libraries. Joined work with Google, HuggingFace, Kaggle, MLCommons, and many more. Join us at NeurIPS 2024!\n\n\nICML Spotlight for MALIBO\nMALIBO is a neural network that meta-learns how to tune hyperparameters - it learns across many prior tasks to tune models much more efficiently than other techniques. Jiarong Pan will present his work as a spotlight talk at ICML 2024."
  },
  {
    "objectID": "index.html#join-us",
    "href": "index.html#join-us",
    "title": "Lab",
    "section": "Join us!",
    "text": "Join us!\nPhD, PostDoc, or AI Engineer positions: please check out our open positions. Without an open position, you need to be self-funded or have a scholarship.\nVisiting researchers: We love discussing new ideas and collaborations! Note that we generally expect you to stay for at least a few months. Our lab is generally not able to offer paid internships."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Lab",
    "section": "Contact",
    "text": "Contact\nE-mail: amore@tue.nl\nAddress: Groene Loper 5, Metaforum 7.104, 5600 MB Eindhoven, The Netherlands\n\n\n\n\nThis website is powered by Quarto, with thanks to Drew Dimmery."
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "2025\nELLIOT: European Large Open Multi-modal Foundation ModelsEU Horizon Europe (2025-2030)ELLIOT aims to develop the next generation of Multimodal Space-Time Foundation Models. More info soon!\n        \n        Project website\n    \nOpenEuroLLM: Open European Family of Large Language ModelsEU Horizon Europe (2025-2029)OpenEuroLLM aims to build an entirely open family of Large Language Models, publishing all data, code, and models in an open way.\n        \n        Project website\n    \nLLMs4EU: Large Language Models for the European UnionDigital Europe (2025-2029)LLMs4EU is OpenEuroLLMs sister project, aiming to develop infrastructure to robustly evaluate and exploit open LLMs. More info soon!\n        \n        Project website\n    \nMOSAIC: Electronic Components and Systems for our Automated Digital FutureEU Horizon Europe (2025-2029)MOSAIC aims to develop the next generation of electronic components offering superior cognitive system intelligence, efficiency, and robustness. More info soon!\n        \n        Project website\n    \nAssessment of Learning technologies and Frameworks for Intelligent and Ethical AIEU Horizon Europe (2025-2028)ALFIE aims to harness AI responsibly with essential skills, ethical practices, and people-centered, trustworthy AI. Our AutoML platform streamlines AI evaluation, ensuring performance aligns with ethical standards and societal values. Together, we‚Äôll build a transparent, democratic AI ecosystem for the public good.\n        \n        Project website\n    \nAI PathfinderEDIH (2025-2025)Connecting Dutch SMEs with expertise to build practical AI solutions that addresses their specific challenges.\n        \n        Project website\n    \n2024\nSYNERGIESEU Horizon Europe (2024-2027)SYNERGIES will enhance the development, training, virtual testing, and validation of cooperative, connected and automated mobility systems.\n        \n        Project website\n    \nEDIH-SNLEU Digital Europe (2024-2025)The European Digital Innovation Hub South Netherlands aims to accellerate the digital transformation of manufacturing and maintenance SMEs.\n        \n        Project website\n    \nAutomated Machine Learning for allDutch Science Foundation, Open Science Fund (2024-2025)This project leverages OpenML and AutoML to automatically build AI models and provide intuitive reports to help scientists make progress across many different fields.\n        \n        Project website\n    \n2022\nMachine Learning for building renovationsDutch Government (2022-2026)To make cities more sustainable, this project uses machine learning to predict the energy performance of buildings and optimize renovation strategies.\n        \n        Project website\n    \nDigital Twin of a Vertical FarmDutch Science Foundation, Merian Fund (2022-2026)Vertical farming allows us to grow more food using less resources. We use AI to model plants in 3D to understand how different light, CO2 and temperature scenarios impact plant growth and photosynthesis.\n        \n        Project website\n    \nAI4EuropeEU Horizon Europe (2022-2025)The AI-on-Demand platform (AIoD) is a community-driven platform designed to empower European research and innovation in Artificial Intelligence (AI).\n        \n        Project website\n    \n2020\nStairway to AIEU Horizon 2020 (2020-2024)The StairwAI project aims to provide a matchmaking service for users of the AI-on-Demand platform to easily find AI assets, experts, knowledge, hardware resource providers and much more.\n        \n        Project website\n    \nContinuous monitoring in personal and physical healthITEA Inno4Health (2020-2024)Inno4Health stimulates continuous monitoring in personal and physical health, improving patient care and athlete performance.\n        \n        Project website\n    \nMulti Modal PhotochemistryDutch Science Foundation, TTW (2020-2024)In this project, we aimed to optimize the photochemical synthesis of complex molecules using machine learning.\nTAILOR Network of AI ExcellenceEU Horizon 2020 (2020-2024)TAILOR is one of the first European networks of research excellence in AI, focussing on Trustworthy AI.\n        \n        Project website\n    \nSkyHigh: Leveraging AI in Vertical FarmingDutch Science Foundation (2020-2024)Vertical farming allows us to grow more food using less resources. We use AI to track plant growth non-invasively and optimize growth.\n        \n        Project website\n    \n2019\nEducational platform for machine learning and medical image analysisTU Eindhoven, BOOST (2019-2025)AI education should be scalable and engaging. This project aims to leverage OpenML for AI-related university courses, challenging students to build the best AI models in an engaging environment.\n        \n        Project website\n    \nThe AutoML GymAmazon Research Award (2019-2020)This project aimed to evolve AutoML systems (agents) in an environment of increasingly difficult tasks, in which AutoML agents can be uploaded as docker images and run on AWS infrastructure.\n        \n        Project website\n    \n2017\nDynamic Data Analytics through Automatically Constructed Machine Learning PipelinesDutch Science Foundation, Commit2Data (2017-2021)This project created new online automated machine learning pipelines, new methods for multi-variate time series prediction, and new approaches for early stage Parkinson's disease diagnostics from videos.\n        \n        Project website\n    \nData Driven Discovery of ModelsDARPA (2017-2021)The first DARPA challenge on AutoML, the Data-Driven Discovery of Models (D3M) program developed automated methods to create empirical models of real, complex processes. It also lead to the creation of the AutoML benchmark.\n        \n        Project website\n    \n2016\nA Cloud-Based Platform for AutoMLMicrosoft Azure Research Award (2016-2016)With sponsorship from Microsoft, we ran the first large-scale machine learning benchmarks on Azure, which are still accessible on OpenML today.\n        \n        Project website\n    \n2012\nMassively Collaborative Machine LearningDutch Science Foundation, Free Competition (2012-2016)This project created OpenML, an open science platform for sharing data, code, and experiments in machine learning.\n        \n        Project website\n    \nMLOpen Machine Learning PlatformEU PASCAL Harvest (2012-2013)This exploratory project brought together scientists and engineers to create an open machine learning platform, forming the foundation of the OpenML community."
  },
  {
    "objectID": "join/join.html",
    "href": "join/join.html",
    "title": "Join us!",
    "section": "",
    "text": "Impressions of our campus.\n\n\n\nCurrent openings\nWe currently (temporarily) do not have any fully-funded positions for Ph.D.¬†students, PostDocs, and Engineers. Please check again later.\nIf you have your own sources of funding, please get in touch with us.\nYou can also check out other positions in our department.\n\n\nWho we are looking for\nWe are seeking exceptional individuals who aspire to leave a mark on the world of machine learning and push humanity forward\n\nFearless Explorers: Visionaries who dare to think big, embrace curiosity, and thrive on pushing the boundaries of innovation.\nDedicated Builders: Innovators who turn ambitious ideas into reality, creating solutions with real-world impact.\nEmpowered Leaders: Changemakers ready to unlock their full potential, inspire others, and define the future of machine learning.\n\nWe especially appreciate people who are curious and open-minded, self-driven and collaborative, kind and respectful, and who contribute to our shared passion for research and exploration. If you‚Äôre passionate about making a difference and pursuing excellence, we want you to be part of our journey.\n\n\nHow we will empower you\n\nWe cultivate an environment that inspires and supports world-class research. Our focus is on quality over quantity, encouraging innovative, blue-sky thinking while embracing openness, transparency, and learning from mistakes. Above all, we prioritize an excellent work-life balance, with minimal meetings, plenty of time for focused work, and numerous opportunities to learn and collaborate with one another.\n\nFully Funded for Success\nAll positions are fully funded, so you can focus entirely on your research and growth. PhD positions are always funded for 4 years. PostDoc and Research Engineer positions are usually for 2-4 years, with possibilities for extension. All positions come with a good salary determined on your experience and seniority, in accordance with the Dutch universities‚Äô labor agreement.\n\n\nWorld-leading research\nWe change the world, together. We‚Äôre a tight-knit team of about 20 PhD‚Äôs, postdocs, and engineers working together to create real impact. You can find our work in top AI conferences and journals, including NeurIPS, ICML, CVPR, JMLR, and TPAMI, and we take pride in good engineering and open science, building AI tools used by hundreds of thousands of people.\n\n\nFreedom to Study Deeply and Think Big\nWe don‚Äôt just follow trends ‚Äî we set them. We encourage every team member to become a leading expert, think outside the box, and find novel avenues of research. We work on extremey efficient continual and meta-learning inspired by the human brain, we design AI models that design themselves using AutoML, we build platforms and create standards that enable research that wasn‚Äôt possible before, and even create new venues (e.g.¬†NeurIPS Datasets & Benchmarks) to create new incentives and strengthen communities.\n\n\nPurpose-Driven Research\nYour work here will matter. Next to doing transformative fundamental research, we also leverage our insight to tackle real-world problems, in line with AI for good and sustainable development goals.\n\n\nExcellent Career Prospects\nResearch labs and employers around the globe recognize the quality of our researchers, and we collaborate extensively with other leading research labs and organizations all over the world, such as Google, HuggingFace, and MLCommons."
  },
  {
    "objectID": "software/software.html",
    "href": "software/software.html",
    "title": "Software",
    "section": "",
    "text": "This page provides an overview of the software that we built with the goal of transforming the way people do (automated) machine learning research in a way that is more open, reproducible, and accessible. Most of the software below is developed with collaborators outside of our lab.\nWe also open-source the code for our papers under our GitHub organisation. Code for each paper is also linked in the overview in the papers page.\nOpenML\nMachine learning research should be easily accessible and reusable. OpenML is an open platform for sharing datasets, algorithms, and experiments - to learn how to learn better, together.\n\n        \n        Website\n     \n        \n        Github\n    \nAutoML Benchmark\nThe AutoML Benchmark (AMLB) is software that performs end-to-end evaluation of AutoML frameworks. It can be used to compare current state-of-the-art and evaluate new methods with ablation studies. Over 10 AutoML frameworks can be evaluated out-of-the-box on over 100 datasets, and it‚Äôs easy to bring your own data or AutoML framework.\n\n        \n        Website\n     \n        \n        Github\n    \nGAMA\nThe General Automated Machine learning Assistant (GAMA) was developed to be able to easily experiment with new AutoML design through ablation studies by providing a modular AutoML framework. We have sunset the project, and decided to focus our efforts on OpenML and AMLB. If you are interested in a modern implementations driven by similar philosophies, please have a look at the AutoML Toolkit.\n\n        \n        Website\n     \n        \n        Github\n    \nSERENA\nSelf-Regulated Neurogenesis for Online Data-Incremental Learning (SERENA) is a neuro-inspired lightweight and efficient method for Online Data-Incremental Learning, designed to continually adapt to streaming data without forgetting the past knowledge.\n\n        \n        Github\n    \nCL-with-DST\nContinual Learning with Dynamic Sparse Training (CL-with-DST) investigates the evolution of sparse network topologies within the continual learning framework, where data arrives sequentially in a streaming fashion. This approach dynamically adapts the sparse structure of the network over time, enabling efficient learning from non-stationary data while mitigating forgetting.\n\n        \n        Github"
  },
  {
    "objectID": "people/people.html",
    "href": "people/people.html",
    "title": "People",
    "section": "",
    "text": "Permanent researchers\n\n\n\n    \n        \n            \n        \n        \n            \n                Joaquin Vanschoren is an associate professor and head of the AMOR/e lab at TU Eindhoven. He aims to scientifically understand (human-like) intelligence and build AI systems with advanced capabilities for the benefit of all humanity. He authored the first book on AutoML, gave tutorials at NeurIPS and AAAI and won several awards, including the Dutch Data Prize and Amazon Research Award. He founded OpenML, a useful open science platform for machine learning, and co-founded the Croissant standard for sharing AI resources. He was the inaugural chair of the NeurIPS Datasets and Benchmarks track, editor-in-chief of the DMLR journal, and co-chair of the MLCommons AI Risk & Reliability working group. He is a founding member of the European AI societies ELLIS and CAIRNE.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nResearch engineers\n\n\n\n    \n        \n            \n        \n        \n            \n                Pieter Gijsbers is working on making (automated) machine learning research simple through developing open source software. He is a long-term contributor to openml-python, started the AutoML Benchmark and GAMA. Pieter is currently working on improving the AI-on-Demand platform.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Subhaditya Mukherjee is a Research Engineer working on various engineering and machine learning tasks for openml.org. He is currently working on making the OpenML experience better and more user-friendly.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nLecturers\n\n\n\n    \n        \n            \n        \n        \n            \n                Prabhant Singh I am currently working as a lecturer and researcher. I have experience in Python, data and systems as well as infrastructure engineering.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nPostdocs\n\n\n\n    \n        \n            \n        \n        \n            \n                Alexis Cvetkov-Iliev is a postdoc in the group, applying machine-learning techniques to study and optimize building renovation. His research interests include active learning, Bayesian optimization, and transfer learning.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Bram Grooten is a postdoc working on multimodal foundation models for robotic manipulation, focusing on generalization across tasks and environments. He is finishing his PhD in deep reinforcement learning at the TU Eindhoven in April 2026. His doctoral research focused on increasing the adaptability of deep RL agents to challenging out-of-distribution scenarios, such as unseen vehicles or backgrounds. During his PhD, he worked at the University of Alberta for five months and interned at Sony AI in the Gran Turismo team. In 2021, Bram received his double MSc degree in Applied Mathematics and Science Education.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Thilina Rajapakse is a postdoc working on the LLMs4EU project. His research focus is on Automated Red Teaming for LLMs. His PhD is on using Transformer models for Search and was conducted at the University of Amsterdam.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Anna Vettoruzzo is a postdoctoral researcher contributing to the OpenEuroLLM project, where she focuses on designing scalable model architectures for Large Language Models (LLMs) and developing efficient post-training techniques. She received her Ph.D. in machine learning from Halmstad University, Sweden, as part of the Center for Applied and Intelligent Systems Research (CAISR). Her doctoral research focused on enhancing the generalization capabilities of machine learning models through meta-learning, exploring how models can learn to learn. Anna received her M.Sc. degree in ICT for Internet and Multimedia from the University of Padova, in 2021, with a focus on machine learning for healthcare. Her broader research interests include model generalization and adaptability, few-shot learning, continual learning, and in-context learning.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nPhD students\n\n\n\n    \n        \n            \n        \n        \n            \n                Shawon Ashraf is a PhD student working on Open Source Large Language Models focusing on Evaluations, Model Interpretability and Learning Dynamics in Neural Networks. His M.Sc. thesis was on exploring the effects of Predicate Arguments on multi-modal Procedural Reasoning at IMS, University of Stuttgart. He's currently associated with the LLMs4EU project.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Bilge Celik is a PhD student focusing on Automated Machine Learning for online data streams. She develops machine learning systems automatically adjusting to changing dynamics of real-time data streams.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Fabian Denoodt is a PhD candidate focusing on Efficient Uncertainty Quantification. He is particularly interested in the fundamental aspects of uncertainty estimation and in developing new, efficient methods targeted for hardware-constrained devices. Before joining TU/e, Fabian worked as a teaching assistant at the University of Antwerp, where he taught the practical sessions for the courses Artificial Intelligence, Artificial Neural Networks, Numerical Linear Algebra, Advanced Programming, and Distributed Systems.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Dalton Harmsen obtained his M.Sc. degree in Data Science & Artificial Intelligence at Eindhoven University of Technology (Netherlands). His M.Sc. dissertation was carried out on the topic of benchmarking post-training quantization techniques for large language models under the supervision of J.M. Tomczak. Currently, he is a Ph.D. student at Eindhoven University of Technology ‚Äì Data Science Domain with the DAI Cluster. Dalton Harmsen works on the OpenEuroLLM project under the supervision of Joaquin Vanschoren.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Diana Alexandra Onutu is a PhD candidate in the AutoML group at Eindhoven University of Technology (TU/e), Netherlands. She is involved in the Open Euro LLM project, which focuses on developing the next generation of open-source, multilingual foundation models. Her main interests include model exploration, pre-training strategies, and diffusion-based language models. Diana obtained her M.Sc. degree in Data Science & Artificial Intelligence from TU/e. Her M.Sc. thesis was carried out within the field of Machine Learning for Science, in which she proposed a score matching generative model for large geometric graphs designed to produce cosmological data.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Jiarong Pan is a PhD student in the group, working on improving sample efficiency in machine learning algorithms. His research interests include  meta-learning, Bayesian optimization and multi-objective optimization.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Shreya Sajid is a PhD Candidate at Eindhoven University of Technology (TU/e), where she works on developing novel methods for fine-tuning multi-modal foundation models as part of the EU-funded Project ELLIOT. She holds an M.Sc. in Data Science and Artificial Intelligence from TU/e, where her thesis focused on token sparsification techniques to improve the efficiency of audio transformer models.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Qingren Yao is a PhD student working on ELLIOT project, which aims to develop advanced open-source multimodal foundation models. His research interests include multimodality, embodied intelligence, and time series analysis.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Elif Ceren Gok Yildirim is a PhD student working on continual learning. Her research centers on exploring the capabilities of machine learning algorithms and advancing them to adapt and evolve over time. She likes to explore novel approaches to enable machines to accumulate knowledge from past experiences and apply it to new tasks without forgetting and contribute to the CL field.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Murat Onur Yildirim is a PhD Candidate in the AutoML group. He focuses on automatically and continually learning sparse experts for computer vision tasks. His research is dedicated to creating efficient continual learners by leveraging sparsity in networks, data, and labels.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Fangqin Zhou is a PhD student working on the TWINERGY project, which aims to optimize the growth of cherry tomatos in a vertical farm. Her interests focus on the Hyperspectral Imaging using deep learning and transformer models, as well as Reinforcement Learning.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\nAlumni and visitors\n\n\n\n    \n        \n            \n        \n        \n            \n                Israel Campero-Jurado is a PhD student in computer science working on the European ITEA project called INNO4HEALTH. His interests focus on applying automated machine learning (AutoML) to healthcare solutions and democratising AutoML.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Matthew Danish is a postdoctoral researcher in the AutoML group at Eindhoven University of Technology (TU/e) in the Netherlands. He is currently engaged with the AI Pathfinder initiative. He obtained his Ph.D. in 2015 from Boston University, worked on research projects at Cambridge University department of Computer Science and Technology ranging from lightweight Fortran code verification to computer vision and sensor networks, collaborating with industry and governmental initiatives. Later, after moving to the Netherlands, he joined the Geosciences department of Utrecht University for a cross-disciplinary machine learning project.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Taniya Das is a former Research Engineer solving various engineering and machine learning tasks for openml.org and  AI-on-Demand platform (an EU project), to make ML research better. She has contributed to openml-tensorflow¬† and openml-pytorch¬†extensions for OpenML, making it possible to use solve deep learning tasks using openml-python API. She also experimented the use of LLMs to make both the platforms more intelligent.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Gennaro Gala is a postdoctoral researcher contributing to the OpenEuroLLM project. His current research focuses on large language models, probabilistic generative modeling, structured representation learning, and tensor networks. In March 2025, he completed his PhD in probabilistic ML at Eindhoven University of Technology, under the supervision of Prof. Cassio de Campos and Dr. Erik Quaeghebeur. From 2015 to 2020, he completed both his BSc and MSc in Computer Science at the Universit√† degli Studi di Bari Aldo Moro (Italy), with a focus on Machine Intelligence and Knowledge Engineering. During this period, he actively collaborated with the Italian National Research Council on projects applying machine learning and deep learning to marine biology.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Mert Kilickaya is a former postdoctoral fellow researching autonomous visual learners that can continuously improve by extracting their own supervision from dynamic visual data streams. He advanced techniques in self-supervised continual learning, enabling AI systems to autonomously adapt and improve without relying on external annotations.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Rafael Gomes Mantovani is a former PhD student, and currently a professor at the Federal Technology University - Paran√° (UTFPR), campus of Apucarana, Brazil. He researches Machine Learning, Data Mining, Meta-learning, and Automated Machine Learning/Data Science.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Branislav Pecher is a former visiting PhD student from Kempelen Institute of Intelligent Technologies. His research mainly focuses on learning with limited labelled data and a better understanding of its sensitivity to different factors that influence how well these approaches work. During the visit, he worked on few-shot learning, exploring how different subsets of samples of different characteristics affect the success of transfer in these approaches, and proposing a new method for selecting a subset of high-quality samples for few-shot learning.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Jan van Rijn is a former PhD student, and currently assistant professor at Leiden University. During his PhD, he co-founded and developed OpenML.org, and did research towards meta-learning and algorithm selection based on OpenML data. He co-authored the Open Access book on Metalearning. His current research activities include AutoML and Trustworthy Artificial Intelligence.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Andrei Simion-Constantinescu is a PhD Researcher in the AutoML group focused on self-supervised learning for computer vision tasks. He holds an MSc degree from TU Delft with the graduation topic on contrastive learning for unlabeled videos. His work involves adapting state-of-the-art self-supervised techniques to solve practical problems in greenhouse crop prediction and vertical farming as part of NWO Sky High Project.\n            \n            \n                \n                \n                \n                \n            \n        \n    \n    \n\n\n\n    \n        \n            \n        \n        \n            \n                Jos van der Velde is a senior engineer focusing on software architecture and infrastructure. I previously worked on the Croissant standard, and the AI-on-Demand platform. I'm still actively contributing to the OpenML infrastructure."
  },
  {
    "objectID": "notebooks/adacl-introduction.html",
    "href": "notebooks/adacl-introduction.html",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Elif Ceren Gok Yildirim, Murat Onur Yildirim, Mert Kilickaya, Joaquin Vanschoren\n\n\nIn the rapidly evolving field of machine learning, one challenge remains particularly persistent: the ability to continually learn new information without forgetting previously acquired knowledge. This forgetting problem is generally referred to in the field as ‚Äúcatastrophic forgetting‚Äù. Catastrophic forgetting occurs because, while models are learning new tasks, they lose the ability to recall information about previously learned tasks. This happens because the model‚Äôs parameters are updated to fit the new data, which can overwrite the representations of the old data.\n\n\n\nClass-Incremental Learning is one challenging scenario in CL which aims to update a model‚Äôs parameters and expand the classification layer to learn new categories while maintaining its accuracy on previously observed classes. Then during the test time, it assumes not to have access to task ids.\n\n\n\nThree major approaches have been explored to mitigate this issue:\n\nRegularization-based Methods: These techniques prevent abrupt shifts in the neural network weights by applying penalties that stabilize important parameters.\nReplay-based Methods: These involve storing a subset of training data and replaying it during the learning of new tasks to maintain previous knowledge.\nArchitecture-based Methods: These methods adapt the network structure, either by expanding it or isolating parts of it to retain past information.\n\n\n\n\nAll these CL methods bring additional hyperparameters to the learning process. For example: regularization strength, learning rate, memory size, etc. Current methods define these hyperparameters to a fixed value and assume that using the same hyperparameter setting will be sufficient for learning all different subsequent tasks. However, this approach does not reflect a realistic scenario. We can imagine 2 realistic scenarios:\n##High Plasticity- Low Stability: Let‚Äôs say you have learned large animal categories and now you will have to learn vehicle categories. In this case, you would go for mid-to low regularization strength to learn the new categories. In addition to that, you might want to store many exemplars from the large animal category in the memory buffer because you will probably forget that since the vehicle category will change the model‚Äôs parameters a lot.\n\n\n\n\n\n\nThis time you have learned small vehicle categories and now you will have to learn large vehicle categories. In this case, you would think that these two subsequent tasks are intuitively similar so you don‚Äôt need to lower regularization strength because with high regularization, model weights will not change too much and current weights will probably be sufficient to learn the large vehicle category. Also, you might not want to store many exemplars from small vehicle categories because you did not change the model‚Äôs weights too much and you don‚Äôt want to use your budget unnecessarily.\n\n\n\nAdaCL (Adaptive Continual Learning) introduces a novel approach where learning rate, regularization, and memory size are treated as dynamic, tunable variables. Instead of being fixed, these parameters adapt according to the learner‚Äôs current condition and the complexity of the task.\nBut how to find optimum hyperparameter values?\nSearch Algorithm = Tree Parzen Estimators\nValidation set = some amount of data from the current task + some amount of data from previous tasks.\nTo achieve this, we use an AutoML tool which is Optuna and as a search algorithm, we apply Bayesian Optimization Tree Parzen Estimators. AdaCL searches and predicts the optimal values for these hyperparameters by evaluating the model‚Äôs performance on the validation set.\n\n\n\n\n\n\nOur experiments utilized two well-known datasets: CIFAR100 and MiniImageNet. Each dataset contains images from 100 different categories, and we trained all models with 10 tasks, with each task containing 10 classes. Standard metrics for evaluation were used:\nAccuracy (ACC) measures the final accuracy averaged over all tasks.\nBackward Transfer (BWT) measures the average accuracy change of each task after learning new tasks.\nWe compared AdaCL with various baseline methods including EWC, LwF, iCaRL, and WA but the GOOD NEWS is: it is a plugged-in approach that means you can actually combine it with other methods that require better hyperparameter setting in CL.\n\n\n\nKey Findings:\n\nAdaCL boosts the performance of regularization-based methods.\nAdaCL yields better resource usage by leading to less memory size with the same performance.\n\n\n\n\n\n\n\nAdaCL treats crucial hyperparameters as adaptable, addressing the challenges of CIL more effectively than traditional fixed-parameter approaches. We aim to underscore the importance of flexible, dynamic adaptation in the CL scenario. For those interested in further details, the full paper provides an in-depth analysis of the methods, experimental setup, and results. Read the full paper here.\n\nThis post was written by Elif Ceren Gok Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#understanding-continual-learning-and-the-catastrophic-forgetting",
    "href": "notebooks/adacl-introduction.html#understanding-continual-learning-and-the-catastrophic-forgetting",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "In the rapidly evolving field of machine learning, one challenge remains particularly persistent: the ability to continually learn new information without forgetting previously acquired knowledge. This forgetting problem is generally referred to in the field as ‚Äúcatastrophic forgetting‚Äù. Catastrophic forgetting occurs because, while models are learning new tasks, they lose the ability to recall information about previously learned tasks. This happens because the model‚Äôs parameters are updated to fit the new data, which can overwrite the representations of the old data."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#understanding-class-incremental-learning",
    "href": "notebooks/adacl-introduction.html#understanding-class-incremental-learning",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Class-Incremental Learning is one challenging scenario in CL which aims to update a model‚Äôs parameters and expand the classification layer to learn new categories while maintaining its accuracy on previously observed classes. Then during the test time, it assumes not to have access to task ids."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#solutions-for-solving-catastrophic-forgetting",
    "href": "notebooks/adacl-introduction.html#solutions-for-solving-catastrophic-forgetting",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Three major approaches have been explored to mitigate this issue:\n\nRegularization-based Methods: These techniques prevent abrupt shifts in the neural network weights by applying penalties that stabilize important parameters.\nReplay-based Methods: These involve storing a subset of training data and replaying it during the learning of new tasks to maintain previous knowledge.\nArchitecture-based Methods: These methods adapt the network structure, either by expanding it or isolating parts of it to retain past information."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#motivation-for-adacl",
    "href": "notebooks/adacl-introduction.html#motivation-for-adacl",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "All these CL methods bring additional hyperparameters to the learning process. For example: regularization strength, learning rate, memory size, etc. Current methods define these hyperparameters to a fixed value and assume that using the same hyperparameter setting will be sufficient for learning all different subsequent tasks. However, this approach does not reflect a realistic scenario. We can imagine 2 realistic scenarios:\n##High Plasticity- Low Stability: Let‚Äôs say you have learned large animal categories and now you will have to learn vehicle categories. In this case, you would go for mid-to low regularization strength to learn the new categories. In addition to that, you might want to store many exemplars from the large animal category in the memory buffer because you will probably forget that since the vehicle category will change the model‚Äôs parameters a lot."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#high-stability---low-plasticity",
    "href": "notebooks/adacl-introduction.html#high-stability---low-plasticity",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "This time you have learned small vehicle categories and now you will have to learn large vehicle categories. In this case, you would think that these two subsequent tasks are intuitively similar so you don‚Äôt need to lower regularization strength because with high regularization, model weights will not change too much and current weights will probably be sufficient to learn the large vehicle category. Also, you might not want to store many exemplars from small vehicle categories because you did not change the model‚Äôs weights too much and you don‚Äôt want to use your budget unnecessarily."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#introducing-adacl",
    "href": "notebooks/adacl-introduction.html#introducing-adacl",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "AdaCL (Adaptive Continual Learning) introduces a novel approach where learning rate, regularization, and memory size are treated as dynamic, tunable variables. Instead of being fixed, these parameters adapt according to the learner‚Äôs current condition and the complexity of the task.\nBut how to find optimum hyperparameter values?\nSearch Algorithm = Tree Parzen Estimators\nValidation set = some amount of data from the current task + some amount of data from previous tasks.\nTo achieve this, we use an AutoML tool which is Optuna and as a search algorithm, we apply Bayesian Optimization Tree Parzen Estimators. AdaCL searches and predicts the optimal values for these hyperparameters by evaluating the model‚Äôs performance on the validation set."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#experimental-setup",
    "href": "notebooks/adacl-introduction.html#experimental-setup",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Our experiments utilized two well-known datasets: CIFAR100 and MiniImageNet. Each dataset contains images from 100 different categories, and we trained all models with 10 tasks, with each task containing 10 classes. Standard metrics for evaluation were used:\nAccuracy (ACC) measures the final accuracy averaged over all tasks.\nBackward Transfer (BWT) measures the average accuracy change of each task after learning new tasks.\nWe compared AdaCL with various baseline methods including EWC, LwF, iCaRL, and WA but the GOOD NEWS is: it is a plugged-in approach that means you can actually combine it with other methods that require better hyperparameter setting in CL."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#results",
    "href": "notebooks/adacl-introduction.html#results",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "Key Findings:\n\nAdaCL boosts the performance of regularization-based methods.\nAdaCL yields better resource usage by leading to less memory size with the same performance."
  },
  {
    "objectID": "notebooks/adacl-introduction.html#conclusion",
    "href": "notebooks/adacl-introduction.html#conclusion",
    "title": "Adaptive Continual Learning",
    "section": "",
    "text": "AdaCL treats crucial hyperparameters as adaptable, addressing the challenges of CIL more effectively than traditional fixed-parameter approaches. We aim to underscore the importance of flexible, dynamic adaptation in the CL scenario. For those interested in further details, the full paper provides an in-depth analysis of the methods, experimental setup, and results. Read the full paper here.\n\nThis post was written by Elif Ceren Gok Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/malibo-blog.html",
    "href": "notebooks/malibo-blog.html",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "",
    "text": "This blogpost is about why we set out to write our paper ‚ÄúMALIBO: Meta-learning for Likelihood-free Bayesian Optimization‚Äù (üìÑpaper, ü§ñcode) and provides a brief overview of its main contributions."
  },
  {
    "objectID": "notebooks/malibo-blog.html#malibo-meta-learning-for-likelihood-free-bayesian-optimization",
    "href": "notebooks/malibo-blog.html#malibo-meta-learning-for-likelihood-free-bayesian-optimization",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "MALIBO: Meta-learning for Likelihood-free Bayesian Optimization",
    "text": "MALIBO: Meta-learning for Likelihood-free Bayesian Optimization\nBayesian Optimization (BO) is the tool of choice for expensive black-box optimization tasks, but its effectiveness often breaks down when dealing with high-dimensional, noisy, and heterogeneously scaled problems across diverse tasks. Traditional meta-learning BO frameworks, which are built atop Gaussian Processes (GPs), struggle with these due to their modeling assumptions and scalability limitations.\nIn this paper, we propose MALIBO (Meta-learning for LIkelihood-free Bayesian Optimization): meta-learning the acquisition function itself, rather than the surrogate model. It combines likelihood-free acuqisition functions with a task-uncertainty-aware meta-learning strategy, resulting in a robust and scalable optimization framework."
  },
  {
    "objectID": "notebooks/malibo-blog.html#what-is-meta-learning-bo",
    "href": "notebooks/malibo-blog.html#what-is-meta-learning-bo",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "What is Meta-learning BO?",
    "text": "What is Meta-learning BO?\nBayesian optimization (BO) aims to optimize an expensive black-box function: \\[\\begin{equation}\n    \\mathbf x^{*} = \\argmin_{\\mathbf x \\in \\mathcal{X}} f(\\mathbf{x})\n\\end{equation}\\]\nMeta-learning BO leverages information from past optimization experiences to accelerate the current optimization process.\n\n\n\nFig.1 - (Left) historical data from various tasks. (Right) Meta-learning model captures the information from histrical data for optimizing the target task.."
  },
  {
    "objectID": "notebooks/malibo-blog.html#motivation-why-move-beyond-gps-in-meta-bo",
    "href": "notebooks/malibo-blog.html#motivation-why-move-beyond-gps-in-meta-bo",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "Motivation: Why Move Beyond GPs in Meta-BO?",
    "text": "Motivation: Why Move Beyond GPs in Meta-BO?\nDespite the prevalence of GPs in BO, their application in meta-learning is fraught with several limitations:\n\nPoor scalability in both data and task number due the cubic computational complexity.\nSensitivity to scale mismatches across tasks (e.g., validation loss on MNIST vs.¬†CIFAR).\nHomoscedastic Gaussian noise assumptions, often violated in real-world data.\nDeterministic task similarity models, which lead to unreliable adaptation when the target task diverges from seen distributions.\n\nTo address these, MALIBO abandons traditional surrogate modeling and instead learns a classifier-based acquisition function, which can directly infer the utility of a query without modeling the response surface."
  },
  {
    "objectID": "notebooks/malibo-blog.html#how-to-tackle-these-limitations",
    "href": "notebooks/malibo-blog.html#how-to-tackle-these-limitations",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "How to tackle these limitations?",
    "text": "How to tackle these limitations?\n\nDirect approximation for acquisition function\nMALIBO is based on likelihood-free Bayesian optimization (LFBO) [1], which bypasses surrogate modeling by directly approximating the acquisition function. This approach eliminates the computational bottleneck associated with GPs and imposes fewer assumptions on the target functions.\nSpecifically, LFBO reformulates the approximation of the acquisition function as a binary classification problem. Observed data points are labeled as ‚Äúgood‚Äù or ‚Äúbad‚Äù based on whether they exceed a predefined threshold \\(\\tau\\). A probabilistic classifier \\(C_{\\theta}(\\mathbf{x})\\) is then trained to estimate the probability that a given input \\(\\mathbf{x}\\) belongs to the ‚Äúgood‚Äù class. This probability serves as a proxy for the acquisition function, guiding the selection of new evaluation points by maximizing the classifier‚Äôs output.\n\n \n\nFig.2 - (Left) A Gaussian process model and the acquisition function (Right) LFBO approximates the acquisition function directly by casting the problem into a classification problem\n\n\n\n\nUncertainty-awared Meta-learning\nThe LFBO framework enables scalable Bayesian optimization by directly approximating the acquisition function through classification, thereby avoiding strong assumptions about the black-box function or noise distribution. To extend this approach to a meta-learning setting, we require a classifier that can incorporate knowledge from prior tasks and generalize effectively to new ones. Unlike using regression model, such a meta-learning classifier is less sensitive to the heterogeneous scales in different tasks, which can improve the meta-learning performance.\n\n\n\nFig.3 - The classifier consists of two key components. 1. The task-agnostic part learns a shared feature mapping across all tasks, capturing the common structure and regularities that are transferable between tasks. 2. The task-specific embedding is modeled as a linear layer that modulates the shared features, introducing task-dependent variations to adapt the model to the current task. The final prediction is obtained by summing the outputs of these two components and applying a sigmoid activation, resulting in a probabilistic, binary prediction.\n\n\nThe meta-learning in MALIBO is constructed as followed: - Task-Agnostic Feature Mapping: A residual feedforward network maps input \\(x\\) to a shared feature space \\(\\phi(x)\\), with a mean prediction head \\(m(\\phi(x))\\).\n\nTask-Specific Latent Embeddings: Each task \\(t\\) is associated with an embedding (the weight in the linear layer) \\(z_t \\sim \\mathcal{N}(0, I)\\), and the classifier outputs \\(C(x) = \\sigma \\left( m(\\phi(x)) + z_t^\\top \\phi(x) \\right)\\)\n\nWith this meta-learning classifier, the model can adapt to new tasks by estimating a task-specific embedding \\(z\\) using the learned feature mapping. A straightforward approach would be to treat this as a maximum likelihood problem and directly optimize for \\(z\\) on the target task. However, this ignores task uncertainty and can lead to unreliable adaptation and over-exploitation of limited data. Moreover, when there is a mismatch between the meta-training data distribution and the non-i.i.d. data collected during optimization, a deterministic model may fail to generalize effectively. To address these issues, we adopt a Bayesian approach to task adaptation. By modeling the uncertainty in the task embedding, our classifier becomes more robust and exploratory, enabling better generalization to new and diverse tasks.\nWe introduce two components to mitigate these issues: a probabilistic way for meta-learning and a residual prediction module to make prediction solely based on target task.\n\nProbabilistic training and task adaptation\n\nProbabilistic meta-learning: During training, the embedding \\(z\\) is encourage to follow a prior distribution \\(\\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) in order to enable Bayesian inference during task adaptation.\nBayesian Adaptation via Laplace Approximation: A posterior over \\(z\\) is inferred using Laplace approximation around the maximum-a-posteriori estimate \\(q(z) \\approx \\mathcal{N}(z_{\\text{MAP}}, \\Sigma)\\).\nThompson Sampling: Enables exploratory behavior early on, and naturally supports parallel BO. Exploration is encouraged by sampling \\(z \\sim q(z)\\) and using: \\(C(x) = \\sigma \\left( m(\\phi(x)) + h_{z}(x) \\right)\\)\n\n\n\nResidual prediction module for robust adaptation\nTo handle out-of-distribution tasks or compensate for weak meta priors, MALIBO augments its prediction with a gradient boosting classifier trained on residuals from the meta-model.\n\n\n\nFig.4 - Effects of exploration and residual predictions. Color circles denote the optimization queries (from bright to dark), the dashed curve denotes a Thompson sample (TS) of the acquisition function and the orange curve shows the sample combined with gradient boosting (GB)."
  },
  {
    "objectID": "notebooks/malibo-blog.html#results",
    "href": "notebooks/malibo-blog.html#results",
    "title": "Meta-learning for Likelihood-free Bayesian Optimization",
    "section": "Results",
    "text": "Results\nBenchmarks: - NASBench-201: NAS with 6D discrete space, evaluated on CIFAR-10/100 and ImageNet-16. - HPOBench: 9D HPO problem on UCI datasets. - HPO-B: Large-scale HPO benchmark.\n\n\n\nFig.5 - Aggregated normalized regrets for BO algorithms on real-world AutoML problems.\n\n\nPerformance: - Faster convergence and better anytime performance than GP, ABLR, MetaBO, and even LFBO. - More robust to scale shifts and heteroscedastic noise. - Low computational overhead‚ÄîThompson sampling and Laplace approximation are lightweight compared to GP training or MCMC.\n\nReferences\n[1] Song, Jiaming, Lantao Yu, Willie Neiswanger, and Stefano Ermon. ‚ÄúA General Recipe for Likelihood-Free Bayesian Optimization.‚Äù, ICML 2022"
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html",
    "href": "notebooks/cl_with_coreset-introduction.html",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "Elif Ceren Gok Yildirim, Murat Onur Yildirim, Joaquin Vanschoren\n\n\nContinual Learning (CL) aims to enable models to learn sequentially from streaming data, retaining previously acquired knowledge while adapting to new tasks, a duality known as the stability-plasticity balance. This balance is critical for mimicking human-like learning, where accumulated knowledge is preserved (stability) yet flexibly updated with novel experiences (plasticity). There is big line of research that focus on achieving better stability-plasticity tradeoff by building and proposing new methods to literature.\n\n\n\nWhile these methods improve performance, they share a common assumption: all training samples are equally valuable and must be exhaustively utilized. By default, this standardized practice prioritizes plasticity (integrating new information) at the risk of destabilizing learned representations, as redundant or noisy samples may overwrite critical prior knowledge. This ‚Äùlearn-it-all‚Äù paradigm diverges from human learning efficiency since, as humans, we are initially exposed to vast amounts of information but intuitively filter and prioritize them, focusing on key experiences (e.g.¬†clear and novel examples) that enrich our understanding while disregarding redundant details.\n\n\n\n\n\n\nIn our approach, we put traditional CL methods to the test by comparing their performance when trained on the full dataset versus when they‚Äôre trained on a carefully selected, informative subset of samples (coreset).\nWe structured our training into two distinct phases:\n\nWarm-Up Phase: The CL model undergoes initial, partial training. This phase is key because it lets us analyze the model‚Äôs behavior and data representations, which in turn helps us pick out the most informative samples.\nLearning Phase: Once the warm-up is complete, we switch to training on just the selected subset of samples. This phase is much longer and is where the model truly refines its knowledge.\n\nNote that the warm-up phase is generally much shorter than the learning phase since it‚Äôs only long enough for the model to ‚Äúwarm up‚Äù and provide the insights needed for effective sample selection.\n\n\n\nBelow is a quick comparison of the CL baselines we used in our work:\n\n\n\n\n\n\n\n\nArchitecture-based\nMemory-based\nRegularization-based\n\n\n\n\nDER, FOSTER, MEMO\niCaRL, ER\nLwF\n\n\nDynamically expands the model‚Äôs representation space by adding new features for new tasks.\nHolds a memory buffer from previous tasks and keeps learning from them along with new tasks.\nRegularizes the model‚Äôs parameters when learning new tasks to retain previously acquired knowledge.\n\n\n\n\n\n\nBelow is a quick comparison of the Coreset baselines we used in our work:\n\n\n\n\n\n\nWe observed a significant improvement in continual learning performance when training with coreset samples instead of full datasets. We found out that this enhancement comes from achieving optimal stability-plasticity tradeoff.\nWhile training with all samples often boosts performance on the current task, it tends to forget earlier tasks. In contrast, using a selective subset (coreset) of samples helps retain prior knowledge better, even if it sacrifices on current task performance.\nThe heatmap below illustrates accuracy across tasks after each learning session on Split-CIFAR10. It compares full-sample training with the best-performing coreset strategy.\n\n\n\nAs seen in the figure, coreset training reduces forgetting, preserving earlier knowledge more effectively.\n\n\n\nTo better understand why this happens, we visualized the learned feature space using t-SNE plots of DER‚Äôs representations on Split-CIFAR10.\n\n\n\nWith 20% of the data, coreset-trained models form well-separated class clusters, suggesting clearer, less entangled representations.\nAs we increased the coreset fraction to 80%, and then to the full dataset, the class clusters became less distinct:\nWe also quantified this observation by calculating inter-class distances in the t-SNE-embedded space:\n\n20% coreset: 11.68\n80% coreset: 10.71\nFull dataset: 10.67\n\nThis confirms our visual findings: more data does not always lead to better representations or ensure better learning. Sometimes, it blurs the boundaries between classes, increasing the risk of forgetting past tasks specially in CL.\n\n\n\nWhile coreset-based training improves many CL methods, there are notable exceptions. Methods like FOSTER and LwF (which relies solely on regularization) perform better when trained with the full dataset. These approaches seem to require the complete data distribution to operate effectively.\nWe provide detailed insights and analysis for these exceptions in the full paper, including possible explanations and ablation studies.\nSince I don‚Äôt want to bore you with nitty-gritty details, I invite you to check out our paper. We would be happy if you want to talk about this work, so please feel free to reach out to us üòä.\n\nThis post was written by Elif Ceren Gok Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#background",
    "href": "notebooks/cl_with_coreset-introduction.html#background",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "Continual Learning (CL) aims to enable models to learn sequentially from streaming data, retaining previously acquired knowledge while adapting to new tasks, a duality known as the stability-plasticity balance. This balance is critical for mimicking human-like learning, where accumulated knowledge is preserved (stability) yet flexibly updated with novel experiences (plasticity). There is big line of research that focus on achieving better stability-plasticity tradeoff by building and proposing new methods to literature."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#motivation",
    "href": "notebooks/cl_with_coreset-introduction.html#motivation",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "While these methods improve performance, they share a common assumption: all training samples are equally valuable and must be exhaustively utilized. By default, this standardized practice prioritizes plasticity (integrating new information) at the risk of destabilizing learned representations, as redundant or noisy samples may overwrite critical prior knowledge. This ‚Äùlearn-it-all‚Äù paradigm diverges from human learning efficiency since, as humans, we are initially exposed to vast amounts of information but intuitively filter and prioritize them, focusing on key experiences (e.g.¬†clear and novel examples) that enrich our understanding while disregarding redundant details."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#the-overall-game-plan",
    "href": "notebooks/cl_with_coreset-introduction.html#the-overall-game-plan",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "In our approach, we put traditional CL methods to the test by comparing their performance when trained on the full dataset versus when they‚Äôre trained on a carefully selected, informative subset of samples (coreset).\nWe structured our training into two distinct phases:\n\nWarm-Up Phase: The CL model undergoes initial, partial training. This phase is key because it lets us analyze the model‚Äôs behavior and data representations, which in turn helps us pick out the most informative samples.\nLearning Phase: Once the warm-up is complete, we switch to training on just the selected subset of samples. This phase is much longer and is where the model truly refines its knowledge.\n\nNote that the warm-up phase is generally much shorter than the learning phase since it‚Äôs only long enough for the model to ‚Äúwarm up‚Äù and provide the insights needed for effective sample selection."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#cl-baselines",
    "href": "notebooks/cl_with_coreset-introduction.html#cl-baselines",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "Below is a quick comparison of the CL baselines we used in our work:\n\n\n\n\n\n\n\n\nArchitecture-based\nMemory-based\nRegularization-based\n\n\n\n\nDER, FOSTER, MEMO\niCaRL, ER\nLwF\n\n\nDynamically expands the model‚Äôs representation space by adding new features for new tasks.\nHolds a memory buffer from previous tasks and keeps learning from them along with new tasks.\nRegularizes the model‚Äôs parameters when learning new tasks to retain previously acquired knowledge."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#coreset-baselines",
    "href": "notebooks/cl_with_coreset-introduction.html#coreset-baselines",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "Below is a quick comparison of the Coreset baselines we used in our work:"
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#findings-and-insights",
    "href": "notebooks/cl_with_coreset-introduction.html#findings-and-insights",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "We observed a significant improvement in continual learning performance when training with coreset samples instead of full datasets. We found out that this enhancement comes from achieving optimal stability-plasticity tradeoff.\nWhile training with all samples often boosts performance on the current task, it tends to forget earlier tasks. In contrast, using a selective subset (coreset) of samples helps retain prior knowledge better, even if it sacrifices on current task performance.\nThe heatmap below illustrates accuracy across tasks after each learning session on Split-CIFAR10. It compares full-sample training with the best-performing coreset strategy.\n\n\n\nAs seen in the figure, coreset training reduces forgetting, preserving earlier knowledge more effectively."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#why-does-it-work-a-look-into-representations",
    "href": "notebooks/cl_with_coreset-introduction.html#why-does-it-work-a-look-into-representations",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "To better understand why this happens, we visualized the learned feature space using t-SNE plots of DER‚Äôs representations on Split-CIFAR10.\n\n\n\nWith 20% of the data, coreset-trained models form well-separated class clusters, suggesting clearer, less entangled representations.\nAs we increased the coreset fraction to 80%, and then to the full dataset, the class clusters became less distinct:\nWe also quantified this observation by calculating inter-class distances in the t-SNE-embedded space:\n\n20% coreset: 11.68\n80% coreset: 10.71\nFull dataset: 10.67\n\nThis confirms our visual findings: more data does not always lead to better representations or ensure better learning. Sometimes, it blurs the boundaries between classes, increasing the risk of forgetting past tasks specially in CL."
  },
  {
    "objectID": "notebooks/cl_with_coreset-introduction.html#when-coresets-dont-help",
    "href": "notebooks/cl_with_coreset-introduction.html#when-coresets-dont-help",
    "title": "Continual Learning with Informative Samples",
    "section": "",
    "text": "While coreset-based training improves many CL methods, there are notable exceptions. Methods like FOSTER and LwF (which relies solely on regularization) perform better when trained with the full dataset. These approaches seem to require the complete data distribution to operate effectively.\nWe provide detailed insights and analysis for these exceptions in the full paper, including possible explanations and ablation studies.\nSince I don‚Äôt want to bore you with nitty-gritty details, I invite you to check out our paper. We would be happy if you want to talk about this work, so please feel free to reach out to us üòä.\n\nThis post was written by Elif Ceren Gok Yildirim and need not reflect the view of co-authors."
  },
  {
    "objectID": "notebooks/amlb-introduction.html",
    "href": "notebooks/amlb-introduction.html",
    "title": "The AutoML Benchmark",
    "section": "",
    "text": "This blogpost is about why we set out to write our paper ‚ÄúAMLB: an AutoML Benchmark‚Äù (üìÑpaper, ü§ñcode) and provides a brief overview of its main contributions. In the end, we will share how you can make an impact on the future of this benchmark and automated machine learning (AutoML). But before we get started, here‚Äôs the tl;dr in haiku form:"
  },
  {
    "objectID": "notebooks/amlb-introduction.html#why-do-we-need-a-standardized-benchmark",
    "href": "notebooks/amlb-introduction.html#why-do-we-need-a-standardized-benchmark",
    "title": "The AutoML Benchmark",
    "section": "Why do we need a standardized benchmark?",
    "text": "Why do we need a standardized benchmark?\nWe started the project back in 2018. Over the summer, I (Pieter Gijsbers) got together with Erin LeDell and Janek Thomas to work on three problems we had identified:\n\nin the field of automated machine learning (in 2018), different papers would rarely evaluate their methods on the same data, and\nresearchers would often make simple ‚Äúuser errors‚Äù when evaluating methods proposed by others, and\nthe analyses were almost exclusively focused on predictive performance.\n\nWe decided to address these problems by introducing a benchmark suite, benchmark software, and a multi-dimensional analysis. That initially culminated into an introductory paper at the AutoML Workshop at ICML 2019, and ultimately in our publication at the Journal of Machine Learning Research. Let‚Äôs have a closer look at why these problems needed solving and our resulting contributions.\n\nThe Benchmark Suite\nResearch papers evaluating their methods using different sets of data inadvertently means that results are not comparable across different papers. This makes it impossible to determine state-of-the-art or to track progress in the field overtime. It may also lead to accidentally reporting optimistic results. For example, when researchers use a dataset both during development and evaluation, design decisions are made during development which improve performance for that dataset, but it is unlikely that it improves performance for unseen datasets equally. Thus, performance on that dataset is likely optimistic (i.e., better) compared to what you would expect on unseen datasets.\nEven if datasets used in evaluation are not used during development, the researchers‚Äô bias on which type of data they find important, such as biomedical data or big data, may lead to a similar ‚Äúaccidental cherry picking‚Äù. That type of data is reasonably assumed to be present during both development and evaluation, and other types of data for which the methods were not developed may be absent also in evaluation. This means that areas where methods may have blind spots are not evaluated and not reported on, giving an incomplete picture of the method‚Äôs strengths and weaknesses.\n\n\n\nHaving a curated set of datasets for evaluation managed by the community and carefully chosen to represent various data domains and characteristics should alleviate these problems. That is why we propose the initial version of such a standardized benchmark suite. The benchmarking suite contains 71 classification and 33 regression datasets that come from many different domains and exhibit many different dataset characteristics. We hope that with a benchmarking suite of this size and variety, we can trust in the generalizability of the results obtained.\n\n\nThe Benchmark Software\nAnother consequence of using a different set of data for each paper is that researchers need to evaluate the methods of others on their data. There are multiple examples where a method‚Äôs performance reported in papers are, for lack of a better word, the result of misunderstandings (or ‚Äúuser errors‚Äù). For example, a paper introducing method \\(X\\) and comparing it against state-of-the-art framework \\(Y\\) would report a failure of framework \\(Y\\) on dataset \\(D\\). In several cases we could diagnose the problem that caused the failure of framework \\(Y\\) on dataset \\(D\\) directly from the paper or a few minutes of inspecting the code. Examples would include using an out-of-date release or a wrong configuration of framework \\(Y\\).\n\n\n\nLuckily, as computer scientists, we have a wonderful tool in our belt to reduce human error: automation. That‚Äôs why we built standardized benchmarking software complete with developer-friendly AutoML framework integrations.\nWith the paper we released our open source benchmarking software. It allows you to run an AutoML evaluation with a single command. A simple python runbenchmark.py autosklearn openml/s/271 1h8c is all you need to evaluate autosklearn on our entire classification benchmark (OpenML suite 271). The software takes care of installing autosklearn, (down)loading the data and providing it to autosklearn, configuring autosklearn with a 1 hour time constraints with a limit of 8 CPU cores (hence 1h8c), monitoring its progress, and processing the predictions of the final model.\nThis level of automation means that no one is accidentally training on a test set. No one accidentally uses different parallelization for different AutoML frameworks, and no one accidentally optimizes for the wrong metrics. It‚Äôs a great step forward for fair comparisons. At least, that‚Äôs the idea. In many cases we make good on the promise, but we are working with a moving target (new framework releases) and bugs happen (see ‚ÄúThe Future‚Äù below).\nThe way we achieve this is by collaborating with AutoML framework developers. We built a strong shared framework that takes care of bootstrapping the experimental setup. It has generic functionalities for (down)loading data, installing dependencies, keeping track of experiments, processing results, and so on. Then, for each framework we have a minimal integration module which requires an installation script and a Python script which takes as input the data and task description and is expected to produce predictions as output. These integration scripts are often developed by, or in collaboration with, the authors of the AutoML frameworks which significantly reduces the chance that frameworks are used or configured incorrectly.\n\n\nA Multi-Dimensional Analysis\nThe last problem we identified was a single-minded analysis of the results. AutoML frameworks were compared (almost) exclusively on predictive performance. While this is a very important aspect of the final model, it provides an incomplete picture. Depending on the application that the model is trained for, other aspects like fast inference time or interpretability may be important or even required. In those cases, being able to assess the trade-off between predictive performance and other model dimensions is crucial. The characteristics of the AutoML framework itself may also be important: is it robust? is it configurable? does it adhere to resource constraints? That‚Äôs why we advocate for a multi-dimensional analysis of the results.\n\n\n\nIn our paper we evaluate 9 AutoML frameworks on our benchmarking suite and analyze the results. When analyzing predictive performance, we answer questions like: Which framework ranks best? Is it significant? How does that translate to absolute and relative differences? Do results hold across all data characteristics, or do we see some AutoML frameworks which work particularly well on datasets with certain characteristics?\nBut we also look beyond predictive accuracy. We inspect the learned models‚Äô inference time, and discuss its trade-off with predictive performance. We also investigate when AutoML frameworks fail and why: Is it triggered by the data? Is it misuse of resources? Do they adhere to the given time constraints?\nThe experiments in the paper are from 2023, so results would indubitably be different for frameworks today. Bugs are fixed, better methods for building better models are developed. However, we hope the paper serves as an inspiration and example for providing a more thorough multi-dimensional analysis.\n\n\nChallenges and Limitations\nFor more background, examples, challenges, and references you will want to have a look at the paper. It presents ideas presented here in more depth but also includes additional information on e.g., limitations of the benchmark, and it also contains the results and analysis of 9 frameworks on 104 datasets!"
  },
  {
    "objectID": "notebooks/amlb-introduction.html#the-future",
    "href": "notebooks/amlb-introduction.html#the-future",
    "title": "The AutoML Benchmark",
    "section": "The Future",
    "text": "The Future\nOvertime the landscape of data problems changes, as do the capabilities of AutoML frameworks, and the benchmarking suite should be updated to reflect that. One glaring ommission in the benchmark (in my personal opinion) is the lack of text data. In the original benchmarking suite presented in the paper, all datasets have strictly numerical or categorical data. This is for historical reasons, as a significant number of the AutoML frameworks in 2018 did not handle text data natively. But text data is ubiquitous, modern AutoML frameworks are well-equipped to deal with it, so it‚Äôs time we reflect that in the benchmark.\nMoreover, there is nothing uniquely ‚ÄúAutoML‚Äù about the AutoML benchmark. It benchmarks anything for which you write an integration script that takes as input a task and constraint description, and provides predictions as output. Sure, many algorithms may not deal with various dataset characteristics out-of-the-box (e.g., text data may need to be encoded) and the method may not adhere to time constraints, but that doesn‚Äôt mean the software can provide much of the same benefits of automation to evaluating ‚Äúregular‚Äù ML algorithms (in fact, TabRepo used AMLB). We hope to explore this further and potentially position AMLB for more general tabular ML benchmarking.\nAnd last but not least, though perhaps not as sexy, is the brunt of software engineering work. As time goes on, AutoML frameworks have new releases, people use different hardware, and AutoML is used for new problems, which means we are always working with a moving target. The benchmarking software needs to be maintained to work with modern stacks, extended to allow evaluation of different problem types, and incidental bugs need to be squashed. On top of that, we are continuously exploring ways to make the software easier to use for both developers and researchers.\nThe benchmark isn‚Äôt perfect, or even finished. It‚Äôs a start. And we invite you to collaborate with us.\nThere are many ways to contribute. The easiest way is to help is to leave a ‚≠êÔ∏è on our GitHub project, talk about the AutoML benchmark, and use it in your work. There are also a number of no-code contributions that help us out greatly:\n\nü§ù Help people on our GitHub issue tracker\nüìö Improve our documentation\nü§ì Identify interesting new datasets\n\nAnd finally, we very much welcome code contributions. üßë‚Äçüíª\n\nThis post was written by Pieter Gijsbers and need not reflect the view of co-authors."
  }
]