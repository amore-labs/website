{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d8c4448",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: MALIBO Meta-learning for Likelihood-free Bayesian Optimization\n",
    "topic: Method\n",
    "author: Jiarong Pan\n",
    "date: 2025-06-04\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "preview: A blogpost for our paper MALIBO, proposing a novel and scalable framework for meta-learning Bayesian optimization.\n",
    "image: malibo/malibo_features.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab08413",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This blogpost is about why we set out to write our paper \"*MALIBO: Meta-learning for Likelihood-free Bayesian Optimization*\" (üìÑ[paper](https://proceedings.mlr.press/v235/pan24b.html), ü§ñ[code](https://github.com/boschresearch/meta-learning-likelihood-free-bayesian-optimization)) and provides a brief overview of its main contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093e357",
   "metadata": {},
   "source": [
    "## MALIBO: Meta-learning for Likelihood-free Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370cab66",
   "metadata": {},
   "source": [
    "Bayesian Optimization (BO) is the tool of choice for expensive black-box optimization tasks, but its effectiveness often breaks down when dealing with high-dimensional, noisy, and heterogeneously scaled problems across diverse tasks. Traditional meta-learning BO frameworks, which are built atop Gaussian Processes (GPs), struggle with these due to their modeling assumptions and scalability limitations.\n",
    "\n",
    "In this paper, we propose **MALIBO** (**M**eta-learning for **LI**kelihood-free **B**ayesian **O**ptimization): meta-learning the acquisition function itself, rather than the surrogate model. It combines likelihood-free acuqisition functions with a task-uncertainty-aware meta-learning strategy, resulting in a robust and scalable optimization framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee6abf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## What is Meta-learning BO?\n",
    "\n",
    "Bayesian optimization (BO) aims to optimize an expensive black-box function:\n",
    "\\begin{equation}\n",
    "    \\mathbf x^{*} = \\argmin_{\\mathbf x \\in \\mathcal{X}} f(\\mathbf{x})\n",
    "\\end{equation}\n",
    "\n",
    "Meta-learning BO leverages information from past optimization experiences to accelerate the current optimization process.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"meta-learning for BO\" src=\"../images/blogs/malibo/meta_learning_bo.png\" style=\"width:70%\">\n",
    "<figcaption>Fig.1 - (Left) historical data from various tasks. (Right) Meta-learning model captures the information from histrical data for optimizing the target task..</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ee83b",
   "metadata": {},
   "source": [
    "## Motivation: Why Move Beyond GPs in Meta-BO?\n",
    "\n",
    "Despite the prevalence of GPs in BO, their application in meta-learning is fraught with several limitations:\n",
    "\n",
    "- Poor scalability in both data and task number due the cubic computational complexity.\n",
    "- Sensitivity to scale mismatches across tasks (e.g., validation loss on MNIST vs. CIFAR).\n",
    "- Homoscedastic Gaussian noise assumptions, often violated in real-world data.\n",
    "- Deterministic task similarity models, which lead to unreliable adaptation when the target task diverges from seen distributions.\n",
    "\n",
    "To address these, MALIBO abandons traditional surrogate modeling and instead learns a classifier-based acquisition function, which can directly infer the utility of a query without modeling the response surface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c319b09",
   "metadata": {},
   "source": [
    "## How to tackle these limitations?\n",
    "\n",
    "### Direct approximation for acquisition function\n",
    "\n",
    "MALIBO is based on likelihood-free Bayesian optimization (LFBO) [1], which bypasses surrogate modeling by directly approximating the acquisition function. This approach eliminates the computational bottleneck associated with GPs and imposes fewer assumptions on the target functions. \n",
    "\n",
    "Specifically, LFBO reformulates the approximation of the acquisition function as a binary classification problem. Observed data points are labeled as \"good\" or \"bad\" based on whether they exceed a predefined threshold $\\tau$. A probabilistic classifier $C_{\\theta}(\\mathbf{x})$ is then trained to estimate the probability that a given input $\\mathbf{x}$ belongs to the \"good\" class. This probability serves as a proxy for the acquisition function, guiding the selection of new evaluation points by maximizing the classifier‚Äôs output. \n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"gp\" src=\"../images/blogs/malibo/gp.png\" style=\"width:40%\">\n",
    "<img alt=\"lfbo\" src=\"../images/blogs/malibo/lfbo.png\" style=\"width:40%\">\n",
    "<figcaption>Fig.2 - (Left) A Gaussian process model and the acquisition function (Right) LFBO approximates the acquisition function directly by casting the problem into a classification problem </figcaption>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Uncertainty-awared Meta-learning\n",
    "\n",
    "The LFBO framework enables scalable Bayesian optimization by directly approximating the acquisition function through classification, thereby avoiding strong assumptions about the black-box function or noise distribution. To extend this approach to a meta-learning setting, we require a classifier that can incorporate knowledge from prior tasks and generalize effectively to new ones. Unlike using regression model, such a meta-learning classifier is less sensitive to the heterogeneous scales in different tasks, which can improve the meta-learning performance.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"malibo scheme\" src=\"../images/blogs/malibo/malibo_scheme.png\" style=\"width:60%\">\n",
    "<figcaption>Fig.3 - The classifier consists of two key components. 1. The task-agnostic part learns a shared feature mapping across all tasks, capturing the common structure and regularities that are transferable between tasks. 2. The task-specific embedding is modeled as a linear layer that modulates the shared features, introducing task-dependent variations to adapt the model to the current task. The final prediction is obtained by summing the outputs of these two components and applying a sigmoid activation, resulting in a probabilistic, binary prediction. </figcaption>\n",
    "</div>\n",
    "\n",
    "The meta-learning in MALIBO is constructed as followed:\n",
    "- Task-Agnostic Feature Mapping: A residual feedforward network maps input $x$ to a shared feature space $\\phi(x)$, with a mean prediction head $m(\\phi(x))$.\n",
    "\n",
    "- Task-Specific Latent Embeddings: Each task $t$ is associated with an embedding (the weight in the linear layer) $z_t \\sim \\mathcal{N}(0, I)$, and the classifier outputs $C(x) = \\sigma \\left( m(\\phi(x)) + z_t^\\top \\phi(x) \\right)$\n",
    "\n",
    "With this meta-learning classifier, the model can adapt to new tasks by estimating a task-specific embedding $z$ using the learned feature mapping. A straightforward approach would be to treat this as a maximum likelihood problem and directly optimize for $z$ on the target task. However, this ignores task uncertainty and can lead to unreliable adaptation and over-exploitation of limited data. Moreover, when there is a mismatch between the meta-training data distribution and the non-i.i.d. data collected during optimization, a deterministic model may fail to generalize effectively. To address these issues, we adopt a Bayesian approach to task adaptation. By modeling the uncertainty in the task embedding, our classifier becomes more robust and exploratory, enabling better generalization to new and diverse tasks.\n",
    "\n",
    "We introduce two components to mitigate these issues: a probabilistic way for meta-learning and a residual prediction module to make prediction solely based on target task.\n",
    "\n",
    "#### Probabilistic training and task adaptation \n",
    "\n",
    "- Probabilistic meta-learning: During training, the embedding $z$ is encourage to follow a prior distribution $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ in order to enable Bayesian inference during task adaptation.\n",
    "- Bayesian Adaptation via Laplace Approximation: A posterior over $z$ is inferred using Laplace approximation around the maximum-a-posteriori estimate $q(z) \\approx \\mathcal{N}(z_{\\text{MAP}}, \\Sigma)$.\n",
    "- Thompson Sampling: Enables exploratory behavior early on, and naturally supports parallel BO. Exploration is encouraged by sampling $z \\sim q(z)$ and using: $C(x) = \\sigma \\left( m(\\phi(x)) + h_{z}(x) \\right)$\n",
    "\n",
    "#### Residual prediction module for robust adaptation\n",
    "\n",
    "To handle out-of-distribution tasks or compensate for weak meta priors, MALIBO augments its prediction with a gradient boosting classifier trained on residuals from the meta-model.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"variants\" src=\"../images/blogs/malibo/ts_gb.png\" style=\"width:60%\">\n",
    "<figcaption>Fig.4 - Effects of exploration and residual predictions. Color circles denote the optimization queries (from bright to dark), the dashed curve denotes a Thompson sample (TS) of the acquisition function and the orange curve shows the sample combined with gradient boosting (GB). </figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a1921",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "Benchmarks:\n",
    "- NASBench-201: NAS with 6D discrete space, evaluated on CIFAR-10/100 and ImageNet-16.\n",
    "- HPOBench: 9D HPO problem on UCI datasets.\n",
    "- HPO-B: Large-scale HPO benchmark.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"results\" src=\"../images/blogs/malibo/results.png\" style=\"width:100%\">\n",
    "<figcaption> Fig.5 - Aggregated normalized regrets for BO algorithms on real-world AutoML problems. </figcaption>\n",
    "</div>\n",
    "\n",
    "Performance:\n",
    "- Faster convergence and better anytime performance than GP, ABLR, MetaBO, and even LFBO.\n",
    "- More robust to scale shifts and heteroscedastic noise.\n",
    "- Low computational overhead‚ÄîThompson sampling and Laplace approximation are lightweight compared to GP training or MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d959b86",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Song, Jiaming, Lantao Yu, Willie Neiswanger, and Stefano Ermon. ‚ÄúA General Recipe for Likelihood-Free Bayesian Optimization.‚Äù, ICML 2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malibo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
